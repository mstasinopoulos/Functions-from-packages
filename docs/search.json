[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Extra material for GAMLSS packages",
    "section": "",
    "text": "This website contains information about the functions in packages gamlss.prepdata and gamlss.ggplots\n\nthe package gamlss.prepdata\nthe package gamlss.ggplots"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About extra materrial for GAMLSS packages",
    "section": "",
    "text": "This site is created to provide a platform to explain some of the function in GAMLSS packages in R software."
  },
  {
    "objectID": "gamlss_prepdata.html",
    "href": "gamlss_prepdata.html",
    "title": "The Functions in the package gamlss.prepdata",
    "section": "",
    "text": "This booklet introduces the gamlss.prepdata package and its functionality. It aims to describe the available functions and how they can be used.\n\nThe latest versions of the packages gamlss, gamlss2 and gamlss.prepdata are shown below:\n\nrm(list=ls())\nlibrary(gamlss)\nlibrary(gamlss2)\nlibrary(ggplot2)\nlibrary(gamlss.ggplots)\nlibrary(gamlss.prepdata)\nlibrary(\"dplyr\") \npackageVersion(\"gamlss\")\n\n[1] '5.4.23'\n\npackageVersion(\"gamlss2\")\n\n[1] '0.1.0'\n\npackageVersion(\"gamlss.prepdata\")\n\n[1] '0.1.6'"
  },
  {
    "objectID": "gamlss_prepdata.html#sec-introduction",
    "href": "gamlss_prepdata.html#sec-introduction",
    "title": "The Functions in the package gamlss.prepdata",
    "section": "",
    "text": "This booklet introduces the gamlss.prepdata package and its functionality. It aims to describe the available functions and how they can be used.\n\nThe latest versions of the packages gamlss, gamlss2 and gamlss.prepdata are shown below:\n\nrm(list=ls())\nlibrary(gamlss)\nlibrary(gamlss2)\nlibrary(ggplot2)\nlibrary(gamlss.ggplots)\nlibrary(gamlss.prepdata)\nlibrary(\"dplyr\") \npackageVersion(\"gamlss\")\n\n[1] '5.4.23'\n\npackageVersion(\"gamlss2\")\n\n[1] '0.1.0'\n\npackageVersion(\"gamlss.prepdata\")\n\n[1] '0.1.6'"
  },
  {
    "objectID": "gamlss_prepdata.html#introduction-to-the-gamlss.prepdata-package",
    "href": "gamlss_prepdata.html#introduction-to-the-gamlss.prepdata-package",
    "title": "The Functions in the package gamlss.prepdata",
    "section": "Introduction to the gamlss.prepdata Package",
    "text": "Introduction to the gamlss.prepdata Package\nThe gamlss.prepdata package originated from the gamlss.ggplots package. As gamlss.ggplots became too large for easy maintenance, it was split into two separate packages, and gamlss.prepdata was created.\nSince gamlss.prepdata is still at an experimental stage, some functions are hidden to allow time for thorough checking and validation. These hidden functions can still be accessed using the triple colon notation, for example: gamlss.prepdata:::.\nThe functions available in gamlss.prepdata are intended for pre-fitting — that is, to be used before applying the gamlss() or gamlss2() fitting functions. The available functions can be grouped into the following categories:\n\nInformation functions\nThese functions provide information about:\n- The size of the dataset\n\n- The presence and extent of missing values\n\n- The structure of the dataset\n\nWhether the variables are numeric or factors, and how they should be prepared for analysis\n\n\n\nPlotting functions\nThese functions allow plotting for:\n-   individual variables\n\n-   Pairwise relationships between variables.\n\n\nFeatures functions\nFunctions that assist in\n\nDetecting outliers\nApplying transformations\nScaling variables.\n\n\n\nData Partition functions\nFunctions that facilitate partitioning data to improve inference and avoid overfitting during model selection.\n\n\nPurpose and Usage\nThe information and plotting functions provide valuable insights that assist in building better models, including:\n\nUnderstanding the distribution of the response variable\nChoosing the appropriate type of analysis\nExamining explanatory variables, including:\n\nRange and spread of values\nPresence of missing values\nAssociations and interactions between explanatory variables\nNature of relationships between response and explanatory variables (linear or non-linear)\n\n\nThe features functions focus on handling outliers, scaling, and transforming explanatory variables (x-variables) before modeling.\nData partitioning is used to avoid overfitting by ensuring models are evaluated more reliably. It falls under the broader category of data manipulation, although merging datasets is not covered here — only partitioning for improved inference is addressed.\nMost of the pre-fitting functions are data-related, and their names typically start with data (e.g., data_NAME), indicating that they either print information, produce plots, or manipulate data.frames.\nThe gamlss.prepdata package is thus a useful tool for carrying out pre-analysis work before beginning the process of fitting a distributional regression model.\nNext, we define what we mean by distributional regression."
  },
  {
    "objectID": "gamlss_prepdata.html#distributional-regression-model",
    "href": "gamlss_prepdata.html#distributional-regression-model",
    "title": "The Functions in the package gamlss.prepdata",
    "section": "Distributional Regression model",
    "text": "Distributional Regression model\nThe aim of this vignette is to demonstrate how to manipulate and prepare data before applying a distributional regression analysis.\nThe general form a distributional regression model can be written as; \\[\n\\begin{split}\n\\textbf{y}     &  \\stackrel{\\small{ind}}{\\sim }  \\mathcal{D}( \\boldsymbol{\\theta}_1, \\ldots, \\boldsymbol{\\theta}_k) \\nonumber \\\\\ng_1(\\boldsymbol{\\theta}_1) &= \\mathcal{ML}_1(\\textbf{x}_{11},\\textbf{x}_{21}, \\ldots,  \\textbf{x}_{p1}) \\nonumber \\\\\n\\ldots &= \\ldots \\nonumber\\\\\ng_k(\\boldsymbol{\\theta}_k) &= \\mathcal{ML}_k(\\textbf{x}_{1k},\\textbf{x}_{2k}, \\ldots,  \\textbf{x}_{pk}).\n\\end{split}\n  \\tag{1}\\] where we assume that the response variable \\(y_i\\) for \\(i=1,\\ldots, n\\), is independently distributed having a distribution \\(\\mathcal{D}( \\theta_1, \\ldots, \\theta_k)\\) with \\(k\\) parameters and where all parameters could be effected by the explanatory variables \\(\\textbf{x}_{1},\\textbf{x}_{2}, \\ldots,  \\textbf{x}_{p}\\). The \\(\\mathcal{ML}\\) represents any regression type machine learning algorithm i.e. LASSO, Neural networks etc.\nWhen only additive smoothing terms are used in the fitting the model can be written as; \\[\\begin{split}\n\\textbf{y}     &  \\stackrel{\\small{ind}}{\\sim }  \\mathcal{D}(  \\boldsymbol{\\theta}_1, \\ldots,  \\boldsymbol{\\theta}_k) \\nonumber \\\\\ng_1( \\boldsymbol{\\theta}_1)  &= b_{01} + s_1(\\textbf{x}_{11})  +  \\cdots,  +s_p(\\textbf{x}_{p1}) \\nonumber\\\\\n\\ldots &= \\ldots \\nonumber\\\\\ng_k( \\boldsymbol{\\theta}_k)  &= b_{0k} + s_1(\\textbf{x}_{1k})  +   \\cdots,  +s_p(\\textbf{x}_{pk}).\n\\end{split}\n\\tag{2}\\] which is the GAMLSS model introduced by Rigby and Stasinopoulos (2005).\nThere are three books on GAMLSS, D. M. Stasinopoulos et al. (2017), Rigby et al. (2019) and M. D. Stasinopoulos et al. (2024) and several ’GAMLSS lecture materials, available from GitHUb, https://github.com/mstasinopoulos/Porto_short_course.git and https://github.com/mstasinopoulos/ShortCourse.git. The latest R packages related to GAMLSS can be found in https://gamlss-dev.r-universe.dev/builds.\nThe aim of this package is to prepare data and extract useful information that can be utilized during the modeling stage."
  },
  {
    "objectID": "gamlss_prepdata.html#sec-Information",
    "href": "gamlss_prepdata.html#sec-Information",
    "title": "The Functions in the package gamlss.prepdata",
    "section": "Ιnformation functions",
    "text": "Ιnformation functions\nThe functions for obtaining information from a dataset are described in this section and summarized in Table 1. These functions can provide:\n\nGeneral information about the dataset such as the dimensions (number of rows and columns) and the percentage of omitted (missing) observations\nInformation about the variables in the dataset\nInformation about the observations in the dataset\n\nAll functions have a data.frame as their first argument.\nHere is a table of the information functions;\n\n\n\nTable 1: A summary table of the functions to obtain information\n\n\n\n\n\n\n\n\n\nFunctions\nUsage\n\n\n\n\ndata_dim()\nReturns the number of rows and columns, and calculates the percentage of omitted (missing) observations\n\n\ndata_names()\nLists the names of the variables in the dataset\n\n\ndata_rename()\nAllows renaming of one or more variables in the dataset\n\n\ndata_shorter_names()\nAllows shortening the names of one or more variables in the dataset\n\n\ndata_distinct()\nDisplays the number of distinct values for each variable in the dataset\n\n\ndata_which_na()\nDisplays the count of NA (missing) values for each variable in the dataset\n\n\ndata_omit()\nRemoves all rows (observations) that contain any NA (missing) values\n\n\ndata_str()\nDisplays the class of each variable (e.g., numeric, factor, etc.) along with additional details about the variables\n\n\ndata_cha2fac()\nConverts all character variables in the dataset to factor type\n\n\ndata_few2fac()\nConverts variables with a small number of distinct values (e.g., binary or categorical) to factor type\n\n\ndata_int2num()\nConverts integer variables with several distinct values to numeric type to allow for continuous analysis\n\n\ndata_rm()\nRemoves one or more variables (columns) from the dataset as specified by the user\n\n\ndata_rmNAvars()\nRemoves variables which have NA’s as all its values.\n\n\ndata_rm1val()\nRemoves factors that have only a single level (no variability) in the dataset\n\n\ndata_select()\nAllows the user to select one or more specific variables (columns) from the dataset for further analysis\n\n\ndata_exclude_class()\nRemoves all variables of a specified class (e.g., factor, numeric) from the dataset\n\n\ndata_only_continuous()\nRetains only the numeric variables (columns) in the dataset, excluding factors or other variable types\n\n\n\n\n\n\nNext we give examples of the functions.\n\ndata_dim()\nThis function provides detailed information about the dimensions of a data.frame. It is similar to the R function dim(), but with additional details. The output is the original data frame, allowing it to be used in a series of piping commands.\n\nrent99 |&gt; data_dim()\n\n************************************************************** \n************************************************************** \nthe R class of the data is: data.frame \nthe dimensions of the data are: 3082 by 9 \nnumber of observations with missing values: 0 \n% of NA's in the data: 0 % \n************************************************************** \n************************************************************** \n\n\nback to table\n\n\ndata_names()\nThis function provides the names of the variables in the data.frame, similar to the R function names().\n\nrent99 |&gt; data_names()\n\n************************************************************** \n************************************************************** \nthe names of variables \n[1] \"rent\"     \"rentsqm\"  \"area\"     \"yearc\"    \"location\" \"bath\"     \"kitchen\" \n[8] \"cheating\" \"district\"\n************************************************************** \n************************************************************** \n\n\nThe output of the function is the original data frame. back to table\n\n\ndata_rename()\nRenames one or more variables (columns) in the data.frame using the function data_rename();\n\nda&lt;- rent99 |&gt; data_rename(oldname=\"rent\", newname=\"R\")\nhead(da)\n\n         R   rentsqm area yearc location bath kitchen cheating district\n1 109.9487  4.228797   26  1918        2    0       0        0      916\n2 243.2820  8.688646   28  1918        2    0       0        1      813\n3 261.6410  8.721369   30  1918        1    0       0        1      611\n4 106.4103  3.547009   30  1918        2    0       0        0     2025\n5 133.3846  4.446154   30  1918        2    0       0        1      561\n6 339.0256 11.300851   30  1918        2    0       0        1      541\n\n\nThe output of the function is the original data frame with new names.\nback to table\n\n\ndata_shorter_names()\nIf the variables in the dataset have very long names, they can be difficult to handle in formulae during modelling. The function data_shorter_names() abbreviates the names of the explanatory variables, making them easier to use in formulas.\n\nrent99 |&gt; data_shorter_names()\n\n************************************************************** \n************************************************************** \nthe names of variables \n[1] \"rent\"  \"rents\" \"area\"  \"yearc\" \"locat\" \"bath\"  \"kitch\" \"cheat\" \"distr\"\n************************************************************** \n************************************************************** \n\n\nIf no long variable names exist in the dataset, the function data_shorter_names() does nothing. However, when applicable, the function abbreviates long names and returns the original data frame with the new shortened names.\n\n\n\n\n\n\nWarning\n\n\n\nNote that there is a risk when using a small value for the max option, as it may result in identical names for different variables. This could lead to confusion or errors in the modelling process. It is important to carefully choose an appropriate value for max to avoid this issue.\n\n\nback to table\n\n\ndata_distinct()\nTThe distinct values for each variable are shown below.\n\nrent99 |&gt; data_distinct()\n\n    rent  rentsqm     area    yearc location     bath  kitchen cheating \n    2723     3053      132       68        3        2        2        2 \ndistrict \n     336 \n\n\nThe output of the function is the original data frame.\nback to table\n\n\ndata_which_na()\nThis function provides information about which variables have missing observations and how many missing values there are for each variable;\n\nrent99 |&gt; data_which_na()\n\n    rent  rentsqm     area    yearc location     bath  kitchen cheating \n       0        0        0        0        0        0        0        0 \ndistrict \n       0 \n\n\nThe output of the function is the original data frame nothing is changing.\nback to table\n\n\ndata_omit\nThe function data_str() (similar to the R function str()) provides information about the types of variables present in the dataset.\n\nrent99 |&gt; data_omit()\n\n************************************************************** \n************************************************************** \nthe R class of the data is: data.frame \nthe dimensions of the data before omition are: 3082 x 9 \nthe dimensions of the data saved after omition are: 3082 x 9 \nthe number of observations omited: 0 \n************************************************************** \n************************************************************** \n\n\nThe output of the function is a new data frame with all NA’s omitted.\n\n\n\n\n\n\nWarning\n\n\n\nIt is important to select the relevant variables before using the data_omit() function, as some unwanted variables may contain many missing values that could lead to unnecessary row omissions.\n\n\nback to table\n\n\ndata_str()\nThe function data_str() (similar to the R function str()) provides information about the types of variable exist in the data.\n\nrent99 |&gt; data_str()\n\n************************************************************** \n************************************************************** \nthe structure of the data \n'data.frame':   3082 obs. of  9 variables:\n $ rent    : num  110 243 262 106 133 ...\n $ rentsqm : num  4.23 8.69 8.72 3.55 4.45 ...\n $ area    : int  26 28 30 30 30 30 31 31 32 33 ...\n $ yearc   : num  1918 1918 1918 1918 1918 ...\n $ location: Factor w/ 3 levels \"1\",\"2\",\"3\": 2 2 1 2 2 2 1 1 1 2 ...\n $ bath    : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ kitchen : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 2 1 1 ...\n $ cheating: Factor w/ 2 levels \"0\",\"1\": 1 2 2 1 2 2 1 2 1 1 ...\n $ district: int  916 813 611 2025 561 541 822 1713 1812 152 ...\n************************************************************** \n************************************************************** \ntable of the class of variabes \n\n factor integer numeric \n      4       2       3 \n************************************************************** \n************************************************************** \ndistinct values in variables \n    rent  rentsqm     area    yearc location     bath  kitchen cheating \n    2723     3053      132       68        3        2        2        2 \ndistrict \n     336 \nconsider to make those characters vectors into factors: \nlocation bath kitchen cheating \n************************************************************** \n************************************************************** \n\n\nThe output of the function is the original data frame.\nThe next set of functions manipulate variables withing data.frames.\nback to table\n\n\ndata_cha2fac()\nOften, variables in datasets are read as character vectors, but for analysis, they may need to be treated as factors. This function transforms any character vector (with a relatively small number of distinct values) into a factor.\n\nrent99 |&gt; data_cha2fac()  -&gt; da \n\n************************************************************** \nnot character vector was found \n\n\nSince no character were found nothing have changed. The output of the function is a new data frame.\nback to table\n\n\ndata_few2fac()\nThere are occasions when some variables have very few distinct observations, and it may be better to treat them as factors. The function data_few2fac() converts vectors with a small number of distinct values into factors.\n\nrent99 |&gt;  data_few2fac() -&gt; da \n\n************************************************************** \n    rent  rentsqm     area    yearc location     bath  kitchen cheating \n    2723     3053      132       68        3        2        2        2 \ndistrict \n     336 \n************************************************************** \n4 vectors with fewer number of values than 5 were transformed to factors \n************************************************************** \n************************************************************** \n\nstr(da)\n\n'data.frame':   3082 obs. of  9 variables:\n $ rent    : num  110 243 262 106 133 ...\n $ rentsqm : num  4.23 8.69 8.72 3.55 4.45 ...\n $ area    : int  26 28 30 30 30 30 31 31 32 33 ...\n $ yearc   : num  1918 1918 1918 1918 1918 ...\n $ location: Factor w/ 3 levels \"1\",\"2\",\"3\": 2 2 1 2 2 2 1 1 1 2 ...\n $ bath    : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ kitchen : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 2 1 1 ...\n $ cheating: Factor w/ 2 levels \"0\",\"1\": 1 2 2 1 2 2 1 2 1 1 ...\n $ district: int  916 813 611 2025 561 541 822 1713 1812 152 ...\n\n\nThe output of the function is a new data frame. This can be seen by using the R function str();\nback to table\n\n\ndata_int2num()\nOccasionally, we need to convert integer variables with a very large range of values into numeric vectors, especially for graphics. The function data_int2num() performs this conversion.\n\nrent99 |&gt; data_int2num() -&gt; da\n\n************************************************************** \n    rent  rentsqm     area    yearc location     bath  kitchen cheating \n    2723     3053      132       68        3        2        2        2 \ndistrict \n     336 \n2 integer vectors with more number of values than 50 were transformed to numeric \n************************************************************** \n\n\nThe output of the function is a new data frame.\nback to table\n\n\ndata_rm()\nFor plotting datasets, it’s easier to keep only the relevant variables. The function data_rm() allows you to remove unnecessary variables, making the dataset more manageable for visualization.\n\ndata_rm(rent99, c(2,9)) -&gt; da\ndim(rent99)\n\n[1] 3082    9\n\ndim(da)\n\n[1] 3082    7\n\n\nThe output of the function is a new data frame. Note that this could be also done using the function select() of the package dplyr, or our own data_select() function.\nback to table\n\n\ndata_rmNAvars()\nOccutionally when we read data from files in Rsome extra variables are accidentaly produced with no values but NA’s. The function data_rmNAvars() removes those variables from the data;\n\ndata_rmNAvars(da)\n\nback to table\n\n\ndata_rm1val()\nThis function searches for variables with only a single distinct value (often factors left over from a previous subset() operation) and removes them from the dataset.\n\nda |&gt; data_rm1val()\n\nThe output of the function is a new data frame.\nback to table\n\n\ndata_select()\nThis function selects specified variables from a dataset.\n\nda1&lt;-rent |&gt; data_select( vars=c(\"R\", \"Fl\", \"A\"))\nhead(da1)\n\n       R Fl    A\n1  693.3 50 1972\n2  422.0 54 1972\n3  736.6 70 1972\n4  732.2 50 1972\n5 1295.1 55 1893\n6 1195.9 59 1893\n\n\nThe output of the function is a new data frame.\nback to table\n\n\ndata_exclude_class()\nThis function searches for variables (columns) of a specified R class and removes them from the dataset. By default, the class to remove is factor.\n\nda |&gt; data_exclude_class()  -&gt; da1\nhead(da1)\n\n      rent area yearc\n1 109.9487   26  1918\n2 243.2820   28  1918\n3 261.6410   30  1918\n4 106.4103   30  1918\n5 133.3846   30  1918\n6 339.0256   30  1918\n\n\nThe output of the function is a new data frame.\n\n\ndata_only_continuous()\nThis function keeps only the continuous variables in the dataset, removing all non-continuous variables.\n\nda |&gt; data_only_continuous()  -&gt; da1\nhead(da1)\n\n      rent area yearc\n1 109.9487   26  1918\n2 243.2820   28  1918\n3 261.6410   30  1918\n4 106.4103   30  1918\n5 133.3846   30  1918\n6 339.0256   30  1918\n\n\nThe output of the function is a new data frame.\nback to table"
  },
  {
    "objectID": "gamlss_prepdata.html#sec-Graphical-functions",
    "href": "gamlss_prepdata.html#sec-Graphical-functions",
    "title": "The Functions in the package gamlss.prepdata",
    "section": "Graphical functions",
    "text": "Graphical functions\nGraphical methods are used in pre-analysis to examine individual variables or pair-wise relationships between variables.\n\n\n\nTable 2: A summary table of the graphical functions\n\n\n\n\n\n\n\n\n\nFunctions\nUsage\n\n\n\n\ndata_plot()\nPlots each variable in the dataset to visualize its distribution or other important characteristics (see also Section 6.1\n\n\ndata_bucket()\nGenerates bucket plots for all numerical variables in the dataset to visualize their skewness and kurtosis\n\n\ndata_response()\nPlots the response variable alongside its z-score, providing a standardized version of the response for comparison\n\n\ndata_zscores()\nPlots the z-scores for all continuous variables in the dataset, allowing for easy visualization of the standardized values (see also Section 6.1)\n\n\ndata_xyplot()\nGenerates pairwise plots of the response variable against all other variables in the dataset to visualize their relationships\n\n\ndata_cor()\nCalculates and plots the pairwise correlations for all continuous variables in the dataset to assess linear relationships between them\n\n\ndata_void()\nSearches for pairwise empty spaces across all continuous variables in the dataset to identify problems with interpretation or prediction\n\n\ndata_pcor()\nCalculates and plots the pairwise patrial-correlations for all continuous variables in the dataset\n\n\ndata_inter()\nSearches for potential pairwise interactions between variables in the dataset to identify relationships or dependencies that may be useful for modelling\n\n\ndata_leverage()\nDetects outliers in the continuous explanatory variables (x-variables) as a group to highlight unusual observations\n\n\ndata_Ptrans_plot()\nPlots the response variable against various power transformations of the continuous x-variables to explore potential relationships and model suitability\n\n\n\n\n\n\nNext we give examples of the graphical functions.\n\ndata_plot()\nThe function data_plot plots all the variables of the data individually; It plots the continuous variable as histograms with a density plots superimposed, see the plot for rent and yearc. As an alternative a dot plots can be requested see for an example in ?@sec-data_response. For integers the function plots needle plots, see area below and for categorical the function plots bar plots, see location, bath kitchen and cheating below.\n\nda |&gt; data_plot()\n\n 100 % of data are saved, \nthat is, 3082 observations. \n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\nRemoved 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\n\nThe function could saves the ggplot2 figures.\nback to table\n\n\ndata_bucket()\nThe function data_bucket plots bucket plots (which identify skewness and kurosid) for all variables in the data set.\n\nda |&gt; data_bucket()\n\n 100 % of data are saved, \nthat is, 3082 observations. \n    rent     area    yearc location     bath  kitchen cheating \n    2723      132       68        3        2        2        2 \n\n\n\n\n\n\n\n\n\nback to table\n\n\ndata_response()\nThe function data_response() plots four different plots related to a continuous response variable. It plots; i) a histogram (and the density function); ii) a dot plot of the response in the original scale iii) a histogram (and the density function) of the response at the z-scores scale and vi) s dot-plot in the z-score scale.\nThe z-score scale is defined by fitting a distribution to the variable (normal or SHASH) and then taking the residuals, see also next section. The dot plots are good in identify highly skew variables and unusual observations. They display the median and inter quantile range of the data. The y-axis of a dot plot is a randomised uniform variable (therefore the plot could look slightly different each time.)\n\nda |&gt; data_response(, response=rent)\n\n 100 % of data are saved, \nthat is, 3082 observations. \nthe class of the response is numeric is this correct? \na continuous distribution on (0,inf) could be used \n\n\n\n\n\n\n\n\n\nback to table\n\n\ndata_zscores()\nOne could fit any four parameter (GAMLSS) distribution, defined on \\(-\\infty\\) to \\(\\infty\\), to any continuous variable, where skewness and kurtosis is suspected and take the quantile residuals (or z-scores) as the transformed values x-values. The function y_zscores() performs just this. It takes a continuous variable and fits a continuous four parameter distribution and gives the z-scores. The fitting distribution can be specified by the user, but as default we use the SHASHo distribution. Below we demonstrate that the function y_zscores() is equivalent to fitting the SHASHo distribution to a continuous variable and then take the quqntile residuals from the fitted model as the result.\n\nda |&gt; data_zscores()\n\n\n\n\n\n\n\n\nIn order to see how the z-scores are calculated consider the function y_zscores() which is taking an individual variables z-scores;\n\n z &lt;- y_zscores(rent99$rent, plot=FALSE)\n\nThe function is equivalent of fitting a constant model to all the parameters of a given distribution and than taking the quantile residuals (or z=scores) as he variable of interest. The default distribution is the four parameter SHASHo distribution.\n\nlibrary(gamlss2)\nm1 &lt;- gamlssML(rent99$rent, family=SHASHo) # fitting a 4 parameter distribution \ncbind(z,resid(m1))[1:5,]#  and taking the residuals\n\n          z          \n1 -2.496097 -2.496097\n2 -1.340464 -1.340464\n3 -1.182014 -1.182014\n4 -2.526326 -2.526326\n5 -2.295069 -2.295069\n\n\nback to table\n\n\ndata_xyplot()\nThe functions data_xyplot() plots the response variable against each of the independent explanatory variables. It plots the continuous against continuous as scatter plots and continuous variables against categorical as box plot.\n\n\n\n\n\n\nNote\n\n\n\nAt the moment there is no provision for categorical response variables.\n\n\n\nda |&gt; data_xyplot(response=rent )\n\n 100 % of data are saved, \nthat is, 3082 observations. \n\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nThe output of the function saves the ggplot2 figures.\nNote hat the package gamlss.prepdata do not provides pairwise plot of the explanatory variables themself but the package GGally does. here is an example ;\n\nlibrary(GGally) \n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\ndac &lt;- gamlss.prepdata:::data_only_continuous(da) \nggpairs(dac, lower=list(continuous = \"cor\",\n      combo = \"box_no_facet\", discrete = \"count\", \n      na = \"na\"),upper=list(continuous = \"points\", \n      combo = \"box_no_facet\", discrete = \"count\", na = \"na\")) + \ntheme_bw(base_size =15)\n\n\n\n\n\n\n\n\nback to table\n\n\ndata_cor()\nThe function data_corr() is taking a data.frame object and plot the correlation coefficients of all its continuous variables.\n\ndata_cor(da, lab=TRUE)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n4 factors have been omited from the data \n\n\nWarning in data_cor(da, lab = TRUE):\n\n\n\n\n\n\n\n\n\nA different type of plot can be produce if we use;\n\ndata_cor(da, method=\"circle\", circle.size = 40)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n4 factors have been omited from the data \n\n\nWarning in data_cor(da, method = \"circle\", circle.size = 40):\n\n\n\n\n\n\n\n\n\nTo get the variables with higher, say, than $0.4 $ correlation values use;\n\nTabcor &lt;- data_cor(da, plot=FALSE)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n4 factors have been omited from the data \n\n\nWarning in data_cor(da, plot = FALSE):\n\nhigh_val(Tabcor, val=0.4)\n\n     name1  name2  corrs  \n[1,] \"rent\" \"area\" \"0.585\"\n\n\nWe can plot the path of those variables using the package corr;\n\nlibrary(corrr)\nnetwork_plot(Tabcor)\n\n\n\n\n\n\n\n\nback to table\n\n\ndata_association()\nThe function data_association() is taking a data.frame object and plot the pair-wise association of all its variables. The pair-wise association for two continuous variables is given by default by the absolute value of the Spearman’s correlation coefficient. For two categorical variables by the (adjusted for bias) Cramers’ v-coefficient. For a continuous against a categorical variables by the \\(\\sqrt R^2\\) obtained by regressing the continuous variable against the categorical variable.\n\ndata_association(da, lab=TRUE)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n\n\n\n\n\n\n\n\n\nA different type of plot can be produce if we use;\n\ndata_association(da, method=\"circle\", circle.size = 40)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n\n\n\n\n\n\n\n\n\nTo get the variables with higher, say, than $0.4 $ association values use;\n\nTabcor &lt;- data_cor(da, plot=FALSE)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n4 factors have been omited from the data \n\n\nWarning in data_cor(da, plot = FALSE):\n\nhigh_val(Tabcor, val=0.4)\n\n     name1  name2  corrs  \n[1,] \"rent\" \"area\" \"0.585\"\n\n\nA different plot can be acheived using the funtion network_plot() of package corr;\n\npp=gamlss.prepdata:::data_association(rent[,-c(4,5,8)], plot=F)\n\n 100 % of data are saved, \nthat is, 1969 observations. \n\nlibrary(corrr)\nnetwork_plot(pp, min_cor = 0, colors = c(\"red\", \"green\"), legend = \"range\")\n\n\n\n\n\n\n\n\nback to table\n\n\ndata_void()\n\n\n\n\n\n\nWarning\n\n\n\nThis function is new. Its theoretical foundation are not proven yet. The function needs testing and therefore it should be used with causion.\n\n\nThe idea behind the functions void() and its equivalent data.frame version data_void() is to be able to identify whether the data in the direction of two continuous variables say \\(x_i\\) and \\(x_j\\) have a lot of empty spaces. The reason is that empty spaces effect prediction since interpolation at empty spaces is dengerous. The function data_void() is taking a data.frame object and plot the percentage of empty spaces for all pair-wise continuous variables. The function used the foreach() function of the package foreach to allow parallel processing.\n\nregisterDoParallel(cores = 9)\ndata_void(da)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n4 factors have been omited from the data \n\n\nWarning in data_void(da):\n\n\n\n\n\n\n\n\n\nA different type of plot can be produce if we use;\n\ndata_void(da, method=\"circle\", circle.size = 40)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n4 factors have been omited from the data \n\n\nWarning in data_void(da, method = \"circle\", circle.size = 40):\n\n\n\n\n\n\n\n\nstopImplicitCluster()\n\nTo get the variables with highter than $0.4 $ values use;\n\nTabvoid &lt;- data_void(da, plot=FALSE)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n4 factors have been omited from the data \n\n\nWarning in data_void(da, plot = FALSE):\n\nhigh_val(Tabvoid, val=0.4)\n\n     name1  name2   corrs  \n[1,] \"rent\" \"area\"  \"0.661\"\n[2,] \"rent\" \"yearc\" \"0.627\"\n[3,] \"area\" \"yearc\" \"0.51\" \n\n\nTo plot their paths use;\n\nlibrary(corrr)\nnetwork_plot(Tabvoid)\n\n\n\n\n\n\n\n\nback to table\n\n\ndata_pcor()\nThe function data_copr() is taking a data.frame object and plot the partial correlation coefficients of all its continuous variables.\n\ndata_pcor(da, lab=TRUE)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n4 factors have been omited from the data \n\n\nWarning in data_pcor(da, lab = TRUE):\n\n\n\n\n\n\n\n\n\nTo get the variables with highter than $0.4 $ partial correlation values use;\n\nTabpcor &lt;- data_pcor(da, plot=FALSE)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n4 factors have been omited from the data \n\n\nWarning in data_pcor(da, plot = FALSE):\n\nhigh_val(Tabpcor, val=0.4)\n\n     name1  name2  corrs  \n[1,] \"rent\" \"area\" \"0.638\"\n\n\nFor plotting you can use;\n\nlibrary(corrr)\nnetwork_plot(Tabpcor)\n\n\n\n\n\n\n\n\nback to table\n\n\ndata_inter()\nThe function data_inter() takes a data.frame, fits all pair-wise interactions of the explanatory variables against the response (using a normal model) and produce a graph displaying their significance levels. The idea behind this is to identify possible first order interactions at an early stage of the analysis.\n\nda |&gt; gamlss.prepdata:::data_inter(response= rent)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n\n\n\n\n\n\n\n\n\n\ntinter &lt;-gamlss.prepdata:::data_inter(da,  response= rent, plot=FALSE)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n\ntinter\n\n         area yearc location  bath kitchen cheating\narea       NA 0.014    0.001 0.119   0.000    0.000\nyearc      NA    NA    0.000 0.027   0.154    0.226\nlocation   NA    NA       NA 0.000   0.000    0.578\nbath       NA    NA       NA    NA   0.868    0.989\nkitchen    NA    NA       NA    NA      NA    0.719\ncheating   NA    NA       NA    NA      NA       NA\n\n\nTo get the variables with lower than $0.05 $ significant interactions use;\n\nTabinter &lt;- gamlss.prepdata:::data_inter(da, response= rent, plot=FALSE)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n\nlow_val(Tabinter)\n\n     name1      name2      value  \n[1,] \"area\"     \"yearc\"    \"0.014\"\n[2,] \"area\"     \"location\" \"0.001\"\n[3,] \"yearc\"    \"location\" \"0\"    \n[4,] \"yearc\"    \"bath\"     \"0.027\"\n[5,] \"location\" \"bath\"     \"0\"    \n[6,] \"area\"     \"kitchen\"  \"0\"    \n[7,] \"location\" \"kitchen\"  \"0\"    \n[8,] \"area\"     \"cheating\" \"0\"    \n\n\nback to table\nNOT DOCUMENTED IN HELP FOR THIS FUNCTION\n\n\ndata_leverage()\nThe function data_leverage() uses the linear model methodology to identify posible unusual observations within the explanatory variables as a group (not indivisually). It fit a linear (normal) model with response, the response of the data, and all explanatory variables in the data, as x’s. It then calculates the leverage points and plots them. A leverage is a number between zero and 1. Large leverage correspond to extremes in the x’s.\n\nrent99[, -c(2,9)] |&gt; data_leverage( response=rent)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe horizontal line is the plot is at point \\(2 \\times (r/n)\\), which is the threshold suggested in the literature; values beyond this point could be identified as extremes. It looks that the point \\(2 \\times (r/n)\\) is too low (at least for our data). Instead the plot identify only observation which are in the one per cent upper quantile. That is a quantile value of the leverage at value 0.99.\n\n\nSection 6.1.3 show how the information of the function data_leverage() can be combined with the information given by data_outliers() in order to confirm hight outliers in the data.\n\n\n\n\n\n\nImportant\n\n\n\nWhat happen if multiple response are in the data?\n\n\nback to table\n\n\ndata_Ptrans_plot()\nThe essence of this plot is to visually decide whether certain values of \\(\\lambda\\) in a power transformation are appropriate or not. Section 6.3 describes the power transformation, \\(T=X^\\lambda\\) for \\(\\lambda&gt;0\\), in more details. For each continuous explanatory variable \\(X\\), the function show the response against \\(X\\), (\\(\\lambda=1\\)), the response against the square root of \\(X\\), (\\(\\lambda=1/2\\)) and the response against the log of \\(X\\) (\\(\\lambda \\rightarrow 0\\)). The user then can decide whether any of those transformation are appropriate.\n\ngamlss.prepdata:::data_Ptrans_plot(da, rent) \n\n\n\n\n\n\n\n\nIt look that no transformation is needed for the continuous explanatory variables area of yearc.\nback to table"
  },
  {
    "objectID": "gamlss_prepdata.html#sec-Features",
    "href": "gamlss_prepdata.html#sec-Features",
    "title": "The Functions in the package gamlss.prepdata",
    "section": "Features",
    "text": "Features\nThe features functions are functions to help with the continuous explanatory x-variables; We divide them here in four sections; Functions for\n\noutliers;\nscaling;\ntransformations and\nfunction approproate for time series\n\nThe available functions are;\n\n\n\nTable 3: The outliers, scalling, transformation and time series functions\n\n\n\n\n\n\n\n\n\nFunctions\nUsage\n\n\n\n\ny_outliers()\nidentify possible outliers in a continuous variable (see below)\n\n\ndata_outliers()\nidentify possible outliers for all continuous x-variable (see also data_zscores in Section 5.0.4)\n\n\ndata_leverage()\nthis is a repear of the function from Section 5.0.11\n\n\ndata_scale()\nscaling all continuous x-variables either to zero mean and one s.d. or to the range [0,1]\n\n\nxy_Ptrans()\nlooking for appropriate power transformation for response against one of the x-variable\n\n\ndata_Ptrans()\nlooking for appropriate power transformation for response against all x-variable\n\n\ndata_Ptrans_plot()\nplotting all x’s agains the response using identiti ty square root ans log transormations\n\n\n\n\n\n\n\nOutliers\nOutliers in the response variable, within a distributional regression framework like GAMLSS, are better handled by selecting an appropriate distribution for the response. For example, an outlier under the assumption of a normal distribution for the response may no longer be considered an outlier if a distribution from the t-family is assumed. The function data_response(), discussed in Section 5.0.3 provides guidelines on appropriate actions. This section focuses on outliers in the explanatory variables.\nOutliers in the continuous explanatory variables can potentially affect curve fitting, whether using linear or smooth non-parametric terms. In such cases, removing outliers from the x-variables can make the model more robust. Outliers are observations that deviate significantly from the rest of the data, but the concept depends on the dimension being considered. An single observation value might be an outlier within an explanatory variable. A pair observation might be an outlier examining within a pair-wise relationships. In a one-dimensional search, we identify extreme observations in individual continuous variables, based on how far they lie from the majority of the data. In a two-dimensional search, we look for outliers in pairwise plots of continuous variables, examining how observations deviate from typical relationships between variable pairs. Leverage points are useful to identify outliers when we look for outliers in \\(r\\) dimension when \\(r\\) is the number of explanatory variables.\n\n\n\n\n\n\nNote\n\n\n\nAt this preliminary stage of the analysis, where no model has yet been fitted, it is difficult to identify outliers among factors. This is because outliers in factors typically do not appear unusual when examining individual variables. Their outlier status usually becomes apparent only when considered in combination with other variables or factors. Identifying such outliers is a task better suited for the modelling stage of the analysis.\n\n\nThe function y_outliers() in Section 6.1.1 uses a simple algorithm to identify potential one-dimensional outliers in continuous explanatory variables. It is designed specifically for continuous variables.\n\nFor variables defined on the full real line, (i.e., from \\(-\\infty\\) to \\(+\\infty\\), the algorithm fits a parametric distribution—by default, the four-parameter SHASHo distribution—and computes z-scores (quantile residuals) to assess outlier status.\nFor variables defined on the positive real line (i.e., from \\(0\\) to \\(+ \\infty\\)), an additional transformation step is included. These variables are often highly right-skewed, and extreme values in the right tail can dominate any outlier detection process. To address this, the function attempts to find a power transformation \\(T = X^\\lambda\\) that minimizes the Jarque-Bera test statistic—a measure of deviation from normality. To find the optimal power parameters \\(\\lambda\\) the function y_outliers() uses the internal function x_Ptrans(). After this transformation, the SHASHo distribution is fitted, and z-scores are computed. The Jarque-Bera test statistic is always non-negative; the further it is from zero, the stronger the evidence against normality. The goal is to reduce skewness and kurtosis in the data before assessing outliers.\n\nOutliers are identify for observations with very low or hight z-scores. For example, if a z-scores is greater in absolute value to a specified value say 3. The methodology is appropriate for identifying outliers in one dimension. Pair-wise scatter plots of all continuous variables in the data can be use to identify outliers in two dimensions. To identify outliers in \\(p\\) dimensions the function data_leverage(), see Section 6.1.3, can be used.\n\n\n\n\n\n\nNote\n\n\n\nSeveral of the graphical functions described in Section 5 are good for identify visualy outliers. In fact we recommend that the function data_outliers() should be used in combination with functions like data_plot() or data_zscores().\n\n\n\ny_outliers()\nThe function y_outliers() identify outliers in one \\(X\\) variable.\n\ny_outliers(da$rent)\n\nnamed numeric(0)\n\n\nNote that the y_outliers() is used by data_outliers() to identify outliers in all continuous variables in the data.\nback to table\n\n\ndata_outliers()\nThe function data_outliers() uses the z-scores technique described above in order to detect outliers in the continuous variables in the data. It fits a SHASHo distribution to the continuous variable in the data and uses the z-scores (quantile residuals) to identify (one dimension) outliers for those continuous variables.\n\ndata_outliers(da)\n\n    rent     area    yearc location     bath  kitchen cheating \n    2723      132       68        3        2        2        2 \n\n\n$rent\nnamed numeric(0)\n\n$area\nnamed integer(0)\n\n$yearc\nnamed numeric(0)\n\n\nAs it happen no individual variable outliers were highlighted in the rent data using the z-scores methodology. In general we recommend the function data_outliers() to be used in combination of the graphical functions data_plot() and data_zscores()\nback to table\n\n\ndata_leverage() (repeat)\nThe function data_leverage(), fisrt describe in Section 5.0.11, can be used to detect extremes in the continuous variables in the data by fiting a linear model with all the continuous variables in the data and then useing the higher leverage points to identify (multi-dimension) outliers for the observations for all continuous variables. By default, it identifies one percent of the observations with high leverage in the data. Here we use the function as a way to identify the obsrvation (not plottoing).\n\ndata_leverage(da, response=rent, plot=FALSE)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n\n\n [1]   38  162  176  204  206  242  454  496  541  651  696  794  910  942 1094\n[16] 1296 1384 1388 1782 1837 1875 1907 1970 2530 2540 2727 2810 2890 2977 3065\n[31] 3070\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe functiondata_leverage() always identifies the top one percent of observations with the highest leverage, which may or may not be outliers. These high-leverage points can have a strong influence on model estimates. To better assess their impact, the list of high-leverage observations returned by data_leverage() should be compared with the list of outliers identified by the data_outliers() function. Observations that appear in both lists are particularly noteworthy, as they may be influential outliers that warrant further investigation.\n\n\nNext we are trying the proceess of identifing if data with high leverage are also coincide with outliers identified using the function data_outliers() function. The function intersect() will flash out the intersection of the two set of observations. First use data_outliers()\n\nul &lt;- unlist(data_outliers(da))\n\n    rent     area    yearc location     bath  kitchen cheating \n    2723      132       68        3        2        2        2 \n\n\nthen data_leverage();\n\nll&lt;-data_leverage(da, response=rent, plot=FALSE)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n\n\nand finally we inspect the intersection of the two sets using intersect();\n\nintersect(ll,ul)\n\nnumeric(0)\n\n\nHere we have zero outliers but in general we would expect to find common observations for further checking.\nback to table\n\n\n\nScaling\nScaling is another form of transformation for the continuous explanatory variables in the data which brings the explanatory variables in the same scale. It is a form of standardization. Standardization means bringing all continuous explanatory variables to similar range of values. For some machine learning techniques, i.e., principal component regression or neural networks standardization is mandatory in other like LASSO is recommended. There are two types of standardisation\n\nscaling to normality and\nscaling to a range from zero to one.\n\nScaling to normality it means that the variable should have a mean zero and a standard deviation one. Note that, this is equivalent of fitting a Normal distribution to the variables and then taking the z-scores (residuals) of the fit as the transformed variable. The problem with this type of scaling is that skewness and kurtosis in the standardised data persist. One could go further and fit a four parameter distribution instead of a normal where the two extra parameters could account for skewness and kurtosis. The function data_scale() described in Section 6.2.1 it gives this option with te argument family in which can used to specify a different distribution.\n\ndata_scale()\nThe function data_scale() perform scaling to all continuous variables an a data set. Note that factors in the data set are left untouched because when fitted within a model they will be transform to dummy variable which take values 0 or 1 (a kind of standardization). The response is left also untouched because it is assumed that an appropriate distribution will be fitted. The response variable has to be declared using the argument response. First we use data_scale() to scale to normality.\n\nhead(data_scale(da, response=rent))\n\n    area    yearc location     bath  kitchen cheating \n     132       68        3        2        2        2 \n\n\n      rent       area      yearc location bath kitchen cheating\n1 109.9487  0.7009962  0.7035881        2    0       0        0\n2 243.2820 -0.4796117  0.7484206        2    0       0        1\n3 261.6410 -0.2266243  0.6587556        1    0       0        1\n4 106.4103 -0.3531180 -1.2242096        2    0       0        0\n5 133.3846  0.1950214 -1.7173671        2    0       0        1\n6 339.0256 -0.5639408  1.6450707        2    0       0        1\n\n\nAs we mention before scaling to normality is equivalent of fitting a Normal variable first. The problem with scaling to normality is that if the variables are highly skew or kurtotic scaling them to normality does correct for skewness or kurtosis. Using a parametric distribution with four parameters some of which are skewness and kurtosis parameters it may correct that. Next we standardised using the SHASHo distribution;\n\nhead(data_scale(da, response=rent,  family=\"SHASHo\"))\n\n    area    yearc location     bath  kitchen cheating \n     132       68        3        2        2        2 \n\n\n      rent       area      yearc location bath kitchen cheating\n1 109.9487  0.7390434  0.6323846        2    0       0        0\n2 243.2820 -0.3940193  0.6795499        2    0       0        1\n3 261.6410 -0.1175129  0.5864434        1    0       0        1\n4 106.4103 -0.2526941 -1.0598183        2    0       0        0\n5 133.3846  0.2949483 -1.6281459        2    0       0        1\n6 339.0256 -0.4916986  2.0610405        2    0       0        1\n\n\nThe second form od standardisation is to transform all continuous x-variables to a range zero to one. Here is how this is done;\n\nhead(data_scale(da, response=rent,  scale.to=\"0to1\"))\n\n    area    yearc location     bath  kitchen cheating \n     132       68        3        2        2        2 \n\n\n      rent       area yearc location bath kitchen cheating\n1 109.9487 0.04285714     0        2    0       0        0\n2 243.2820 0.05714286     0        2    0       0        1\n3 261.6410 0.07142857     0        1    0       0        1\n4 106.4103 0.07142857     0        2    0       0        0\n5 133.3846 0.07142857     0        2    0       0        1\n6 339.0256 0.07142857     0        2    0       0        1\n\n\nback to table\n\n\n\nTransformations\nIn the context of outliers, the focus on how far a single observation deviates from the rest of the data. However, when considering transformations, the goal shifts toward a better capturing of the relationship between a continuous predictor and the response variable. Specifically, we aim to model peaks and troughs in the relationship between a single x-variable and the response better. To achieve this, it is sometimes sufficient to stretch the x-axis so that the predictor variable is more evenly distributed within its its range. A common approach for this is to use a power transformation of the form \\(T = X^\\lambda\\). This transoformation can reduce the curvature in the response and make the curve fitting easier. Importantly, the power transformation smoothly transitions into a logarithmic transformation as \\(\\lambda \\to 0\\), since: \\(\\frac{X^{\\lambda} - 1} {\\lambda} \\rightarrow \\log(X) \\quad \\text{as } \\lambda \\to 0\\). Thus, in the limit as \\(\\lambda \\to 0\\) is the power traformation is the log transformation \\(T = \\log(X)\\).\n\n\n\n\n\n\n\nNote\n\n\n\nThe power transformation is a subclass of the shifted power transformation\n\\(T = (\\alpha + X)^\\lambda\\) which is not consider here\n\n\nThe aim of the power transformation is to make the model fitting process more robust and reliable and maximizing the information extracted from the data. There are three different functions in gamlss.prepdata dealling with power transformations, xy_Ptrans(), data_Ptrans() and data_Ptrans_plot()\n\n\n\n\n\n\nNote\n\n\n\nIt’s important to distinguish between the concepts of transformations (the subject of this section) and of feature extraction, a term often used in machine learning. We refer to transformations as functions applied to a individual explanatory variables while we use feature extraction when multiple explanatory variables are involved. In both cases, the goal is to enhance the model capabilities.\n\n\n\nxy_Ptrans()\n\ngamlss.prepdata:::xy_Ptrans(da$area, da$rent) \n\n\ngamlss.prepdata:::xy_Ptrans(da$area, da$rent, prof=TRUE, k=4) \n\n\n\n\n\n\n\n\n\n\ndata_Ptrans()\n\ngamlss.prepdata:::data_Ptrans(da, response=rent)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n\n\n$area\n[1] 1.06212\n\n$yearc\n[1] 1.499942\n\n\nback to table\n\n\ndata_Ptrans_plot() (repeat)\n\ngamlss.prepdata:::data_Ptrans_plot(da, rent) \n\n\n\n\n\n\n\n\nback to table\n\n\n\nTime series\nTime series data sets consist of observations collected at consistent time intervals—e.g., daily, weekly, or monthly. These data require special treatment because observations that are closer together in time tend to be more similar, violating the usual assumption of independence between observations. A similar issue arises in spatial data sets, where observations that are geographically closer often exhibit spatial correlation, again violating independence.\nMany data sets include spatial or temporal features —such as longitude and latitude or timestamps (e.g., 12/10/2012)— those variables are can be used for for modeling directly, but also for interpreting or visualization of the results. If temporal or spatial information exists in the data, it is essential to ensure these features are correctly read and interpreted in R. For instance, dates in R can be handled using the as.Date() function. To understand its functionality and options associated with the function please consult the help documentation via help(\"as.Date\"). Below, we provide two simple functions of how to work with date-time variables.\n\ntime_dt2dhour()\nSuppose the variable dt contains both a date and a time. We may want to extract and separate this into two components: one containing the date, and another capturing the hour of the day.\n\ndt &lt;- c(\"01/01/2011 00:00\", \"01/01/2011 01:00\", \"01/01/2011 02:00\", \"01/01/2011 03:00\", \"01/01/2011 04:00\", \"01/01/2011 05:00\") \n\nThe function to do this separation can lok like;\n\ntime_dt2dhour &lt;- function(datetime, format=NULL) \n  { \n          X &lt;- t(as.data.frame(strsplit(datetime,' '))) \nrownames(X) &lt;- NULL \ncolnames(X) &lt;- c(\"date\", \"time\") \n       hour &lt;- as.numeric(sub(\":\",\".\",X[,2])) \n      date &lt;- as.Date(X[,1],format=format) \n      data.frame(date, hour) \n  } \n\n\nP &lt;- time_dt2dhour(dt, \"%d/%m/%Y\") \nP\n\n        date hour\n1 2011-01-01    0\n2 2011-01-01    1\n3 2011-01-01    2\n4 2011-01-01    3\n5 2011-01-01    4\n6 2011-01-01    5\n\n\n\n\ntime_2num()\nThe second example contains time given in its common format, c(\"12:35\", \"10:50\") and we would like t change it to numeric which we can use in the model.\n\nt &lt;- c(\"12:35\", \"10:50\")\n\nThe function to change this to numeric;\n\ntime_2num &lt;- function(time, pattern=\":\") \n  { \n  t &lt;- gsub(pattern, \".\", time) \n   as.numeric(t) \n  } \ntime_2num(t) \n\n[1] 12.35 10.50\n\n\nFor user who deal with a lot of time series try the function POSXct() and also the function select() from the dplur package."
  },
  {
    "objectID": "gamlss_prepdata.html#sec-Partition",
    "href": "gamlss_prepdata.html#sec-Partition",
    "title": "The Functions in the package gamlss.prepdata",
    "section": "Data Partition",
    "text": "Data Partition\n\nIntrodunction to data partition\nPartitioning the data is essential for comparing different models and checking for overfitting. Figure Figure 1 illustrates the different ways the data can be split. A dataset can be split multiple times or just once, depending on the size of the dataset. For large datasets, it is common to split the data into a training set and a test set, and, if necessary, a validation set. Observations in the training set are referred to as in-bag, while observations in the test or validation set are termedout-of-bag`.\nThe training set is used for fitting the model, the test set is used for prediction and comparison between models, and the validation set is used for tuning the model’s hyperparameters. In additive smoothing regression models, for instance, the hyperparameters refer to the smoothing parameters in the model. Data partitioning allows for performance evaluation, model assumption checks, and detection of overfitting.\nIf the fitted model performs reasonably well on both the training and test sets, one can be confident that overfitting has been avoided. Overfitting occurs when a model fails to generalize well to new data, typically because it is too closely fitted to the sample data. In contrast, underfitting refers to a poor model that fails to capture the essential patterns in the data, resulting in a model that is distant from both the sample and the population it is intended to represent.\n\nVarious goodness-of-fit measures can be used to check and compare different distributional regression models. One of the most popular methods is the Generalized Akaike Information Criterion (GAIC). To calculate the GAIC, only the in-bag data are required, meaning there is no need for data partitioning. However, calculating the GAIC requires an accurate measure of the degrees of freedom used to fit the model. In mathematical (stochastic) models, the degrees of freedom are straightforward to define as they correspond to the number of estimated parameters. In contrast, for algorithmic models, particularly over-parameterized ones like neural networks, estimating degrees of freedom is more challenging. In mathematical models, the degrees of freedom serve as a measure of the model’s complexity. More complex models are typically closer to the training data, which may reduce their ability to generalize well to new data.\nIn addition to GAIC, other commonly used goodness-of-fit measures for regression models include the deviance, \\(R^2\\), and mean squared error (MSE), among others. However, these measures are prone to overfitting when evaluated on the in-bag dataset rather than on the out-of-bag data. Furthermore, \\(R^2\\) may not generalize well to non-normal error distributions, and MSE (and its variations) is often inappropriate for distributional regression models. This is because MSE focuses primarily on the location parameters of the response distribution, while other aspects of the response distribution are often of greater interest to practitioners.\n\n\n\n\n\n\n\nflowchart TB\n  A[Data] --&gt; B{Multi-split} \n  A --&gt; C{Split once}\n  C --&gt; D[Training]\n  D --&gt; E[Validate]\n  E --&gt; O[Test]\n  B --&gt; F(Rearranged)\n  B --&gt; G(Reweighed)\n  G--&gt; H(Bayesian \\n Bootstrap)\n  G--&gt; J(Boosting)\n  F --&gt; K(Non-param. \\n Bootstrap)\n  F --&gt; L(Cross \\n Validation)\n  L --&gt; M(k-Fold) \n  L --&gt; N(LOO) \n\n\n\n\nFigure 1: Different ways of splitting data to get in order to get more information”\n\n\n\n\n\nPartitioning the data into training, test, and validation sets should be done randomly. A potential risk when splitting the data only once arises when the investigation focuses on the tails of the response distribution. In such cases, the few extreme observations may end up in only one of the partitions, leading to potential issues when evaluating the model. Repeatedly reusing parts of the data, such as through bootstrapping or cross-validation, can help mitigate this risk. Multi-splitting the data (shown on the left side of Figure Figure 1) enables the evaluation of appropriate goodness-of-fit measures using out-of-bag data. There are two conceptual ways to multi-split a dataset: the first involves rearranging the data, and the second involves reweighting the observations. [As we will see later, the distinction between these two approaches is not always as clear-cut.] Rearranging the data is typically accomplished by creating appropriate indices that select different parts of the data. This leads to methods such as bootstrapping or cross-validation. On the other hand, reweighting the observations during the fitting process is a technique employed in the Bayesian Bootstrap and boosting.\n\nIn classical bootstrapping, the data are re-sampled randomly with replacement \\(B\\) times, and the model is refitted the same number of times. While computationally expensive, this process provides additional insights into the model, particularly regarding the variability of its parameters. For distributional regression models, both classical and Bayesian bootstrapping offer extra information about all the parameters in the model—namely, the distributional parameters, fitted coefficients, hyperparameters, and random effects. The Bayesian bootstrap re-weights the data as samples from a multinomial distribution and fits the re-weighted models \\(B\\) times. Like the classical bootstrap, it provides valuable information about the variability of the model’s parameters. In boosting, each weak learner (i.e., each simple fitted model) is reweighted using the residuals from the previous fits. The results of all these learners are then aggregated into the final model. Boosting can be seen as a method of iteratively improving model accuracy by focusing on the previously mispredicted observations.\n\n\nIn \\(K\\)-fold cross-validation, the data are split into \\(K\\) sub-samples. The model is then fitted \\(k\\) times, each time using \\(k-1\\) sub-samples for training and the remaining one sub-sample as the test set. By the end of the process, all \\(n\\) observations will have been used as test data exactly once. The main advantage of \\(K\\)-fold cross-validation is that it provides reliable test data for model evaluation with the additional cost of fitting \\(K\\) models. Leave-One-Out Cross-Validation (LOO) is a special case of k-fold cross-validation, where the model is trained on \\(n-1\\) observations and tested on the remaining single observation, iterating this process \\(n\\) times. Consequently, \\(n\\) different models are refitted. While computationally intensive, LOO ensures that each data point is used for testing, providing an unbiased evaluation of model performance. Both bootstrapping and \\(K\\)-fold cross-validation, when appropriate performance metrics are used, can help in several ways: i) by facilitating model comparison on the test data, ii) by avoiding overfitting, and iii) by assisting in the selection of hyperparameters.\n\nNotice that in distributional regression models, there is no inherent need to physically partition the original data into subgroups when performing a single or multiple partitions. The same results can be achieved by indexing the original data.frame or by using prior weights during the model fitting process. For example, in \\(K\\)-fold cross-validation, we need \\(K\\) dummy vectors, \\(i_k\\), where $k = 1, , K. These dummy vectors take the value of 1 for the \\(k\\)-th set of observations and \\(0\\) for the rest. The dummy vectors \\(i_k\\) can be used in two ways:\n\nTo select the observations from the data.frame for fitting.\nAs prior weights when fitting the model.\n\nLet da represent the data.frame used to fit the model, and \\(i\\) the \\(i_k\\) dummy vector. For each of the \\(K\\) fits, the model fitting process would use the command \\(data=da[,i]\\) to select the appropriate data, and newdata=da[,i==0] for evaluating the out-of-bag measure. Alternatively, one can use weights=i during the model fitting and newdata=da[,i==0] for evaluating any out-of-bag measures. Note that the appropriate measures for comparing models include predictive deviance (PD) and continuous rank probability scores (CRPS). The function data_Kfold() in the package generates the correct dummy vectors as a matrix of dimensions n B, which can be used for the cross-validation process.\n\nFor the classical bootstrap, we need \\(B\\) vectors. Unlike cross-validation (CV), where the vectors for indexing and prior weights are identical, bootstrapping requires different vectors. The function data_boot_index() creates the appropriate vectors for indexing, while data_boot_weights() generates the vectors for prior weighting. These vectors should differ whether used as indices to select the relevant columns from the data matrix da or as prior weights, which indicate how many times each observation is selected. Typically, a vector like \\((1, 0, 2, \\ldots, 3, 2)\\) means that observation \\(1\\) is picked once, observation \\(2\\) is excluded from the fit, observation \\(3\\) is selected twice, and so on. For Bayesian bootstrap, the \\(B\\) vectors of length \\(n\\) represent weights that sum to \\(n\\). An out-of-bag observation, in this case, is one with zero weight. This distinction makes the split between rearranged and reweighed in Figure 1 somewhat artificial, as both methods can be interpreted as refitting approaches using prior weights.\n\n\n\n\n\n\nTable 4: The data partition functions\n\n\n\n\n\n\n\n\n\nFunctions\nUsage\n\n\n\n\ndata_part()\nCreates a single or multiple (CV) partitions by introducing a factor with different levels\n\n\ndata_part_list()\nCreates a single or multiple (CV) partitions with output a list of data.frames\n\n\ndata_boot_index()\nCreates two lists. The in-bag,IB, indices for fitting and the out-of-bag, OOB, indices for prediction\n\n\ndata_boot_weights()\nCreate a \\(n \\times B\\) matrix with columns weights for bootstrap fits\n\n\ndata_Kfold_index()\nCreates a \\(n \\times K\\) matrix of variables to be used for cross validation data indexing\n\n\ndata_Kfold_weights()\nCreates a \\(n \\times K\\) matrix of dummy variables to be used for cross validation weighted fits\n\n\ndata_cut()\nThis is not a partition funtion but randomly select a specified porpotion of the data\n\n\n\n\n\n\nHere are the data partition functions.\n\ndata_part()\nThe function data_part() it does not partitioning the data as such but adds a factor called partition to the data indicating the partition. By default the data are partitioned into two sets the training ,train, and the test data, test.\n\ndap &lt;- gamlss.prepdata:::data_part(da)\n\ndata partition into two sets \n\nhead(dap)\n\n      rent area yearc location bath kitchen cheating partition\n1 109.9487   26  1918        2    0       0        0     train\n2 243.2820   28  1918        2    0       0        1      test\n3 261.6410   30  1918        1    0       0        1     train\n4 106.4103   30  1918        2    0       0        0      test\n5 133.3846   30  1918        2    0       0        1      test\n6 339.0256   30  1918        2    0       0        1     train\n\n\nIf the user would like to split the data in two separate data.frame’s she/he can use;\n\ndaTrain &lt;- subset(dap, partition==\"train\")\ndaTest &lt;- subset(dap, partition==\"test\")\ndim(daTrain)\n\n[1] 1846    8\n\ndim(daTest)\n\n[1] 1236    8\n\n\nNote, that use of the option partition has the following behaviour;\n\npartition=2L (the default) the factor has two levels train, and test.\npartition=3L the factor has three levels train, val (for validation) and test.\npartition &gt; 4L say K then the levels are 1, 2…K. The factor then can be used to identify K-fold cross validation, see also the function data_Kfols_CV().\n\nback to table\n\n\ndata_part_list()\nThe function data_part_list() creates a list of dataframes which can be used either for single partition or for cross validation. The argument partition allows the splitting of the data to up to 20 subsets. The default is a list of 2 with elements training and test (single partition).\nHere is a single partition in train and test data sets.\n\nallda &lt;-  data_part_list(rent) \nlength(allda)\n\n[1] 2\n\ndim(allda[[\"train\"]]) # training data\n\n[1] 1200    9\n\ndim(allda[[\"test\"]]) # test data\n\n[1] 769   9\n\n\nHere is a multiple partition for cross validation.\n\nallda &lt;-  data_part_list(rent, partition=10) \n\n10-fold data partition \n\nlength(allda)\n\n[1] 10\n\nnames(allda)\n\n [1] \"1\"  \"2\"  \"3\"  \"4\"  \"5\"  \"6\"  \"7\"  \"8\"  \"9\"  \"10\"\n\n\nback to table\n\n\ndata_boot_index()\nThe function data_boot_index() create two lists. The first called IB (for in-bag) and the second OOB, (out-of-bag). Each ellement of the list IB can be use to select a bootstrap sample from the original data.frame while each element of OOB can be use to select the out of sample data points. Note that each bootstap sample approximately contains 2/3 of the original data so we expact on avarage to 1/3 to be in OOB sample.\n\nDD &lt;- data_boot_index(rent, B=10)\n\nThe class, length and names of the created lists;\n\nclass(DD)\n\n[1] \"list\"\n\nlength(DD)\n\n[1] 2\n\nnames(DD)\n\n[1] \"IB\"  \"OOB\"\n\n\nthe first obsevations of the\n\nhead(DD$IB[[1]]) # in bag\n\n[1] 1 2 4 6 8 8\n\nhead(DD$OOB[[1]]) # out-of-bag\n\n[1]  3  5  7 15 21 22\n\n\nback to table\n\n\ndata_boot_weights()\nThe function data_boot_weights() create a \\(n \\times B\\). matrix with columns possible weights for bootstap fits. Note that each bootstap sample approximately contains .666 of the original obsbation so we in general we expect 0.333 of the data to have zero weights.\n\nMM &lt;- data_boot_weights(rent, B=10)\n\nThe M is a matrix which columns can be used as weights in bootstrap fits; Here is a plot of the first column\n\nplot(MM[,1])\n\n\n\n\n\n\n\n\nand here is the first 6 rows os the matrix;\n\nhead(MM)\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n[1,]    1    2    2    1    0    1    3    1    3     1\n[2,]    1    1    0    2    2    1    2    1    1     2\n[3,]    0    0    2    1    2    1    2    1    1     0\n[4,]    1    0    1    0    3    0    1    1    0     2\n[5,]    0    2    3    1    1    1    2    1    2     1\n[6,]    1    0    0    1    0    1    4    0    1     0\n\n\nback to table\n\n\ndata_Kfold_index()\nThe function data_Kfold() creates a matrix which columns can be use for cross validation either as indices to select data for fitting or as prior weights.\n\nPP &lt;- data_Kfold_index(rent, K=10)\nhead(PP)\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n[1,]    1    1    1    1    0    1    1    1    1     1\n[2,]    2    2    0    2    2    2    2    2    2     2\n[3,]    3    3    3    3    3    3    3    3    0     3\n[4,]    4    4    4    4    4    0    4    4    4     4\n[5,]    5    5    5    5    0    5    5    5    5     5\n[6,]    6    6    6    6    6    6    6    0    6     6\n\n\nback to table\n\n\ndata_Kfold_weights()\nThe function data_Kfold() creates a matrix which columns can be use for cross validation either as indices to select data for fitting or as prior weights.\n\nPP &lt;- data_Kfold_weights(rent, K=10)\nhead(PP)\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n[1,]    1    1    1    1    0    1    1    1    1     1\n[2,]    1    1    0    1    1    1    1    1    1     1\n[3,]    1    1    1    1    1    1    1    1    0     1\n[4,]    1    1    1    1    1    0    1    1    1     1\n[5,]    1    1    1    1    0    1    1    1    1     1\n[6,]    1    1    1    1    1    1    1    0    1     1\n\n\nback to table\n\n\ndata_cut()\nThe function data_cut() is not included in the partition Section for its data partitioning properties. It is designed to select a random subset of the data specifically for plotting purposes. This is especially useful when working with large datasets, where plotting routines—such as those from the ggplot2 package—can become very slow.\nThe data_cut() function can either: • Automatically reduce the data size based on the total number of observations, or • Subset the data according to a user-specified percentage.\nHere is an example of the function assuming that that the user requires \\(50%\\) of the data;\n\nda1 &lt;- data_cut(rent99, percentage=.5)\n\n 50 % of data are saved, \nthat is, 1541 observations. \n\ndim(rent99)\n\n[1] 3082    9\n\ndim(da1)\n\n[1] 1541    9\n\n\nThe function data_cut() is used extensively in many plotting routines within the gamlss.ggplots package. When the percentage option is not specified, a default rule is applied to determine the proportion of the data used for plotting. This approach balances performance and visualization quality when working with large datasets.\nLet \\(n\\) denote the total number of observations. Then:\n\nif \\(n \\le 50,000\\): All data are used for plotting.\nif \\(50,000 &lt; n \\le 100,000\\): 50% of the data are used.\nIf \\(100,000 &lt; n \\le 1,000,000\\): 20% of the data are used.\nIf \\(n&gt;1,000,000\\) : 10% of the data are used.\n\nThis default behaviour ensures that plotting remains efficient even with very large datasets.\nback to table"
  },
  {
    "objectID": "gamlss_prepdata.html#sec-Family",
    "href": "gamlss_prepdata.html#sec-Family",
    "title": "The Functions in the package gamlss.prepdata",
    "section": "Plotting Distribution Families",
    "text": "Plotting Distribution Families\n\n\n\n\n\n\nNote\n\n\n\nAll functions listed below belong to the gamlss.ggplots package, not the gamlss.prepdata package. However, since they can be useful during the pre-modeling stage, they are presented here. The later version of the gamlss.ggplots can be found in https://github.com/gamlss-dev/gamlss.ggplots\n\n\nIn the following function, no fitted model is required—only values for the parameters need to be specified.\n\nfamily_pdf()\nThe function family_pdf() plots individual probability density functions (PDFs) from distributions in the gamlss.family package. Although it typically requires the family argument, it defaults to the Normal distribution (NO) if none is provided.\n\nfamily_pdf(from=-5,to=5, mu=0, sigma=c(.5,1,2))\n\n\n\n\n\n\n\nFigure 2: Continuous response example: plotting the pdf of a normal random variable.\n\n\n\n\n\nThe following demonstrates how discrete distributions are displayed. We begin with a distribution that can take infinitely many count values—the Negative Binomial distribution;\n\nfamily_pdf(NBI, to=15, mu=1, sigma=c(.5,1,2), alpha=.9, size.seqment = 3)\n\n\n\n\n\n\n\nFigure 3: Count response example: plotting the pdf of a beta binomial.\n\n\n\n\n\nHere we use a discrete distribution with a finite range of possible values: the Beta-Binomial distribution.\n\nfamily_pdf(BB, to=15, mu=.5, sigma=c(.5,1,2),  alpha=.9, , size.seqment = 3)\n\n\n\n\n\n\n\nFigure 4: Beta binomial response example: plotting the pdf of a beta binomial.\n\n\n\n\n\n\n\nfamily_cdf\nThe function family_cdf() plots cumulative distribution functions (CDFs) from the gamlss.family distributions. The primary argument required is the family specifying the distribution to be used.\n\nfamily_cdf(NBI, to=15, mu=1, sigma=c(.5,1,2), alpha=.9, size.seqment = 3)\n\n\n\n\n\n\n\nFigure 5: Count response example: plotting the cdf of a negative binomial.\n\n\n\n\n\nThe CDf of the negative binomial;\n\nfamily_cdf(BB, to=15, mu=.5, sigma=c(.5,1,2),  alpha=.9, , size.seqment = 3)\n\n\n\n\n\n\n\nFigure 6: Count response example: plotting the cdf of a beta binomial.\n\n\n\n\n\n\n\nfamily_cor()\nThe function family_cor() offers a basic method for examining the inter-correlation among the parameters of any distribution from the gamlss.family. It performs the following steps:\n\nGenerates 10,000 random values from the specified distribution.\nFits the same distribution to the generated data.\nExtracts and plots the correlation coefficients of the distributional parameters.\n\nThese correlation coefficients are derived from the variance-covariance matrix of the fitted model.\n\n\n\n\n\n\nWarning\n\n\n\nThis method provides only a rough indication of how the parameters are correlated at specific values of the distribution’s parameters. The correlation structure may vary significantly at different points in the parameter space, as the distribution can behave quite differently depending on those values.\n\n\n\n#source(\"~/Dropbox/github/gamlss-ggplots/R/family_cor.R\")\ngamlss.ggplots:::family_cor(\"BCTo\", mu=1, sigma=0.11, nu=1, tau=5, no.sim=10000)\n\n\n\n\n\n\n\nFigure 7: Family correlation of a BCTo distribution at specified values of the parameters."
  },
  {
    "objectID": "gamlss-prepdata.html",
    "href": "gamlss-prepdata.html",
    "title": "The Functions in the Package gamlss.prepdata",
    "section": "",
    "text": "This booklet introduces the gamlss.prepdata package and its functionality. It aims to describe the available functions and how they can be used.\n\nThe latest versions of the packages gamlss, gamlss2 and gamlss.prepdata are shown below:\n\nrm(list=ls())\nlibrary(gamlss)\nlibrary(gamlss2)\nlibrary(ggplot2)\nlibrary(gamlss.ggplots)\nlibrary(gamlss.prepdata)\nlibrary(\"dplyr\") \npackageVersion(\"gamlss\")\n\n[1] '5.4.23'\n\npackageVersion(\"gamlss2\")\n\n[1] '0.1.0'\n\npackageVersion(\"gamlss.prepdata\")\n\n[1] '0.1.9'"
  },
  {
    "objectID": "gamlss-prepdata.html#sec-introduction",
    "href": "gamlss-prepdata.html#sec-introduction",
    "title": "The Functions in the Package gamlss.prepdata",
    "section": "",
    "text": "This booklet introduces the gamlss.prepdata package and its functionality. It aims to describe the available functions and how they can be used.\n\nThe latest versions of the packages gamlss, gamlss2 and gamlss.prepdata are shown below:\n\nrm(list=ls())\nlibrary(gamlss)\nlibrary(gamlss2)\nlibrary(ggplot2)\nlibrary(gamlss.ggplots)\nlibrary(gamlss.prepdata)\nlibrary(\"dplyr\") \npackageVersion(\"gamlss\")\n\n[1] '5.4.23'\n\npackageVersion(\"gamlss2\")\n\n[1] '0.1.0'\n\npackageVersion(\"gamlss.prepdata\")\n\n[1] '0.1.9'"
  },
  {
    "objectID": "gamlss-prepdata.html#introduction-to-the-gamlss.prepdata-package",
    "href": "gamlss-prepdata.html#introduction-to-the-gamlss.prepdata-package",
    "title": "The Functions in the Package gamlss.prepdata",
    "section": "Introduction to the gamlss.prepdata Package",
    "text": "Introduction to the gamlss.prepdata Package\nThe gamlss.prepdata package originated from the gamlss.ggplots package. As gamlss.ggplots became too large for easy maintenance, it was split into two separate packages, and gamlss.prepdata was created.\nSince gamlss.prepdata is still at an experimental stage, some functions are hidden to allow time for thorough checking and validation. These hidden functions can still be accessed using the triple colon notation, for example: gamlss.prepdata:::.\nThe functions available in gamlss.prepdata are intended for pre-fitting — that is, to be used before applying the gamlss() or gamlss2() fitting functions. The available functions can be grouped into the following categories:\n\nInformation functions\nThese functions provide information about:\n- The size of the dataset\n\n- The presence and extent of missing values\n\n- The structure of the dataset\n\nWhether the variables are numeric or factors, and how they should be prepared for analysis\n\n\n\nPlotting functions\nThese functions allow plotting for:\n-   individual variables\n\n-   Pairwise relationships between variables.\n\n\nFeatures functions\nFunctions that assist in\n\nDetecting outliers\nApplying transformations\nScaling variables.\n\n\n\nData Partition functions\nFunctions that facilitate partitioning data to improve inference and avoid overfitting during model selection.\n\n\nPurpose and Usage\nThe information and plotting functions provide valuable insights that assist in building better models, including:\n\nUnderstanding the distribution of the response variable\nChoosing the appropriate type of analysis\nExamining explanatory variables, including:\n\nRange and spread of values\nPresence of missing values\nAssociations and interactions between explanatory variables\nNature of relationships between response and explanatory variables (linear or non-linear)\n\n\nThe features functions focus on handling outliers, scaling, and transforming explanatory variables (x-variables) before modeling.\nData partitioning is used to avoid overfitting by ensuring models are evaluated more reliably. It falls under the broader category of data manipulation, although merging datasets is not covered here — only partitioning for improved inference is addressed.\nMost of the pre-fitting functions are data-related, and their names typically start with data (e.g., data_NAME), indicating that they either print information, produce plots, or manipulate data.frames.\nThe gamlss.prepdata package is thus a useful tool for carrying out pre-analysis work before beginning the process of fitting a distributional regression model.\nNext, we define what we mean by distributional regression."
  },
  {
    "objectID": "gamlss-prepdata.html#distributional-regression-model",
    "href": "gamlss-prepdata.html#distributional-regression-model",
    "title": "The Functions in the Package gamlss.prepdata",
    "section": "Distributional Regression model",
    "text": "Distributional Regression model\nThe aim of this vignette is to demonstrate how to manipulate and prepare data before applying a distributional regression analysis.\nThe general form a distributional regression model can be written as; \\[\n\\begin{split}\n\\textbf{y}     &  \\stackrel{\\small{ind}}{\\sim }  \\mathcal{D}( \\boldsymbol{\\theta}_1, \\ldots, \\boldsymbol{\\theta}_k) \\nonumber \\\\\ng_1(\\boldsymbol{\\theta}_1) &= \\mathcal{ML}_1(\\textbf{x}_{11},\\textbf{x}_{21}, \\ldots,  \\textbf{x}_{p1}) \\nonumber \\\\\n\\ldots &= \\ldots \\nonumber\\\\\ng_k(\\boldsymbol{\\theta}_k) &= \\mathcal{ML}_k(\\textbf{x}_{1k},\\textbf{x}_{2k}, \\ldots,  \\textbf{x}_{pk}).\n\\end{split}\n  \\tag{1}\\] where we assume that the response variable \\(y_i\\) for \\(i=1,\\ldots, n\\), is independently distributed having a distribution \\(\\mathcal{D}( \\theta_1, \\ldots, \\theta_k)\\) with \\(k\\) parameters and where all parameters could be effected by the explanatory variables \\(\\textbf{x}_{1},\\textbf{x}_{2}, \\ldots,  \\textbf{x}_{p}\\). The \\(\\mathcal{ML}\\) represents any regression type machine learning algorithm i.e. LASSO, Neural networks etc.\nWhen only additive smoothing terms are used in the fitting the model can be written as; \\[\\begin{split}\n\\textbf{y}     &  \\stackrel{\\small{ind}}{\\sim }  \\mathcal{D}(  \\boldsymbol{\\theta}_1, \\ldots,  \\boldsymbol{\\theta}_k) \\nonumber \\\\\ng_1( \\boldsymbol{\\theta}_1)  &= b_{01} + s_1(\\textbf{x}_{11})  +  \\cdots,  +s_p(\\textbf{x}_{p1}) \\nonumber\\\\\n\\ldots &= \\ldots \\nonumber\\\\\ng_k( \\boldsymbol{\\theta}_k)  &= b_{0k} + s_1(\\textbf{x}_{1k})  +   \\cdots,  +s_p(\\textbf{x}_{pk}).\n\\end{split}\n\\tag{2}\\] which is the GAMLSS model introduced by Rigby and Stasinopoulos (2005).\nThere are three books on GAMLSS, D. M. Stasinopoulos et al. (2017), Rigby et al. (2019) and M. D. Stasinopoulos et al. (2024) and several ’GAMLSS lecture materials, available from GitHUb, https://github.com/mstasinopoulos/Porto_short_course.git and https://github.com/mstasinopoulos/ShortCourse.git. The latest R packages related to GAMLSS can be found in https://gamlss-dev.r-universe.dev/builds.\nThe aim of this package is to prepare data and extract useful information that can be utilized during the modeling stage."
  },
  {
    "objectID": "gamlss-prepdata.html#sec-Information",
    "href": "gamlss-prepdata.html#sec-Information",
    "title": "The Functions in the Package gamlss.prepdata",
    "section": "Ιnformation functions",
    "text": "Ιnformation functions\nThe functions for obtaining information from a dataset are described in this section and summarized in Table 1. These functions can provide:\n\nGeneral information about the dataset such as the dimensions (number of rows and columns) and the percentage of omitted (missing) observations\nInformation about the variables in the dataset\nInformation about the observations in the dataset\n\nAll functions have a data.frame as their first argument.\nHere is a table of the information functions;\n\n\n\nTable 1: A summary table of the functions to obtain information\n\n\n\n\n\n\n\n\n\nFunctions\nUsage\n\n\n\n\ndata_dim()\nReturns the number of rows and columns, and calculates the percentage of omitted (missing) observations\n\n\ndata_names()\nLists the names of the variables in the dataset\n\n\ndata_rename()\nAllows renaming of one or more variables in the dataset\n\n\ndata_shorter_names()\nAllows shortening the names of one or more variables in the dataset\n\n\ndata_distinct()\nDisplays the number of distinct values for each variable in the dataset\n\n\ndata_which_na()\nDisplays the count of NA (missing) values for each variable in the dataset\n\n\ndata_omit()\nRemoves all rows (observations) that contain any NA (missing) values\n\n\ndata_str()\nDisplays the class of each variable (e.g., numeric, factor, etc.) along with additional details about the variables\n\n\ndata_cha2fac()\nConverts all character variables in the dataset to factor type\n\n\ndata_few2fac()\nConverts variables with a small number of distinct values (e.g., binary or categorical) to factor type\n\n\ndata_int2num()\nConverts integer variables with several distinct values to numeric type to allow for continuous analysis\n\n\ndata_rm()\nRemoves one or more variables (columns) from the dataset as specified by the user\n\n\ndata_rmNAvars()\nRemoves variables which have NA’s as all its values.\n\n\ndata_rm1val()\nRemoves factors that have only a single level (no variability) in the dataset\n\n\ndata_select()\nAllows the user to select one or more specific variables (columns) from the dataset for further analysis\n\n\ndata_exclude_class()\nRemoves all variables of a specified class (e.g., factor, numeric) from the dataset\n\n\ndata_only_continuous()\nRetains only the numeric variables (columns) in the dataset, excluding factors or other variable types\n\n\n\n\n\n\nNext we give examples of the functions.\n\ndata_dim()\nThis function provides detailed information about the dimensions of a data.frame. It is similar to the R function dim(), but with additional details. The output is the original data frame, allowing it to be used in a series of piping commands.\n\nrent99 |&gt; data_dim()\n\n************************************************************** \n************************************************************** \nthe R class of the data is: data.frame \nthe dimensions of the data are: 3082 by 9 \nnumber of observations with missing values: 0 \n% of NA's in the data: 0 % \n************************************************************** \n************************************************************** \n\n\nback to table\n\n\ndata_names()\nThis function provides the names of the variables in the data.frame, similar to the R function names().\n\nrent99 |&gt; data_names()\n\n************************************************************** \n************************************************************** \nthe names of variables \n[1] \"rent\"     \"rentsqm\"  \"area\"     \"yearc\"    \"location\" \"bath\"     \"kitchen\" \n[8] \"cheating\" \"district\"\n************************************************************** \n************************************************************** \n\n\nThe output of the function is the original data frame. back to table\n\n\ndata_rename()\nRenames one or more variables (columns) in the data.frame using the function data_rename();\n\nda&lt;- rent99 |&gt; data_rename(oldname=\"rent\", newname=\"R\")\nhead(da)\n\n         R   rentsqm area yearc location bath kitchen cheating district\n1 109.9487  4.228797   26  1918        2    0       0        0      916\n2 243.2820  8.688646   28  1918        2    0       0        1      813\n3 261.6410  8.721369   30  1918        1    0       0        1      611\n4 106.4103  3.547009   30  1918        2    0       0        0     2025\n5 133.3846  4.446154   30  1918        2    0       0        1      561\n6 339.0256 11.300851   30  1918        2    0       0        1      541\n\n\nThe output of the function is the original data frame with new names.\nback to table\n\n\ndata_shorter_names()\nIf the variables in the dataset have very long names, they can be difficult to handle in formulae during modelling. The function data_shorter_names() abbreviates the names of the explanatory variables, making them easier to use in formulas.\n\nrent99 |&gt; data_shorter_names()\n\n************************************************************** \n************************************************************** \nthe names of variables \n[1] \"rent\"  \"rents\" \"area\"  \"yearc\" \"locat\" \"bath\"  \"kitch\" \"cheat\" \"distr\"\n************************************************************** \n************************************************************** \n\n\nIf no long variable names exist in the dataset, the function data_shorter_names() does nothing. However, when applicable, the function abbreviates long names and returns the original data frame with the new shortened names.\n\n\n\n\n\n\nWarning\n\n\n\nNote that there is a risk when using a small value for the max option, as it may result in identical names for different variables. This could lead to confusion or errors in the modelling process. It is important to carefully choose an appropriate value for max to avoid this issue.\n\n\nback to table\n\n\ndata_distinct()\nTThe distinct values for each variable are shown below.\n\nrent99 |&gt; data_distinct()\n\n    rent  rentsqm     area    yearc location     bath  kitchen cheating \n    2723     3053      132       68        3        2        2        2 \ndistrict \n     336 \n\n\nThe output of the function is the original data frame.\nback to table\n\n\ndata_which_na()\nThis function provides information about which variables have missing observations and how many missing values there are for each variable;\n\nrent99 |&gt; data_which_na()\n\n    rent  rentsqm     area    yearc location     bath  kitchen cheating \n       0        0        0        0        0        0        0        0 \ndistrict \n       0 \n\n\nThe output of the function is the original data frame nothing is changing.\nback to table\n\n\ndata_omit\nThe function data_str() (similar to the R function str()) provides information about the types of variables present in the dataset.\n\nrent99 |&gt; data_omit()\n\n************************************************************** \n************************************************************** \nthe R class of the data is: data.frame \nthe dimensions of the data before omition are: 3082 x 9 \nthe dimensions of the data saved after omition are: 3082 x 9 \nthe number of observations omited: 0 \n************************************************************** \n************************************************************** \n\n\nThe output of the function is a new data frame with all NA’s omitted.\n\n\n\n\n\n\nWarning\n\n\n\nIt is important to select the relevant variables before using the data_omit() function, as some unwanted variables may contain many missing values that could lead to unnecessary row omissions.\n\n\nback to table\n\n\ndata_str()\nThe function data_str() (similar to the R function str()) provides information about the types of variable exist in the data.\n\nrent99 |&gt; data_str()\n\n************************************************************** \n************************************************************** \nthe structure of the data \n'data.frame':   3082 obs. of  9 variables:\n $ rent    : num  110 243 262 106 133 ...\n $ rentsqm : num  4.23 8.69 8.72 3.55 4.45 ...\n $ area    : int  26 28 30 30 30 30 31 31 32 33 ...\n $ yearc   : num  1918 1918 1918 1918 1918 ...\n $ location: Factor w/ 3 levels \"1\",\"2\",\"3\": 2 2 1 2 2 2 1 1 1 2 ...\n $ bath    : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ kitchen : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 2 1 1 ...\n $ cheating: Factor w/ 2 levels \"0\",\"1\": 1 2 2 1 2 2 1 2 1 1 ...\n $ district: int  916 813 611 2025 561 541 822 1713 1812 152 ...\n************************************************************** \n************************************************************** \ntable of the class of variabes \n\n factor integer numeric \n      4       2       3 \n************************************************************** \n************************************************************** \ndistinct values in variables \n    rent  rentsqm     area    yearc location     bath  kitchen cheating \n    2723     3053      132       68        3        2        2        2 \ndistrict \n     336 \nconsider to make those characters vectors into factors: \nlocation bath kitchen cheating \n************************************************************** \n************************************************************** \n\n\nThe output of the function is the original data frame.\nThe next set of functions manipulate variables withing data.frames.\nback to table\n\n\ndata_cha2fac()\nOften, variables in datasets are read as character vectors, but for analysis, they may need to be treated as factors. This function transforms any character vector (with a relatively small number of distinct values) into a factor.\n\nrent99 |&gt; data_cha2fac()  -&gt; da \n\n************************************************************** \nnot character vector was found \n\n\nSince no character were found nothing have changed. The output of the function is a new data frame.\nback to table\n\n\ndata_few2fac()\nThere are occasions when some variables have very few distinct observations, and it may be better to treat them as factors. The function data_few2fac() converts vectors with a small number of distinct values into factors.\n\nrent99 |&gt;  data_few2fac() -&gt; da \n\n************************************************************** \n    rent  rentsqm     area    yearc location     bath  kitchen cheating \n    2723     3053      132       68        3        2        2        2 \ndistrict \n     336 \n************************************************************** \n4 vectors with fewer number of values than 5 were transformed to factors \n************************************************************** \n************************************************************** \n\nstr(da)\n\n'data.frame':   3082 obs. of  9 variables:\n $ rent    : num  110 243 262 106 133 ...\n $ rentsqm : num  4.23 8.69 8.72 3.55 4.45 ...\n $ area    : int  26 28 30 30 30 30 31 31 32 33 ...\n $ yearc   : num  1918 1918 1918 1918 1918 ...\n $ location: Factor w/ 3 levels \"1\",\"2\",\"3\": 2 2 1 2 2 2 1 1 1 2 ...\n $ bath    : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ kitchen : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 2 1 1 ...\n $ cheating: Factor w/ 2 levels \"0\",\"1\": 1 2 2 1 2 2 1 2 1 1 ...\n $ district: int  916 813 611 2025 561 541 822 1713 1812 152 ...\n\n\nThe output of the function is a new data frame. This can be seen by using the R function str();\nback to table\n\n\ndata_int2num()\nOccasionally, we need to convert integer variables with a very large range of values into numeric vectors, especially for graphics. The function data_int2num() performs this conversion.\n\nrent99 |&gt; data_int2num() -&gt; da\n\n************************************************************** \n    rent  rentsqm     area    yearc location     bath  kitchen cheating \n    2723     3053      132       68        3        2        2        2 \ndistrict \n     336 \n2 integer vectors with more number of values than 50 were transformed to numeric \n************************************************************** \n\n\nThe output of the function is a new data frame.\nback to table\n\n\ndata_rm()\nFor plotting datasets, it’s easier to keep only the relevant variables. The function data_rm() allows you to remove unnecessary variables, making the dataset more manageable for visualization.\n\ndata_rm(rent99, c(2,9)) -&gt; da\ndim(rent99)\n\n[1] 3082    9\n\ndim(da)\n\n[1] 3082    7\n\n\nThe output of the function is a new data frame. Note that this could be also done using the function select() of the package dplyr, or our own data_select() function.\nback to table\n\n\ndata_rmNAvars()\nOccutionally when we read data from files in Rsome extra variables are accidentaly produced with no values but NA’s. The function data_rmNAvars() removes those variables from the data;\n\ndata_rmNAvars(da)\n\nback to table\n\n\ndata_rm1val()\nThis function searches for variables with only a single distinct value (often factors left over from a previous subset() operation) and removes them from the dataset.\n\nda |&gt; data_rm1val()\n\nThe output of the function is a new data frame.\nback to table\n\n\ndata_select()\nThis function selects specified variables from a dataset.\n\nda1&lt;-rent |&gt; data_select( vars=c(\"R\", \"Fl\", \"A\"))\nhead(da1)\n\n       R Fl    A\n1  693.3 50 1972\n2  422.0 54 1972\n3  736.6 70 1972\n4  732.2 50 1972\n5 1295.1 55 1893\n6 1195.9 59 1893\n\n\nThe output of the function is a new data frame.\nback to table\n\n\ndata_exclude_class()\nThis function searches for variables (columns) of a specified R class and removes them from the dataset. By default, the class to remove is factor.\n\nda |&gt; data_exclude_class()  -&gt; da1\nhead(da1)\n\n      rent area yearc\n1 109.9487   26  1918\n2 243.2820   28  1918\n3 261.6410   30  1918\n4 106.4103   30  1918\n5 133.3846   30  1918\n6 339.0256   30  1918\n\n\nThe output of the function is a new data frame.\n\n\ndata_only_continuous()\nThis function keeps only the continuous variables in the dataset, removing all non-continuous variables.\n\nda |&gt; data_only_continuous()  -&gt; da1\nhead(da1)\n\n      rent area yearc\n1 109.9487   26  1918\n2 243.2820   28  1918\n3 261.6410   30  1918\n4 106.4103   30  1918\n5 133.3846   30  1918\n6 339.0256   30  1918\n\n\nThe output of the function is a new data frame.\nback to table"
  },
  {
    "objectID": "gamlss-prepdata.html#sec-Graphical-functions",
    "href": "gamlss-prepdata.html#sec-Graphical-functions",
    "title": "The Functions in the Package gamlss.prepdata",
    "section": "Graphical functions",
    "text": "Graphical functions\nGraphical methods are used in pre-analysis to examine individual variables or pair-wise relationships between variables.\n\n\n\nTable 2: A summary table of the graphical functions\n\n\n\n\n\n\n\n\n\nFunctions\nUsage\n\n\n\n\ndata_plot()\nPlots each variable in the dataset to visualize its distribution or other important characteristics (see also Section 6.1\n\n\ndata_bucket()\nGenerates bucket plots for all numerical variables in the dataset to visualize their skewness and kurtosis\n\n\ndata_response()\nPlots the response variable alongside its z-score, providing a standardized version of the response for comparison\n\n\ndata_zscores()\nPlots the z-scores for all continuous variables in the dataset, allowing for easy visualization of the standardized values (see also Section 6.1)\n\n\ndata_xyplot()\nGenerates pairwise plots of the response variable against all other variables in the dataset to visualize their relationships\n\n\ndata_cor()\nCalculates and plots the pairwise correlations for all continuous variables in the dataset to assess linear relationships between them\n\n\ndata_void()\nSearches for pairwise empty spaces across all continuous variables in the dataset to identify problems with interpretation or prediction\n\n\ndata_pcor()\nCalculates and plots the pairwise patrial-correlations for all continuous variables in the dataset\n\n\ndata_inter()\nSearches for potential pairwise interactions between variables in the dataset to identify relationships or dependencies that may be useful for modelling\n\n\ndata_leverage()\nDetects outliers in the continuous explanatory variables (x-variables) as a group to highlight unusual observations\n\n\ndata_Ptrans_plot()\nPlots the response variable against various power transformations of the continuous x-variables to explore potential relationships and model suitability\n\n\n\n\n\n\nNext we give examples of the graphical functions.\n\ndata_plot()\nThe function data_plot plots all the variables of the data individually; It plots the continuous variable as histograms with a density plots superimposed, see the plot for rent and yearc. As an alternative a dot plots can be requested see for an example in ?@sec-data_response. For integers the function plots needle plots, see area below and for categorical the function plots bar plots, see location, bath kitchen and cheating below.\n\nda |&gt; data_plot()\n\n 100 % of data are saved, \nthat is, 3082 observations. \n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\nRemoved 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\n\nThe function could saves the ggplot2 figures.\nback to table\n\n\ndata_bucket()\nThe function data_bucket plots bucket plots (which identify skewness and kurosid) for all variables in the data set.\n\nda |&gt; data_bucket()\n\n 100 % of data are saved, \nthat is, 3082 observations. \n    rent     area    yearc location     bath  kitchen cheating \n    2723      132       68        3        2        2        2 \n\n\n\n\n\n\n\n\n\nback to table\n\n\ndata_response()\nThe function data_response() plots four different plots related to a continuous response variable. It plots; i) a histogram (and the density function); ii) a dot plot of the response in the original scale iii) a histogram (and the density function) of the response at the z-scores scale and vi) s dot-plot in the z-score scale.\nThe z-score scale is defined by fitting a distribution to the variable (normal or SHASH) and then taking the residuals, see also next section. The dot plots are good in identify highly skew variables and unusual observations. They display the median and inter quantile range of the data. The y-axis of a dot plot is a randomised uniform variable (therefore the plot could look slightly different each time.)\n\nda |&gt; data_response(, response=rent)\n\n 100 % of data are saved, \nthat is, 3082 observations. \nthe class of the response is numeric is this correct? \na continuous distribution on (0,inf) could be used \n\n\n\n\n\n\n\n\n\nback to table\n\n\ndata_zscores()\nOne could fit any four parameter (GAMLSS) distribution, defined on \\(-\\infty\\) to \\(\\infty\\), to any continuous variable, where skewness and kurtosis is suspected and take the quantile residuals (or z-scores) as the transformed values x-values. The function y_zscores() performs just this. It takes a continuous variable and fits a continuous four parameter distribution and gives the z-scores. The fitting distribution can be specified by the user, but as default we use the SHASHo distribution. Below we demonstrate that the function y_zscores() is equivalent to fitting the SHASHo distribution to a continuous variable and then take the quqntile residuals from the fitted model as the result.\n\nda |&gt; data_zscores()\n\n\n\n\n\n\n\n\nIn order to see how the z-scores are calculated consider the function y_zscores() which is taking an individual variables z-scores;\n\n z &lt;- y_zscores(rent99$rent, plot=FALSE)\n\nThe function is equivalent of fitting a constant model to all the parameters of a given distribution and than taking the quantile residuals (or z=scores) as he variable of interest. The default distribution is the four parameter SHASHo distribution.\n\nlibrary(gamlss2)\nm1 &lt;- gamlssML(rent99$rent, family=SHASHo) # fitting a 4 parameter distribution \ncbind(z,resid(m1))[1:5,]#  and taking the residuals\n\n          z          \n1 -2.496097 -2.496097\n2 -1.340464 -1.340464\n3 -1.182014 -1.182014\n4 -2.526326 -2.526326\n5 -2.295069 -2.295069\n\n\nback to table\n\n\ndata_xyplot()\nThe functions data_xyplot() plots the response variable against each of the independent explanatory variables. It plots the continuous against continuous as scatter plots and continuous variables against categorical as box plot.\n\n\n\n\n\n\nNote\n\n\n\nAt the moment there is no provision for categorical response variables.\n\n\n\nda |&gt; data_xyplot(response=rent )\n\n 100 % of data are saved, \nthat is, 3082 observations. \n\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nThe output of the function saves the ggplot2 figures.\nNote hat the package gamlss.prepdata do not provides pairwise plot of the explanatory variables themself but the package GGally does. here is an example ;\n\nlibrary(GGally) \n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\ndac &lt;- gamlss.prepdata:::data_only_continuous(da) \nggpairs(dac, lower=list(continuous = \"cor\",\n      combo = \"box_no_facet\", discrete = \"count\", \n      na = \"na\"),upper=list(continuous = \"points\", \n      combo = \"box_no_facet\", discrete = \"count\", na = \"na\")) + \ntheme_bw(base_size =15)\n\n\n\n\n\n\n\n\nback to table\n\n\ndata_cor()\nThe function data_corr() is taking a data.frame object and plot the correlation coefficients of all its continuous variables.\n\ndata_cor(da, lab=TRUE)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n4 factors have been omited from the data \n\n\nWarning in data_cor(da, lab = TRUE):\n\n\n\n\n\n\n\n\n\nA different type of plot can be produce if we use;\n\ndata_cor(da, method=\"circle\", circle.size = 40)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n4 factors have been omited from the data \n\n\nWarning in data_cor(da, method = \"circle\", circle.size = 40):\n\n\n\n\n\n\n\n\n\nTo get the variables with higher, say, than $0.4 $ correlation values use;\n\nTabcor &lt;- data_cor(da, plot=FALSE)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n4 factors have been omited from the data \n\n\nWarning in data_cor(da, plot = FALSE):\n\nhigh_val(Tabcor, val=0.4)\n\n     name1  name2  corrs  \n[1,] \"rent\" \"area\" \"0.585\"\n\n\nWe can plot the path of those variables using the package corr;\n\nlibrary(corrr)\nnetwork_plot(Tabcor)\n\n\n\n\n\n\n\n\nback to table\n\n\ndata_association()\nThe function data_association() is taking a data.frame object and plot the pair-wise association of all its variables. The pair-wise association for two continuous variables is given by default by the absolute value of the Spearman’s correlation coefficient. For two categorical variables by the (adjusted for bias) Cramers’ v-coefficient. For a continuous against a categorical variables by the \\(\\sqrt R^2\\) obtained by regressing the continuous variable against the categorical variable.\n\ndata_association(da, lab=TRUE)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n\n\n\n\n\n\n\n\n\nA different type of plot can be produce if we use;\n\ndata_association(da, method=\"circle\", circle.size = 40)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n\n\n\n\n\n\n\n\n\nTo get the variables with higher, say, than $0.4 $ association values use;\n\nTabcor &lt;- data_cor(da, plot=FALSE)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n4 factors have been omited from the data \n\n\nWarning in data_cor(da, plot = FALSE):\n\nhigh_val(Tabcor, val=0.4)\n\n     name1  name2  corrs  \n[1,] \"rent\" \"area\" \"0.585\"\n\n\nA different plot can be acheived using the funtion network_plot() of package corr;\n\npp=gamlss.prepdata:::data_association(rent[,-c(4,5,8)], plot=F)\n\n 100 % of data are saved, \nthat is, 1969 observations. \n\nlibrary(corrr)\nnetwork_plot(pp, min_cor = 0, colors = c(\"red\", \"green\"), legend = \"range\")\n\n\n\n\n\n\n\n\nback to table\n\n\ndata_void()\n\n\n\n\n\n\nWarning\n\n\n\nThis function is new. Its theoretical foundation are not proven yet. The function needs testing and therefore it should be used with causion.\n\n\nThe idea behind the functions void() and its equivalent data.frame version data_void() is to be able to identify whether the data in the direction of two continuous variables say \\(x_i\\) and \\(x_j\\) have a lot of empty spaces. The reason is that empty spaces effect prediction since interpolation at empty spaces is dengerous. The function data_void() is taking a data.frame object and plot the percentage of empty spaces for all pair-wise continuous variables. The function used the foreach() function of the package foreach to allow parallel processing.\n\nregisterDoParallel(cores = 9)\ndata_void(da)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n4 factors have been omited from the data \n\n\nWarning in data_void(da):\n\n\n\n\n\n\n\n\n\nA different type of plot can be produce if we use;\n\ndata_void(da, method=\"circle\", circle.size = 40)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n4 factors have been omited from the data \n\n\nWarning in data_void(da, method = \"circle\", circle.size = 40):\n\n\n\n\n\n\n\n\nstopImplicitCluster()\n\nTo get the variables with highter than $0.4 $ values use;\n\nTabvoid &lt;- data_void(da, plot=FALSE)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n4 factors have been omited from the data \n\n\nWarning in data_void(da, plot = FALSE):\n\nhigh_val(Tabvoid, val=0.4)\n\n     name1  name2   corrs  \n[1,] \"rent\" \"area\"  \"0.661\"\n[2,] \"rent\" \"yearc\" \"0.627\"\n[3,] \"area\" \"yearc\" \"0.51\" \n\n\nTo plot their paths use;\n\nlibrary(corrr)\nnetwork_plot(Tabvoid)\n\n\n\n\n\n\n\n\nback to table\n\n\ndata_pcor()\nThe function data_copr() is taking a data.frame object and plot the partial correlation coefficients of all its continuous variables.\n\ndata_pcor(da, lab=TRUE)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n4 factors have been omited from the data \n\n\nWarning in data_pcor(da, lab = TRUE):\n\n\n\n\n\n\n\n\n\nTo get the variables with highter than $0.4 $ partial correlation values use;\n\nTabpcor &lt;- data_pcor(da, plot=FALSE)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n4 factors have been omited from the data \n\n\nWarning in data_pcor(da, plot = FALSE):\n\nhigh_val(Tabpcor, val=0.4)\n\n     name1  name2  corrs  \n[1,] \"rent\" \"area\" \"0.638\"\n\n\nFor plotting you can use;\n\nlibrary(corrr)\nnetwork_plot(Tabpcor)\n\n\n\n\n\n\n\n\nback to table\n\n\ndata_inter()\nThe function data_inter() takes a data.frame, fits all pair-wise interactions of the explanatory variables against the response (using a normal model) and produce a graph displaying their significance levels. The idea behind this is to identify possible first order interactions at an early stage of the analysis.\n\nda |&gt; gamlss.prepdata:::data_inter(response= rent)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n\n\n\n\n\n\n\n\n\n\ntinter &lt;-gamlss.prepdata:::data_inter(da,  response= rent, plot=FALSE)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n\ntinter\n\n         area yearc location  bath kitchen cheating\narea       NA 0.014    0.001 0.119   0.000    0.000\nyearc      NA    NA    0.000 0.027   0.154    0.226\nlocation   NA    NA       NA 0.000   0.000    0.578\nbath       NA    NA       NA    NA   0.868    0.989\nkitchen    NA    NA       NA    NA      NA    0.719\ncheating   NA    NA       NA    NA      NA       NA\n\n\nTo get the variables with lower than $0.05 $ significant interactions use;\n\nTabinter &lt;- gamlss.prepdata:::data_inter(da, response= rent, plot=FALSE)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n\nlow_val(Tabinter)\n\n     name1      name2      value  \n[1,] \"area\"     \"yearc\"    \"0.014\"\n[2,] \"area\"     \"location\" \"0.001\"\n[3,] \"yearc\"    \"location\" \"0\"    \n[4,] \"yearc\"    \"bath\"     \"0.027\"\n[5,] \"location\" \"bath\"     \"0\"    \n[6,] \"area\"     \"kitchen\"  \"0\"    \n[7,] \"location\" \"kitchen\"  \"0\"    \n[8,] \"area\"     \"cheating\" \"0\"    \n\n\nback to table\nNOT DOCUMENTED IN HELP FOR THIS FUNCTION\n\n\ndata_leverage()\nThe function data_leverage() uses the linear model methodology to identify posible unusual observations within the explanatory variables as a group (not indivisually). It fit a linear (normal) model with response, the response of the data, and all explanatory variables in the data, as x’s. It then calculates the leverage points and plots them. A leverage is a number between zero and 1. Large leverage correspond to extremes in the x’s.\n\nrent99[, -c(2,9)] |&gt; data_leverage( response=rent)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe horizontal line is the plot is at point \\(2 \\times (r/n)\\), which is the threshold suggested in the literature; values beyond this point could be identified as extremes. It looks that the point \\(2 \\times (r/n)\\) is too low (at least for our data). Instead the plot identify only observation which are in the one per cent upper quantile. That is a quantile value of the leverage at value 0.99.\n\n\nSection 6.1.4 show how the information of the function data_leverage() can be combined with the information given by data_outliers() in order to confirm hight outliers in the data.\n\n\n\n\n\n\nImportant\n\n\n\nWhat happen if multiple response are in the data?\n\n\nback to table\n\n\ndata_Ptrans_plot()\nThe essence of this plot is to visually decide whether certain values of \\(\\lambda\\) in a power transformation are appropriate or not. Section 6.3 describes the power transformation, \\(T=X^\\lambda\\) for \\(\\lambda&gt;0\\), in more details. For each continuous explanatory variable \\(X\\), the function show the response against \\(X\\), (\\(\\lambda=1\\)), the response against the square root of \\(X\\), (\\(\\lambda=1/2\\)) and the response against the log of \\(X\\) (\\(\\lambda \\rightarrow 0\\)). The user then can decide whether any of those transformation are appropriate.\n\ngamlss.prepdata:::data_Ptrans_plot(da, rent) \n\n 100 % of data are saved, \nthat is, 3082 observations. \n\n\n\n\n\n\n\n\n\nIt look that no transformation is needed for the continuous explanatory variables area of yearc.\nback to table"
  },
  {
    "objectID": "gamlss-prepdata.html#sec-Features",
    "href": "gamlss-prepdata.html#sec-Features",
    "title": "The Functions in the Package gamlss.prepdata",
    "section": "Feature functions",
    "text": "Feature functions\nThe features functions are functions to help with the continuous explanatory x-variables; We divide them here in four sections; Functions for\n\noutliers;\nscaling;\ntransformations and\nfunction approproate for time series\n\nThe available functions are;\n\n\n\nTable 3: The outliers, scalling, transformation and time series functions\n\n\n\n\n\n\n\n\n\nFunctions\nUsage\n\n\n\n\ny_outliers()\nidentify possible outliers in a continuous variable (see below)\n\n\ny_both_outliers()\nidentify possible outliers using both zscores and quaniles methodology\n\n\ndata_outliers()\nidentify possible outliers for all continuous x-variable (see also data_zscores in Section 5.0.4)\n\n\ndata_leverage()\nthis is a repear of the function from Section 5.0.11\n\n\ndata_scale()\nscaling all continuous x-variables either to zero mean and one s.d. or to the range [0,1]\n\n\nxy_Ptrans()\nlooking for appropriate power transformation for response against one of the x-variable\n\n\ndata_Ptrans()\nlooking for appropriate power transformation for response against all x-variable\n\n\ndata_Ptrans_plot()\nplotting all x’s agains the response using identiti ty square root ans log transormations\n\n\n\n\n\n\n\nOutliers\nOutliers in the response variable, within a distributional regression framework like GAMLSS, are better handled by selecting an appropriate distribution for the response. For example, an outlier under the assumption of a normal distribution for the response may no longer be considered an outlier if a distribution from the t-family is assumed. The function data_response(), discussed in Section 5.0.3 provides guidelines on appropriate actions. This section focuses on outliers in the explanatory variables.\nOutliers in the continuous explanatory variables can potentially affect curve fitting, whether using linear or smooth non-parametric terms. In such cases, removing outliers from the x-variables can make the model more robust. Outliers are observations that deviate significantly from the rest of the data, but the concept depends on the dimension being considered. An single observation value might be an outlier within an explanatory variable. A pair observation might be an outlier examining within a pair-wise relationships. In a one-dimensional search, we identify extreme observations in individual continuous variables, based on how far they lie from the majority of the data. In a two-dimensional search, we look for outliers in pairwise plots of continuous variables, examining how observations deviate from typical relationships between variable pairs. Leverage points are useful to identify outliers when we look for outliers in \\(r\\) dimension when \\(r\\) is the number of explanatory variables.\n\n\n\n\n\n\nNote\n\n\n\nAt this preliminary stage of the analysis, where no model has yet been fitted, it is difficult to identify outliers among factors. This is because outliers in factors typically do not appear unusual when examining individual variables. Their outlier status usually becomes apparent only when considered in combination with other variables or factors. Identifying such outliers is a task better suited for the modelling stage of the analysis.\n\n\nThe function y_outliers() in Section 6.1.1 uses two methodologies to identify outliers; The default uses simple algorithm to identify potential one-dimensional outliers in continuous explanatory variables. It is designed specifically for continuous variables.\n\nFor variables defined on the full real line, (i.e., from \\(-\\infty\\) to \\(+\\infty\\), the algorithm fits a parametric distribution—by default, the four-parameter SHASHo distribution—and computes z-scores (quantile residuals) to assess outlier status.\nFor variables defined on the positive real line (i.e., from \\(0\\) to \\(+ \\infty\\)), an additional transformation step is included. These variables are often highly right-skewed, and extreme values in the right tail can dominate any outlier detection process. To address this, the function attempts to find a power transformation \\(T = X^\\lambda\\) that minimizes the Jarque-Bera test statistic—a measure of deviation from normality. To find the optimal power parameters \\(\\lambda\\) the function y_outliers() uses the internal function x_Ptrans(). After this transformation, the SHASHo distribution is fitted, and z-scores are computed. The Jarque-Bera test statistic is always non-negative; the further it is from zero, the stronger the evidence against normality. The goal is to reduce skewness and kurtosis in the data before assessing outliers.\n\nOutliers are identify for observations with very low or hight z-scores. For example, if a z-scores is greater in absolute value to a specified value say 3. The methodology is appropriate for identifying outliers in one dimension. Pair-wise scatter plots of all continuous variables in the data can be use to identify outliers in two dimensions. To identify outliers in \\(p\\) dimensions the function data_leverage(), see Section 6.1.4, can be used.\n\n\n\n\n\n\nNote\n\n\n\nSeveral of the graphical functions described in Section 5 are good for identify visualy outliers. In fact we recommend that the function data_outliers() should be used in combination with functions like data_plot() or data_zscores().\n\n\n\ny_outliers()\nThe function y_outliers() identify outliers in one \\(X\\) variable. There are two methodologies to do that;\n\nzscores: A distribution is fitted to the data (usually SHASHo) then the residuals (z-scores) are taken and observations with large residuals are identified (see also the above comment).\nquantile: This is the classical methodology and it is based on sample quantiles. An observation is identified if it far from the median. How far usually is taken as three time the semi-inter-quantiles range. The option value is set to 4 as a default in the function.\n\nUsing the zscores;\n\ny_outliers(da$rent)\n\nnamed integer(0)\n\n\nor using the quantile\n\ny_outliers(da$rent, type=\"quantile\", value=4)\n\n [1]  321  350  354  379  395  404  420  421  424  426  429  431  433  434  438\n[16]  439  619  657  675  838  851  856 1015 1016 1576 1877 2005 2070 2271 2551\n[31] 2598 2621 2635 2749 2755 2857 2889 2900 2901 2966 2982 3015 3059 3078 3082\n\n\nNote that the y_outliers() is used by data_outliers() to identify outliers in all continuous variables in the data.\n\n\n\n\n\n\nNote\n\n\n\nFor very skew data the value in the option type=\"quantile\" need to be increased.\n\n\nback to table\n\n\ny_both_outliers()\nThe function y_both_outliers() uses both zscores and quantiles and reports the obsrarvations appearing in both sets. Note that the defaule value is 4 for both cases.\n\ny_both_outliers(da$rent)\n\ninteger(0)\n\n\nback to table\n\n\ndata_outliers()\nThe function data_outliers() uses the z-scores technique described above in order to detect outliers in the continuous variables in the data. It fits a SHASHo distribution to the continuous variable in the data and uses the z-scores (quantile residuals) to identify (one dimension) outliers for those continuous variables.\n\ndata_outliers(da)\n\n$rent\nnamed integer(0)\n\n$area\nnamed integer(0)\n\n$yearc\nnamed integer(0)\n\n\nAs it happen no individual variable outliers were highlighted in the rent data using the z-scores methodology. In general we recommend the function data_outliers() to be used in combination of the graphical functions data_plot() and data_zscores()\nback to table\n\n\ndata_leverage() (repeat)\nThe function data_leverage(), fisrt describe in Section 5.0.11, can be used to detect extremes in the continuous variables in the data by fiting a linear model with all the continuous variables in the data and then useing the higher leverage points to identify (multi-dimension) outliers for the observations for all continuous variables. By default, it identifies one percent of the observations with high leverage in the data. Here we use the function as a way to identify the obsrvation (not plottoing).\n\ndata_leverage(da, response=rent, plot=FALSE)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n\n\n [1]   38  162  176  204  206  242  454  496  541  651  696  794  910  942 1094\n[16] 1296 1384 1388 1782 1837 1875 1907 1970 2530 2540 2727 2810 2890 2977 3065\n[31] 3070\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe functiondata_leverage() always identifies the top one percent of observations with the highest leverage, which may or may not be outliers. These high-leverage points can have a strong influence on model estimates. To better assess their impact, the list of high-leverage observations returned by data_leverage() should be compared with the list of outliers identified by the data_outliers() function. Observations that appear in both lists are particularly noteworthy, as they may be influential outliers that warrant further investigation.\n\n\nNext we are trying the process of identifying if data with high leverage are also coincide with outliers identified using the function data_outliers() function. The function intersect() will flash out the intersection of the two set of observations. First use data_outliers()\n\nul &lt;- unlist(data_outliers(da))\n\nthen data_leverage();\n\nll&lt;-data_leverage(da, response=rent, plot=FALSE)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n\n\nand finally we inspect the intersection of the two sets using intersect();\n\nintersect(ll,ul)\n\ninteger(0)\n\n\nHere we have zero outliers but in general we would expect to find common observations for further checking.\nback to table\n\n\n\nScaling\nScaling is another form of transformation for the continuous explanatory variables in the data which brings the explanatory variables in the same scale. It is a form of standardization. Standardization means bringing all continuous explanatory variables to similar range of values. For some machine learning techniques, i.e., principal component regression or neural networks standardization is mandatory in other like LASSO is recommended. There are two types of standardisation\n\nscaling to normality and\nscaling to a range from zero to one.\n\nScaling to normality it means that the variable should have a mean zero and a standard deviation one. Note that, this is equivalent of fitting a Normal distribution to the variables and then taking the z-scores (residuals) of the fit as the transformed variable. The problem with this type of scaling is that skewness and kurtosis in the standardised data persist. One could go further and fit a four parameter distribution instead of a normal where the two extra parameters could account for skewness and kurtosis. The function data_scale() described in Section 6.2.1 it gives this option with te argument family in which can used to specify a different distribution.\n\ndata_scale()\nThe function data_scale() perform scaling to all continuous variables an a data set. Note that factors in the data set are left untouched because when fitted within a model they will be transform to dummy variable which take values 0 or 1 (a kind of standardization). The response is left also untouched because it is assumed that an appropriate distribution will be fitted. The response variable has to be declared using the argument response. First we use data_scale() to scale to normality.\n\nhead(data_scale(da, response=rent))\n\n    area    yearc location     bath  kitchen cheating \n     132       68        3        2        2        2 \n\n\n      rent       area      yearc location bath kitchen cheating\n1 109.9487  0.7009962  0.7035881        2    0       0        0\n2 243.2820 -0.4796117  0.7484206        2    0       0        1\n3 261.6410 -0.2266243  0.6587556        1    0       0        1\n4 106.4103 -0.3531180 -1.2242096        2    0       0        0\n5 133.3846  0.1950214 -1.7173671        2    0       0        1\n6 339.0256 -0.5639408  1.6450707        2    0       0        1\n\n\nAs we mention before scaling to normality is equivalent of fitting a Normal variable first. The problem with scaling to normality is that if the variables are highly skew or kurtotic scaling them to normality does correct for skewness or kurtosis. Using a parametric distribution with four parameters some of which are skewness and kurtosis parameters it may correct that. Next we standardised using the SHASHo distribution;\n\nhead(data_scale(da, response=rent,  family=\"SHASHo\"))\n\n    area    yearc location     bath  kitchen cheating \n     132       68        3        2        2        2 \n\n\n      rent       area      yearc location bath kitchen cheating\n1 109.9487  0.7390434  0.6323846        2    0       0        0\n2 243.2820 -0.3940193  0.6795499        2    0       0        1\n3 261.6410 -0.1175129  0.5864434        1    0       0        1\n4 106.4103 -0.2526941 -1.0598183        2    0       0        0\n5 133.3846  0.2949483 -1.6281459        2    0       0        1\n6 339.0256 -0.4916986  2.0610405        2    0       0        1\n\n\nThe second form od standardisation is to transform all continuous x-variables to a range zero to one. Here is how this is done;\n\nhead(data_scale(da, response=rent,  scale.to=\"0to1\"))\n\n    area    yearc location     bath  kitchen cheating \n     132       68        3        2        2        2 \n\n\n      rent       area yearc location bath kitchen cheating\n1 109.9487 0.04285714     0        2    0       0        0\n2 243.2820 0.05714286     0        2    0       0        1\n3 261.6410 0.07142857     0        1    0       0        1\n4 106.4103 0.07142857     0        2    0       0        0\n5 133.3846 0.07142857     0        2    0       0        1\n6 339.0256 0.07142857     0        2    0       0        1\n\n\nback to table\n\n\n\nTransformations\nIn the context of outliers, the focus on how far a single observation deviates from the rest of the data. However, when considering transformations, the goal shifts toward a better capturing of the relationship between a continuous predictor and the response variable. Specifically, we aim to model peaks and troughs in the relationship between a single x-variable and the response better. To achieve this, it is sometimes sufficient to stretch the x-axis so that the predictor variable is more evenly distributed within its its range. A common approach for this is to use a power transformation of the form \\(T = X^\\lambda\\). This transoformation can reduce the curvature in the response and make the curve fitting easier. Importantly, the power transformation smoothly transitions into a logarithmic transformation as \\(\\lambda \\to 0\\), since: \\(\\frac{X^{\\lambda} - 1} {\\lambda} \\rightarrow \\log(X) \\quad \\text{as } \\lambda \\to 0\\). Thus, in the limit as \\(\\lambda \\to 0\\) is the power transformation is the log transformation \\(T = \\log(X)\\).\n\n\n\n\n\n\n\nNote\n\n\n\nThe power transformation is a subclass of the shifted power transformation\n\\(T = (\\alpha + X)^\\lambda\\) which is not consider here\n\n\nThe aim of the power transformation is to make the model fitting process more robust and reliable and maximizing the information extracted from the data. There are three different functions in gamlss.prepdata dealling with power transformations, xy_Ptrans(), data_Ptrans() and data_Ptrans_plot()\n\n\n\n\n\n\nNote\n\n\n\nIt’s important to distinguish between the concepts of transformations (the subject of this section) and of feature extraction, a term often used in machine learning. We refer to transformations as functions applied to a individual explanatory variables while we use feature extraction when multiple explanatory variables are involved. In both cases, the goal is to enhance the model capabilities.\n\n\n\nxy_Ptrans()\n\npp&lt;-gamlss.prepdata:::xy_Ptrans(da$area, da$rent) \npp\n\n[1] 1.06212\n\n\n\ngamlss.prepdata:::xy_Ptrans(da$area, da$rent, prof=TRUE, k=4) \n\n\n\n\n\n\n\n\n\n\ndata_Ptrans()\n\ngamlss.prepdata:::data_Ptrans(da, response=rent)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n\n\n$area\n[1] 1.06212\n\n$yearc\n[1] 1.499942\n\n\nback to table\n\n\ndata_Ptrans_plot() (repeat)\n\ngamlss.prepdata:::data_Ptrans_plot(da, rent) \n\n 100 % of data are saved, \nthat is, 3082 observations. \n\n\n\n\n\n\n\n\n\nback to table\n\n\n\nTime series\nTime series data sets consist of observations collected at consistent time intervals—e.g., daily, weekly, or monthly. These data require special treatment because observations that are closer together in time tend to be more similar, violating the usual assumption of independence between observations. A similar issue arises in spatial data sets, where observations that are geographically closer often exhibit spatial correlation, again violating independence.\nMany data sets include spatial or temporal features —such as longitude and latitude or timestamps (e.g., 12/10/2012)— those variables are can be used for for modeling directly, but also for interpreting or visualization of the results. If temporal or spatial information exists in the data, it is essential to ensure these features are correctly read and interpreted in R. For instance, dates in R can be handled using the as.Date() function. To understand its functionality and options associated with the function please consult the help documentation via help(\"as.Date\"). Below, we provide two simple functions of how to work with date-time variables.\n\ntime_dt2dhour()\nSuppose the variable dt contains both a date and a time. We may want to extract and separate this into two components: one containing the date, and another capturing the hour of the day.\n\ndt &lt;- c(\"01/01/2011 00:00\", \"01/01/2011 01:00\", \"01/01/2011 02:00\", \"01/01/2011 03:00\", \"01/01/2011 04:00\", \"01/01/2011 05:00\") \n\nThe function to do this separation can lok like;\n\ntime_dt2dhour &lt;- function(datetime, format=NULL) \n  { \n          X &lt;- t(as.data.frame(strsplit(datetime,' '))) \nrownames(X) &lt;- NULL \ncolnames(X) &lt;- c(\"date\", \"time\") \n       hour &lt;- as.numeric(sub(\":\",\".\",X[,2])) \n      date &lt;- as.Date(X[,1],format=format) \n      data.frame(date, hour) \n  } \n\n\nP &lt;- time_dt2dhour(dt, \"%d/%m/%Y\") \nP\n\n        date hour\n1 2011-01-01    0\n2 2011-01-01    1\n3 2011-01-01    2\n4 2011-01-01    3\n5 2011-01-01    4\n6 2011-01-01    5\n\n\n\n\ntime_2num()\nThe second example contains time given in its common format, c(\"12:35\", \"10:50\") and we would like t change it to numeric which we can use in the model.\n\nt &lt;- c(\"12:35\", \"10:50\")\n\nThe function to change this to numeric;\n\ntime_2num &lt;- function(time, pattern=\":\") \n  { \n  t &lt;- gsub(pattern, \".\", time) \n   as.numeric(t) \n  } \ntime_2num(t) \n\n[1] 12.35 10.50\n\n\nFor user who deal with a lot of time series try the function POSXct() and also the function select() from the dplur package."
  },
  {
    "objectID": "gamlss-prepdata.html#sec-Partition",
    "href": "gamlss-prepdata.html#sec-Partition",
    "title": "The Functions in the Package gamlss.prepdata",
    "section": "Data Partition",
    "text": "Data Partition\n\nIntroduction to data partition\nPartitioning the data is essential for comparing different models and checking for overfitting. Figure 1 illustrates the different ways the data can be split. A dataset can be split multiple times or just once, depending on the size of the dataset. For large datasets, it is common to split the data into a training set and a test set, and, if necessary, a validation set. Observations in the training set are referred to as in-bag, while observations in the test or validation set are termed out-of-bag.\nThe training set is used for fitting the model, the test set is used for prediction and comparison between models, and the validation set is used for tuning the model’s hyperparameters. In additive smoothing regression models, for instance, the hyperparameters refer to the smoothing parameters. Data partitioning allows for performance evaluation, model assumption checks, and detection of overfitting.\nIf the fitted model performs reasonably well on both the training and test data sets, one can be confident that overfitting has been avoided. Overfitting occurs when a model fails to generalize well to test data, typically because it is too closely fitted to the training data. In contrast, underfitting refers to a poor model that fails to capture the essential patterns in both training and test data, resulting to a model that is far from both the sample and the population intended to represent.\n\nVarious goodness-of-fit measures can be used to check and compare different distributional regression models. One of the most popular methods when there is no portioning of data is the Generalized Akaike Information Criterion (GAIC). To calculate the GAIC, only the in-bag data are required. However, calculating the GAIC requires an accurate measure of the degrees of freedom used to fit the model. For mathematical (stochastic) models, the degrees of freedom are straight forward to define as they correspond the number of estimated parameters. In contrast, for over-parametrised algorithmic models like neural networks, estimating he degrees of freedom is challenging. Note that the degrees of freedom serve as a measure of the model’s complexity. We expect more complex models to be closer to the training data, which may reduce their ability to generalize well to new data (overfitting).\n\n\n\n\n\n\nNote\n\n\n\nNote that measure of goodness-of-fit used for the classical regression models when data partition is involves are mostly not appropriate for distributional regression model like GAMLSS. This because those measures concentrate in the behaviour at the centre of the distribution of the response which may or not be of interest to a practitioner.\n\n\nThe measures of goodness-of-fit used in classical regression models when partinioning of the data is involve are;\n\nRMSE: root mean squared error;\nMAE: mean absolute error;\n\\(R^2\\): and\nMAPE: mean absolute percentage error.\n\nAll of the above measures of goodness-of-fit are NOT generalize well when the distribution of the response is not symmetric (like the Normal distribution), and also if the focus of the study is not the location parameter (mean or median) of the response distribution. If the focus of the study is on the tail of the distribution (how often a threshold is bridged) those measure are not appropriate. The following two measures are appropriate for distributional regression model and can be use for both into bag and out of bag data sets;\n\n\\(\\ell=\\log L\\); the log-likelihood, or equivalent the deviance \\(d=-2 \\ell\\)\nCRPS; the continuous rank probability scores.\n\n\n\n\n\n\n\n\nflowchart TB\n  A[Data] --&gt; B{Multi-split} \n  A --&gt; C{Split once}\n  C --&gt; D[Training]\n  D --&gt; E[Validate]\n  E --&gt; O[Test]\n  B --&gt; F(Rearranged)\n  B --&gt; G(Reweighed)\n  G--&gt; H(Bayesian \\n Bootstrap)\n  G--&gt; J(Boosting)\n  F --&gt; K(Non-param. \\n Bootstrap)\n  F --&gt; L(Cross \\n Validation)\n  L --&gt; M(k-Fold) \n  L --&gt; N(LOO) \n\n\n\n\nFigure 1: Different ways of splitting data to get in order to get more information”\n\n\n\n\n\nPartitioning the data into training, test, and validation sets should be done randomly. A potential risk when splitting the data once arises when the investigation focuses on the tails of the response distribution. In such cases, the few extreme observations may end up in only one of the partitions, leading to potential issues when evaluating the model. Repeatedly reusing parts of the data, such as through bootstrapping or cross-validation, can help mitigate this risk. Multi-splitting the data (shown on the left side of Figure Figure 1) enables the evaluation of appropriate goodness-of-fit measures using out-of-bag data. There are two conceptual ways to multi-split a dataset: the first involves rearranging the data, and the second involves reweighting the observations. [As we will see later, the distinction between these two approaches is not always as clear-cut.] Rearranging the data is typically accomplished by creating appropriate indices that select different parts of the data. For example data in this case are partitioned as data[,index]. This leads to methods such as bootstrapping or cross-validation. On the other hand, reweighting the observations during the fitting process is a technique employed in the Bayesian Bootstrap and boosting.\n\nIn classical bootstrapping, the data are re-sampled randomly with replacement \\(B\\) times, and the model is refitted the same number of times. While computationally expensive, this process provides additional insights into the model and in particular regarding the variability of all its parameters. For distributional regression models, both classical and Bayesian bootstrapping offer extra information about all the fitted parameters of the model which includes the distributional parameters, the fitted coefficients, the hyperparameters, and the random effects. The Bayesian bootstrap re-weights the data as samples from a multinomial distribution and fits the re-weighted models \\(B\\) times. Like the classical bootstrap, it provides valuable information about the variability of all the model parameters. In boosting, each weak learner (i.e., each simple fitted model) is reweighted using the residuals from the previous fits. The results of all these learners are then aggregated into a final model. Boosting can be seen as a method of iteratively improving model accuracy by focusing on the previously mis-fitted observations.\n\n\nIn \\(K\\)-fold cross-validation, the data are split into \\(K\\) sub-samples. The model is then fitted \\(K\\) times, each time using \\(K-1\\) sub-samples for training and the remaining one sub-sample as test data set. By the end of the process, all \\(n\\) observations will have been used as test data exactly once. The main advantage of \\(K\\)-fold cross-validation is that it provides reliable test data for model evaluation, with the cost of fitting \\(K\\) models. Leave-One-Out Cross-Validation (LOO) is a special case of K-fold cross-validation, where the model is trained on \\(n-1\\) observations and tested on the remaining single observation, iterating this process \\(n\\) times. Consequently, \\(n\\) different models are refitted. While computationally intensive, LOO ensures that each data point is used for testing, providing an unbiased evaluation of model performance. Both bootstrapping and \\(K\\)-fold cross-validation, when appropriate performance metrics are used for the specific problem, can help in several ways: i) by facilitating model comparison , ii) by avoiding overfitting, and iii) by assisting in the selection of hyper-parameters.\nNote that the partition of the data in any distributional regression model, can be achieved different ways;\n\nby splitting the original data frame in different subsets;\nby using a factor which classifies the partitions;\nby indexing the original data i.e. data[index,],\nby using prior weights in the fitting.\n\n\nSo there is no inherent need for physically partition into subgroups when performing a single or multiple partitions since the same results can be achieved with different methods which could be more appropriate for the data in hand.\nFor example for prior weights in \\(K\\)-fold cross-validation, we need \\(K\\) dummy vectors \\(i_k\\), where $k = 1, , K. These dummy vectors take the value of 1 for the \\(k\\)-th set of observations and \\(0\\) for the rest. The dummy vectors \\(i_k\\) can be used in two ways:\n\nTo select the observations from the data.frame for fitting.\nAs prior weights when fitting the model.\n\nLet da represent the data.frame used to fit the model, and \\(i\\) the \\(i_k\\) a dummy index. vector. For each of the \\(K\\) fits, the model fitting process would use the command \\(data=da[,i]\\) to select the appropriate data, and newdata=da[,i==0] for evaluating the out-of-bag measure. Alternatively, one can use weights=i during the model fitting and newdata=da[,i==0] for evaluating any out-of-bag measures. Note that the appropriate measures for comparing models include predictive deviance (PD) and continuous rank probability scores (CRPS). The function data_Kfold() in the package generates the correct dummy vectors as a matrix of dimensions n B, which can be used for the cross-validation process.\n\nFor the classical bootstrap, we need \\(B\\) vectors. Unlike cross-validation (CV), where the vectors for indexing and prior weights are identical, bootstrapping requires different vectors. The function data_boot_index() creates the appropriate vectors for indexing, while data_boot_weights() generates the vectors for prior weighting. These vectors should differ whether used as indices to select the relevant columns from the data matrix da or as prior weights, which indicate how many times each observation is selected. Typically, a vector like \\((1, 0, 2, \\ldots, 3, 2)\\) means that observation \\(1\\) is picked once, observation \\(2\\) is excluded from the fit, observation \\(3\\) is selected twice, and so on. For Bayesian bootstrap, the \\(B\\) vectors of length \\(n\\) represent weights that sum to \\(n\\). An out-of-bag observation, in this case, is one with zero weight. This distinction makes the split between rearranged and reweighed in Figure 1 somewhat artificial, as both methods can be interpreted as refitting approaches using prior weights.\n\n\n\n\n\n\nTable 4: The data partition functions\n\n\n\n\n\n\n\n\n\nFunctions\nUsage\n\n\n\n\ndata_part()\nCreates a single or multiple (CV) partitions by introducing a factor with different levels\n\n\ndata_part_list()\nCreates a single or multiple (CV) partitions with output a list of data.frames\n\n\ndata_boot_index()\nCreates two lists. The in-bag,IB, indices for fitting and the out-of-bag, OOB, indices for prediction\n\n\ndata_boot_weights()\nCreate a \\(n \\times B\\) matrix with columns weights for bootstrap fits\n\n\ndata_Kfold_index()\nCreates a \\(n \\times K\\) matrix of variables to be used for cross validation data indexing\n\n\ndata_Kfold_weights()\nCreates a \\(n \\times K\\) matrix of dummy variables to be used for cross validation weighted fits\n\n\ndata_cut()\nThis is not a partition funtion but randomly select a specified porpotion of the data\n\n\n\n\n\n\nHere are the data partition functions.\n\ndata_part()\nThe function data_part() it does not partitioning the data as such but adds a factor called partition to the data indicating the partition. By default the data are partitioned into two sets the training ,train, and the test data, test.\n\ndap &lt;- gamlss.prepdata:::data_part(da)\n\ndata partition into two sets \n\nhead(dap)\n\n      rent area yearc location bath kitchen cheating partition\n1 109.9487   26  1918        2    0       0        0     train\n2 243.2820   28  1918        2    0       0        1      test\n3 261.6410   30  1918        1    0       0        1     train\n4 106.4103   30  1918        2    0       0        0      test\n5 133.3846   30  1918        2    0       0        1      test\n6 339.0256   30  1918        2    0       0        1     train\n\n\nIf the user would like to split the data in two separate data.frame’s she/he can use;\n\ndaTrain &lt;- subset(dap, partition==\"train\")\ndaTest &lt;- subset(dap, partition==\"test\")\ndim(daTrain)\n\n[1] 1846    8\n\ndim(daTest)\n\n[1] 1236    8\n\n\nNote, that use of the option partition has the following behaviour;\n\npartition=2L (the default) the factor has two levels train, and test.\npartition=3L the factor has three levels train, val (for validation) and test.\npartition &gt; 4L say K then the levels are 1, 2…K. The factor then can be used to identify K-fold cross validation, see also the function data_Kfols_CV().\n\nback to table\n\n\ndata_part_list()\nThe function data_part_list() creates a list of dataframes which can be used either for single partition or for cross validation. The argument partition allows the splitting of the data to up to 20 subsets. The default is a list of 2 with elements training and test (single partition).\nHere is a single partition in train and test data sets.\n\nallda &lt;-  data_part_list(rent) \nlength(allda)\n\n[1] 2\n\ndim(allda[[\"train\"]]) # training data\n\n[1] 1200    9\n\ndim(allda[[\"test\"]]) # test data\n\n[1] 769   9\n\n\nHere is a multiple partition for cross validation.\n\nallda &lt;-  data_part_list(rent, partition=10) \n\n10-fold data partition \n\nlength(allda)\n\n[1] 10\n\nnames(allda)\n\n [1] \"1\"  \"2\"  \"3\"  \"4\"  \"5\"  \"6\"  \"7\"  \"8\"  \"9\"  \"10\"\n\n\nback to table\n\n\ndata_boot_index()\nThe function data_boot_index() create two lists. The first called IB (for in-bag) and the second OOB, (out-of-bag). Each ellement of the list IB can be use to select a bootstrap sample from the original data.frame while each element of OOB can be use to select the out of sample data points. Note that each bootstap sample approximately contains 2/3 of the original data so we expact on avarage to 1/3 to be in OOB sample.\n\nDD &lt;- data_boot_index(rent, B=10)\n\nThe class, length and names of the created lists;\n\nclass(DD)\n\n[1] \"list\"\n\nlength(DD)\n\n[1] 2\n\nnames(DD)\n\n[1] \"IB\"  \"OOB\"\n\n\nthe first obsevations of the\n\nhead(DD$IB[[1]]) # in bag\n\n[1] 1 2 4 6 8 8\n\nhead(DD$OOB[[1]]) # out-of-bag\n\n[1]  3  5  7 15 21 22\n\n\nback to table\n\n\ndata_boot_weights()\nThe function data_boot_weights() create a \\(n \\times B\\). matrix with columns possible weights for bootstap fits. Note that each bootstap sample approximately contains .666 of the original obsbation so we in general we expect 0.333 of the data to have zero weights.\n\nMM &lt;- data_boot_weights(rent, B=10)\n\nThe M is a matrix which columns can be used as weights in bootstrap fits; Here is a plot of the first column\n\nplot(MM[,1])\n\n\n\n\n\n\n\n\nand here is the first 6 rows os the matrix;\n\nhead(MM)\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n[1,]    1    2    2    1    0    1    3    1    3     1\n[2,]    1    1    0    2    2    1    2    1    1     2\n[3,]    0    0    2    1    2    1    2    1    1     0\n[4,]    1    0    1    0    3    0    1    1    0     2\n[5,]    0    2    3    1    1    1    2    1    2     1\n[6,]    1    0    0    1    0    1    4    0    1     0\n\n\nback to table\n\n\ndata_Kfold_index()\nThe function data_Kfold() creates a matrix which columns can be use for cross validation either as indices to select data for fitting or as prior weights.\n\nPP &lt;- data_Kfold_index(rent, K=10)\nhead(PP)\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n[1,]    1    1    1    1    0    1    1    1    1     1\n[2,]    2    2    0    2    2    2    2    2    2     2\n[3,]    3    3    3    3    3    3    3    3    0     3\n[4,]    4    4    4    4    4    0    4    4    4     4\n[5,]    5    5    5    5    0    5    5    5    5     5\n[6,]    6    6    6    6    6    6    6    0    6     6\n\n\nback to table\n\n\ndata_Kfold_weights()\nThe function data_Kfold() creates a matrix which columns can be use for cross validation either as indices to select data for fitting or as prior weights.\n\nPP &lt;- data_Kfold_weights(rent, K=10)\nhead(PP)\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n[1,]    1    1    1    1    0    1    1    1    1     1\n[2,]    1    1    0    1    1    1    1    1    1     1\n[3,]    1    1    1    1    1    1    1    1    0     1\n[4,]    1    1    1    1    1    0    1    1    1     1\n[5,]    1    1    1    1    0    1    1    1    1     1\n[6,]    1    1    1    1    1    1    1    0    1     1\n\n\nback to table\n\n\ndata_cut()\nThe function data_cut() is not included in the partition Section for its data partitioning properties. It is designed to select a random subset of the data specifically for plotting purposes. This is especially useful when working with large datasets, where plotting routines—such as those from the ggplot2 package—can become very slow.\nThe data_cut() function can either: • Automatically reduce the data size based on the total number of observations, or • Subset the data according to a user-specified percentage.\nHere is an example of the function assuming that that the user requires \\(50%\\) of the data;\n\nda1 &lt;- data_cut(rent99, percentage=.5)\n\n 50 % of data are saved, \nthat is, 1541 observations. \n\ndim(rent99)\n\n[1] 3082    9\n\ndim(da1)\n\n[1] 1541    9\n\n\nThe function data_cut() is used extensively in many plotting routines within the gamlss.ggplots package. When the percentage option is not specified, a default rule is applied to determine the proportion of the data used for plotting. This approach balances performance and visualization quality when working with large datasets.\nLet \\(n\\) denote the total number of observations. Then:\n\nif \\(n \\le 50,000\\): All data are used for plotting.\nif \\(50,000 &lt; n \\le 100,000\\): 50% of the data are used.\nIf \\(100,000 &lt; n \\le 1,000,000\\): 20% of the data are used.\nIf \\(n&gt;1,000,000\\) : 10% of the data are used.\n\nThis default behaviour ensures that plotting remains efficient even with very large datasets.\nback to table"
  },
  {
    "objectID": "gamlss-prepdata.html#sec-Family",
    "href": "gamlss-prepdata.html#sec-Family",
    "title": "The Functions in the Package gamlss.prepdata",
    "section": "Distribution Families",
    "text": "Distribution Families\n\n\n\n\n\n\nNote\n\n\n\nAll functions listed below belong to the gamlss.ggplots package, not the gamlss.prepdata package. However, since they can be useful during the pre-modeling stage, they are presented here. The later version of the gamlss.ggplots can be found in https://github.com/gamlss-dev/gamlss.ggplots\n\n\nIn the following function, no fitted model is required—only values for the parameters need to be specified.\n\nfamily_pdf()\nThe function family_pdf() plots individual probability density functions (PDFs) from distributions in the gamlss.family package. Although it typically requires the family argument, it defaults to the Normal distribution (NO) if none is provided.\n\nfamily_pdf(from=-5,to=5, mu=0, sigma=c(.5,1,2))\n\n\n\n\n\n\n\nFigure 2: Continuous response example: plotting the pdf of a normal random variable.\n\n\n\n\n\nThe following demonstrates how discrete distributions are displayed. We begin with a distribution that can take infinitely many count values—the Negative Binomial distribution;\n\nfamily_pdf(NBI, to=15, mu=1, sigma=c(.5,1,2), alpha=.9, size.seqment = 3)\n\n\n\n\n\n\n\nFigure 3: Count response example: plotting the pdf of a beta binomial.\n\n\n\n\n\nHere we use a discrete distribution with a finite range of possible values: the Beta-Binomial distribution.\n\nfamily_pdf(BB, to=15, mu=.5, sigma=c(.5,1,2),  alpha=.9, , size.seqment = 3)\n\n\n\n\n\n\n\nFigure 4: Beta binomial response example: plotting the pdf of a beta binomial.\n\n\n\n\n\n\n\nfamily_cdf\nThe function family_cdf() plots cumulative distribution functions (CDFs) from the gamlss.family distributions. The primary argument required is the family specifying the distribution to be used.\n\nfamily_cdf(NBI, to=15, mu=1, sigma=c(.5,1,2), alpha=.9, size.seqment = 3)\n\n\n\n\n\n\n\nFigure 5: Count response example: plotting the cdf of a negative binomial.\n\n\n\n\n\nThe CDf of the negative binomial;\n\nfamily_cdf(BB, to=15, mu=.5, sigma=c(.5,1,2),  alpha=.9, , size.seqment = 3)\n\n\n\n\n\n\n\nFigure 6: Count response example: plotting the cdf of a beta binomial.\n\n\n\n\n\n\n\nfamily_cor()\nThe function family_cor() offers a basic method for examining the inter-correlation among the parameters of any distribution from the gamlss.family. It performs the following steps:\n\nGenerates 10,000 random values from the specified distribution.\nFits the same distribution to the generated data.\nExtracts and plots the correlation coefficients of the distributional parameters.\n\nThese correlation coefficients are derived from the variance-covariance matrix of the fitted model.\n\n\n\n\n\n\nWarning\n\n\n\nThis method provides only a rough indication of how the parameters are correlated at specific values of the distribution’s parameters. The correlation structure may vary significantly at different points in the parameter space, as the distribution can behave quite differently depending on those values.\n\n\n\n#source(\"~/Dropbox/github/gamlss-ggplots/R/family_cor.R\")\ngamlss.ggplots:::family_cor(\"BCTo\", mu=1, sigma=0.11, nu=1, tau=5, no.sim=10000)\n\n\n\n\n\n\n\nFigure 7: Family correlation of a BCTo distribution at specified values of the parameters."
  },
  {
    "objectID": "gamlss-ggplots.html",
    "href": "gamlss-ggplots.html",
    "title": "The Functions in gamlss.ggplots",
    "section": "",
    "text": "This booklet provides an overview of the R package gamlss.ggplots and its functions, focusing on their use and the output they generate.\nThe gamlss.ggplots package offers a set of ggplot2-based visual tools for diagnostic and exploratory plots specifically tailored for models fitted using gamlss() and gamlss2(). Most of the functions in this package are compatible with both model types, making them broadly useful across the Generalized Additive Models for Location, Scale and Shape framework.\nSupported Model Objects\n\ngamlss — the original modeling interface for GAMLSS\ngamlss2 — a modern interface with cleaner formula syntax and streamlined model handling\n\n\n\n\nThe latest versions of the key packages are:\n\nrm(list=ls())\n#getRversion()\nlibrary(gamlss)\nlibrary(gamlss2)\nlibrary(ggplot2)\nlibrary(gamlss.ggplots)\nlibrary(\"dplyr\") \npackageVersion(\"gamlss\")\n\n[1] '5.4.23'\n\npackageVersion(\"gamlss2\")\n\n[1] '0.1.0'\n\npackageVersion(\"gamlss.ggplots\")\n\n[1] '2.1.17'\n\n\n\n\n\nOriginally, the package gamlss.ggplots included all the functionality currently split between gamlss.ggplots and gamlss.prepdata. Due to maintenance challenges caused by the growing size and complexity of the package, it was divided into two separate components:\n- `gamlss.ggplots` — visualization and diagnostics\n\n- `gamlss.prepdata` — data preparation utilities\nAt this stage, gamlss.ggplots is experimental. Some functions are not yet exported (i.e., “hidden”) to allow for further testing and validation. These hidden functions can still be accessed using the triple-colon syntax:\n\ngamlss.ggplots:::function_name()\n\n\n\nFunctions in gamlss.ggplots can be broadly classified into three categories:\n\n\nMost if the before-fitting a model functions are moved to gamlss.prepdata package but few visualizing distribution families are remaining. Distribution relatated function names begin with family_Name() - Example: family_pdf(NO) which plots the density distribution from the Normal GAMLSS family.\n\n\n\nThese functions are used after a model has been fitted, providing diagnostics, interpretation aids, and predictions.\n\nResidual-based diagnostics - resid_NAME() – Diagnostics on residuals E.g., resid_wp() produces a worm plot for assessing the distribution.\nFitted value visualizations - fitted_NAME() – Visualizes fitted parameters (\\(\\mu\\), \\(\\sigma\\), etc.) from a fitted model.\nResponse variable plots resp_NAME() – Plots the response variable against various model-based quantities such as parameters or quantiles.\nModel interpretation\n\n\npe_NAME() – Plots partial effects to aid model interpretation.\ninfluence_NAME() – Shows influence measures of model terms.\npredict_NAME() – Provides predictions, often using a newdata argument.\n\n\nModel comparison diagnostics\n\nmodel_NAME() – Functions for comparing multiple models, typically using diagnostic statistics or GAIC.\n\n\n\n\n\nThese functions serve as helpers for data exploration and bootstrap analyses.\n\nSingle-vector visualizations • y_NAME() – For example, y_hist(y) plots a histogram, y_acf(y) plots the autocorrelation function.\nBootstrap summaries • boot_NAME() – Plots parameter estimates or fitted values across bootstrap samples.\nFunction imitations\n\nSome utility functions imitate existing gamlss functions. E.g., histSmo_plot() mimics histSmo() but uses ggplot2.\nBecause the package is at an experimental stage, some of the functions are hidden to allowed time for checking. The hidden functions can be accessed using gamlss.ggplots:::functionname().\nNote future functions that are not included in the package at the moment are:\n\nale_param() for accumulated local effects of a specific term on the parameter (Mikis)\npd_param() for partial dependants plots of a specific term on the parameter (Mikis)\npe_exceedance() (Julian)\n\n\n\n\n\n\nThe general GAMLSS model can be written as \\[\n\\begin{split}\ny_i     &  \\stackrel{\\small{ind}}{\\sim }  \\mathcal{D}( \\theta_1, \\ldots, \\theta_k) \\nonumber \\\\\ng(\\theta_1) &= \\mathcal{ML}_1(x_{1i},x_{2i}, \\ldots,  x_{pi}) \\nonumber \\\\\n\\ldots &= \\ldots \\nonumber\\\\\ng(\\theta_k) &= \\mathcal{ML}_k(x_{1i},x_{2i}, \\ldots,  x_{pi}).\n\\end{split}\n  \\tag{1}\\] When only additive smoothing terms are fitted the model can be written; \\[\\begin{split}\ny_i     &  \\stackrel{\\small{ind}}{\\sim }  \\mathcal{D}( \\theta_1, \\ldots, \\theta_k) \\nonumber \\\\\ng(\\theta_1)  &= b_0 + s_1(x_{1i})  +  \\cdots,  +s_p(x_{pi}) \\nonumber\\\\\n\\cdots &=& \\cdots \\nonumber\\\\\ng(\\theta_k)  &= b_0 + s_1(x_{1i})  +   \\cdots,  +s_p(x_{pi}).\n\\end{split}\n\\tag{2}\\]"
  },
  {
    "objectID": "gamlss-ggplots.html#the-functions",
    "href": "gamlss-ggplots.html#the-functions",
    "title": "The Functions in gamlss.ggplots",
    "section": "",
    "text": "This booklet provides an overview of the R package gamlss.ggplots and its functions, focusing on their use and the output they generate.\nThe gamlss.ggplots package offers a set of ggplot2-based visual tools for diagnostic and exploratory plots specifically tailored for models fitted using gamlss() and gamlss2(). Most of the functions in this package are compatible with both model types, making them broadly useful across the Generalized Additive Models for Location, Scale and Shape framework.\nSupported Model Objects - gamlss — the original modeling interface for GAMLSS -gamlss2 — a modern interface with cleaner formula syntax and streamlined model handling"
  },
  {
    "objectID": "gamlss-ggplots.html#the-model",
    "href": "gamlss-ggplots.html#the-model",
    "title": "The Functions in gamlss.ggplots",
    "section": "",
    "text": "The general GAMLSS model can be written as \\[\n\\begin{split}\ny_i     &  \\stackrel{\\small{ind}}{\\sim }  \\mathcal{D}( \\theta_1, \\ldots, \\theta_k) \\nonumber \\\\\ng(\\theta_1) &= \\mathcal{ML}_1(x_{1i},x_{2i}, \\ldots,  x_{pi}) \\nonumber \\\\\n\\ldots &= \\ldots \\nonumber\\\\\ng(\\theta_k) &= \\mathcal{ML}_k(x_{1i},x_{2i}, \\ldots,  x_{pi}).\n\\end{split}\n  \\tag{1}\\] When only additive smoothing terms are fitted the model can be written; \\[\\begin{split}\ny_i     &  \\stackrel{\\small{ind}}{\\sim }  \\mathcal{D}( \\theta_1, \\ldots, \\theta_k) \\nonumber \\\\\ng(\\theta_1)  &= b_0 + s_1(x_{1i})  +  \\cdots,  +s_p(x_{pi}) \\nonumber\\\\\n\\cdots &=& \\cdots \\nonumber\\\\\ng(\\theta_k)  &= b_0 + s_1(x_{1i})  +   \\cdots,  +s_p(x_{pi}).\n\\end{split}\n\\tag{2}\\]"
  },
  {
    "objectID": "gamlss-ggplots.html#family_pdf",
    "href": "gamlss-ggplots.html#family_pdf",
    "title": "The Functions in gamlss.ggplots",
    "section": "family_pdf()",
    "text": "family_pdf()\nThe function family_pdf() plots individual pdf’s from gamlss.family distribution. It needs the family argument.\n\nContinuous\n\nfamily_pdf(NO, from=-5,to=5, mu=0, sigma=c(.5,1,2))\n\n\n\n\n\n\n\nFigure 1: Continuous response example: plotting the pdf of a normal random variable.\n\n\n\n\n\n\n\nDiscrere counts\n\nfamily_pdf(NBI, to=15, mu=1, sigma=c(.5,1,2), alpha=.9, size.seqment = 3)\n\n\n\n\n\n\n\nFigure 2: Count response example: plotting the pdf of a beta binomial.\n\n\n\n\n\n\n\nDiscrere binomial type\n\nfamily_pdf(BB, to=15, mu=.5, sigma=c(.5,1,2),  alpha=.9, , size.seqment = 3)\n\n\n\n\n\n\n\nFigure 3: Beta binomial response example: plotting the pdf of a beta binomial."
  },
  {
    "objectID": "gamlss-ggplots.html#family_cdf",
    "href": "gamlss-ggplots.html#family_cdf",
    "title": "The Functions in gamlss.ggplots",
    "section": "family_cdf",
    "text": "family_cdf\nThe function plots individual cdf’s from gamlss.family distribution. It needs the family argument.\n\nContinuous\n\nfamily_cdf(NO, from=-5,to=5, mu=0, sigma=c(.5,1,2))\n\n\n\n\n\n\n\nFigure 4: Continuous response example: plotting the cdf of a normal random variable.\n\n\n\n\n\n\n\nDiscrere counts\n\nfamily_cdf(NBI, to=15, mu=1, sigma=c(.5,1,2), alpha=.9, size.seqment = 3)\n\n\n\n\n\n\n\nFigure 5: Count response example: plotting the cdf of a negative binomial.\n\n\n\n\n\n\n\nDiscrere binomial type\n\nfamily_cdf(BB, to=15, mu=.5, sigma=c(.5,1,2),  alpha=.9, , size.seqment = 3)\n\n\n\n\n\n\n\nFigure 6: Count response example: plotting the cdf of a beta binomial."
  },
  {
    "objectID": "gamlss-ggplots.html#family_cor",
    "href": "gamlss-ggplots.html#family_cor",
    "title": "The Functions in gamlss.ggplots",
    "section": "family_cor()",
    "text": "family_cor()\nThe function family_cor() provides a crude way of checking the inter correlation of parameters within any gamlss.family distribution. It\n\ngenerates 10000 values from the specified distribution,\nfits the distribution to the generared data and\nplots the correlation coefficients of the parameters.\n\nThose correlation coefficients are taken from the fitted variance covariance matrix.\n\n\n\n\n\n\nWarning\n\n\n\nThe method only provides an idea of how the correlations between parameters are at specified points of the distribution parameters. At different parameter points the distribution could behave completely different.\n\n\n\n#source(\"~/Dropbox/github/gamlss-ggplots/R/family_cor.R\")\ngamlss.ggplots:::family_cor(\"BCTo\", mu=1, sigma=0.11, nu=1, tau=5, no.sim=10000)\n\n\n\n\n\n\n\nFigure 7: Family correlation of a BCTo distribution at specified values of the parameters."
  },
  {
    "objectID": "gamlss-ggplots.html#resid_index",
    "href": "gamlss-ggplots.html#resid_index",
    "title": "The Functions in gamlss.ggplots",
    "section": "resid_index()",
    "text": "resid_index()\nBy plotting, the residual against the data index we would expect that there is no pattern in the plot because of the assumption that the observations (given the explanatory variables) are independent. Figure 8 show the standard residual plot against the index of the data. The argument value has to do with the outliers hoghlighted in the plot and can be change to a higher cut-off point. Other type of residuals, suitable standardised, not necessarily GAMLSS model residuals, can be plotted by using the argument resid.\n\ngg &lt;-resid_index(g4)\ngg\n\n\n\n\n\n\n\nFigure 8: Residual plot.\n\n\n\n\n\n\nDifferent models\nResidual plots from different models can be plotted in different graphs see Figure 9.\n\ng11 &lt;-resid_index(g1)\ng12 &lt;-resid_index(g2)\ng13 &lt;-resid_index(g3)\ng14 &lt;-resid_index(g4)\nlibrary(gridExtra)\ngrid.arrange(g11,g12,g13,g14 )\n\n\n\n\n\n\n\nFigure 9: Residual plot.\n\n\n\n\n\n\n\nDifferent range of an x-variable\nHere the residual plot is split against one continuous explanatory variables Fl using the function facet_wrap(). The function splits in three different cut points because the cut_number(rent\\$Fl, 3) is used. Note that in order for facet_wrap() to work we had to suppress the horizontal likes using the argument no.lines=TRUE.\n\nresid_index(g1, no.lines=TRUE)+\n   facet_wrap(~cut_number(rent$Fl, 3))\n\n\n\n\n\n\n\nFigure 10: Resisduals plot against the x-variable split in tree sections.\n\n\n\n\n\nHere we split according to one continuous an one categorical x-variables.\n\nresid_index(g1, no.lines=TRUE) +\n  facet_grid(cut_number(rent$Fl, 3)~rent$loc)\n\n\n\n\n\n\n\nFigure 11: Residuals plot against two x-variable split in tree sections."
  },
  {
    "objectID": "gamlss-ggplots.html#resid_mu",
    "href": "gamlss-ggplots.html#resid_mu",
    "title": "The Functions in gamlss.ggplots",
    "section": "resid_mu()",
    "text": "resid_mu()\nA plot of the residual against the fitted values of the model, usually reveals whether there is a heterogeneity in the data. The resid_mu() plots the residual from model m1 against the fitted values for the model for \\(\\mu\\):\n\nresid_mu(g1)\n\n\n\n\n\n\n\nFigure 12: Residuals plot against the fitted values for models m1\n\n\n\n\n\nWe can observe a fan type of behaviour in the plot, that is the residuals are become bigger for larger fitted values of the response. This behaviour is typical for models with heterogeneity in the data. Remember m1 does not have a model for \\(\\sigma\\). In model m4 we fit an mode for \\(\\sigma\\) so the residuals are better.\n\nresid_mu(g4)\n\n\n\n\n\n\n\nFigure 13: Residuals plot against the fitted values for models m4"
  },
  {
    "objectID": "gamlss-ggplots.html#resid_quantile",
    "href": "gamlss-ggplots.html#resid_quantile",
    "title": "The Functions in gamlss.ggplots",
    "section": "resid_quantile()",
    "text": "resid_quantile()\nResidual plots from model m1 against the fitted quantile (including the median) values can be obtained using:\n\nresid_quantile(g4)\n\n\n\n\nResiduals plot against the fitted 50 percent fitted quantiles of model m4"
  },
  {
    "objectID": "gamlss-ggplots.html#resid_xvar",
    "href": "gamlss-ggplots.html#resid_xvar",
    "title": "The Functions in gamlss.ggplots",
    "section": "resid_xvar()",
    "text": "resid_xvar()\n\nAgainst a continuous explanatory variable\nResidual plots against a continuous explanatory variables can be plotted using: resid_xvar():\n\nresid_xvar(g1,Fl)\n\n\n\n\nResisduals plot against the x-variables\n\n\n\nresid_xvar(g1,A)\n\n\n\n\nResisduals plot against the x-variables\n\n\n\n\n\n\nAgainst a categorical explanatory variable\nThe function is working differently for categorical x-variables.\n\nresid_xvar(g1,H)\n\n\n\n\nResisduals plot against categorical x-variables for model m1\n\n\n\nresid_xvar(g1,loc)\n\n\n\n\nResisduals plot against categorical x-variables for model m1\n\n\n\n\nOr for a different models:\n\nresid_xvar(g4,loc)\n\n\n\n\nResisduals plot against categorical x-variables for model m4"
  },
  {
    "objectID": "gamlss-ggplots.html#resid_qqplot",
    "href": "gamlss-ggplots.html#resid_qqplot",
    "title": "The Functions in gamlss.ggplots",
    "section": "resid_qqplot()",
    "text": "resid_qqplot()\nThe function resid_qqplot() can be use to get a QQ-plot of the residuals.\n\ngg &lt;-resid_qqplot(g1)\ngg\n\n\n\n\n\n\n\nFigure 14: QQ-plot for model m1\n\n\n\n\n\nThe plot appears in Figure 14.\nFor comparing two different models you can use\nTo add another model QQ-plot try\n\n\n\n\n\n\nWarning\n\n\n\nadd_resid_qqplot is not working for extra gamlss2 objects\n\n\n\ngg1 &lt;-add_resid_qqplot(gg, m4)\n#gg1\n\nor alternatively you can use the function model_qqplot() which can display more than two models.\n\nFor different values of x-variables\n\nOne x-variable\n\ngg+facet_grid(cut_number(rent$Fl, 4))+ \n  ggtitle(\"qqplot of model m1 against Fl\") \n\n\n\n\n\n\n\nFigure 15: QQ-plot of the residuals of model m1 against Fl and `A’\n\n\n\n\n\n\n\nTwo x-variables\n\ngg+facet_grid(cut_number(rent$Fl, 3)~rent$loc)+ \n  ggtitle(\"qqplot of model m1 against Fl and A\") \n\n\n\n\n\n\n\nFigure 16: QQ-plot of the residuals of model m1 against Fl and `A’\n\n\n\n\n\n\ngg1+facet_grid(cut_number(rent$Fl, 3)~rent$loc)+ \n  ggtitle(\"qqplot of models m1 and m4 against Fl and A\")\n\n\n\n\n\n\n\nFigure 17: QQ-plot of the residuals of models m1 and m4 against Fl and `A’"
  },
  {
    "objectID": "gamlss-ggplots.html#resid_wp-worm-plots",
    "href": "gamlss-ggplots.html#resid_wp-worm-plots",
    "title": "The Functions in gamlss.ggplots",
    "section": "resid_wp(), Worm plots",
    "text": "resid_wp(), Worm plots\nThe worm plot for a single model can be plotted using:\n\ngg&lt;-resid_wp(g4)\ngg\n\n\n\n\n\n\n\nFigure 18: Worm-plot of the residuals for model m4\n\n\n\n\n\nSee Figure 18."
  },
  {
    "objectID": "gamlss-ggplots.html#resid_density",
    "href": "gamlss-ggplots.html#resid_density",
    "title": "The Functions in gamlss.ggplots",
    "section": "resid_density()",
    "text": "resid_density()\n\nThe single density\nThe function resid_density() plots the density of the residuals for one model while the function model_densitity() for more than one model.\nHere we plot the density of the residuals for model m4. See Figure Figure 20 for the plot.\n\ngg&lt;-resid_density(g4)\ngg\n\n\n\n\n\n\n\nFigure 20: A density plot of the residuals\n\n\n\n\n\n\n\nThe multiple densities\nHere we plot the density of the residuals for model m4 against two explanatory variables, Fl and A.\n\ngg+facet_grid(cut_number(rent$Fl, 3)~rent$loc)\n\n\n\n\n\n\n\nFigure 21: A density plot of the residuals against Fl and A"
  },
  {
    "objectID": "gamlss-ggplots.html#resid_ecdf",
    "href": "gamlss-ggplots.html#resid_ecdf",
    "title": "The Functions in gamlss.ggplots",
    "section": "resid_ecdf()",
    "text": "resid_ecdf()\nFigure Figure 22 show the empirical cumulative distribution function of the residuals of model m4\n\ngg &lt;- resid_ecdf(g4)\ngg\n\n\n\n\n\n\n\nFigure 22: The ECDF of the residuals.\n\n\n\n\n\nBelow cdf of a normally distributed variable with \\(\\mu=0\\) and \\(\\sigma=1\\) is added to the previous plot.\n\ngg+stat_function(fun = pNO, args=list(mu=0, sigma=1), col=\"red\")\n\n\n\n\n\n\n\nFigure 23: The ECDF of the residuals with added normal distribution."
  },
  {
    "objectID": "gamlss-ggplots.html#resid_dtop",
    "href": "gamlss-ggplots.html#resid_dtop",
    "title": "The Functions in gamlss.ggplots",
    "section": "resid_dtop()",
    "text": "resid_dtop()\nThe function resid_dtop() plot a de-trended empirical cdf plot. See Figure 24 for the plot.\n\ngg&lt;-resid_dtop(g4)\ngg\n\n\n\n\n\n\n\nFigure 24: A detrended Own’s plot."
  },
  {
    "objectID": "gamlss-ggplots.html#resid_plots",
    "href": "gamlss-ggplots.html#resid_plots",
    "title": "The Functions in gamlss.ggplots",
    "section": "resid_plots()",
    "text": "resid_plots()\nThe function resid_plots() tries to imitate the function plot.gamlss() of the gamlss package.\n\nresid_plots(g4)\n\n\n\n\n\n\n\nFigure 25: The plot of residuals.\n\n\n\n\n\n\nDifferent themes\nThere are also different themes in the plot. Next in Figure Figure 26 we are trying theme=\"new\":\n\nresid_plots(g4, theme=\"new\")\n\n\n\n\n\n\n\nFigure 26: The plot of residuals with `theme=new’.\n\n\n\n\n\nFigure 27 has theme=\"ecdf\":\n\nresid_plots(g4, theme=\"ecdf\")\n\n\n\n\n\n\n\nFigure 27: The plot of residuals with `theme=ecdf’.\n\n\n\n\n\nFigure 28 has theme=\"ecdf\":\n\n  resid_plots(g4, theme=\"ts\")\n\n\n\n\n\n\n\nFigure 28: The plot of residuals with `theme=ts’."
  },
  {
    "objectID": "gamlss-ggplots.html#the-symmetry-plot-resid_symmetry",
    "href": "gamlss-ggplots.html#the-symmetry-plot-resid_symmetry",
    "title": "The Functions in gamlss.ggplots",
    "section": "",
    "text": "A symmetry plot is useful for detecting asymmetry (skewnwess) in the residuals. Here we plot the symmetry plot of the residuals for model m4.\n\ngg&lt;-resid_symmetry(g4)\ngg\n\n\n\n\n\n\n\nFigure 28: A symmetry plot of the residuals."
  },
  {
    "objectID": "gamlss-ggplots.html#sec-respmu",
    "href": "gamlss-ggplots.html#sec-respmu",
    "title": "The Functions in gamlss.ggplots",
    "section": "resp_mu()",
    "text": "resp_mu()\nPlotting, the response variable against the fitted values is a traditional way of checking the adequacy of the model in linear regression model situation. The closer the points of the plot to 45% line the better the model. Also a hight correlation coeficient indicates a good fit. For GAMLSS this is equivalent of plotting the response against the fitted values of the \\(\\mu\\) model. A strong linear pattern indicates that the \\(\\mu\\) model is adequate.\nFigure 30 plots the response variable against the fitted values for \\(\\mu\\). The plot also show the 45 degrees line between the two variables. The relation should be close to linear as possible. The correlation between the response and the fitted values for \\(\\mu\\) is 0.6154328, -0.1275195 which is reasonable hight.\n\nresp_mu(g4)\n\n\n\n\n\n\n\nFigure 30: Response against the mu fitted values."
  },
  {
    "objectID": "gamlss-ggplots.html#resp_param",
    "href": "gamlss-ggplots.html#resp_param",
    "title": "The Functions in gamlss.ggplots",
    "section": "resp_param()",
    "text": "resp_param()\nGAMLSS have more than one parameters, so plotting the response variable against the the other parameters fitted values could be of interest. The function resp_param() can do that. It plots the response against any parameter fitted values. Figure 31 shows the response against the fitted values for \\(\\mu\\) and \\(\\sigma\\) respectively for model g4. The first plot has the usual interpretation describes in Section Section 5.1 the second need more thought.\n\nresp_param(g4)\n# resp_param(g4, \"sigma\")\n\n\n\n\n\n\n\nFigure 31: Residuals plot against the fitted mu and sigma, respectively, for model `g4’"
  },
  {
    "objectID": "gamlss-ggplots.html#resp_quantile",
    "href": "gamlss-ggplots.html#resp_quantile",
    "title": "The Functions in gamlss.ggplots",
    "section": "resp_quantile",
    "text": "resp_quantile\nThe function resp_quantile plots the response variable against any fitted quantile. Here we plot the respose against the 0.95 quantile. Again the interpretation needs more thought.\n\nresp_quantile(g4, quantile=0.95)\n\n\n\n\n\n\n\nFigure 32: The respose against the fitted median."
  },
  {
    "objectID": "gamlss-ggplots.html#fitted_devianceincr",
    "href": "gamlss-ggplots.html#fitted_devianceincr",
    "title": "The Functions in gamlss.ggplots",
    "section": "fitted_devianceIncr()",
    "text": "fitted_devianceIncr()\nThe functionfitted_devianceIncr() plots the deviance increment from a fitted gamlss model.\n\nfitted_devianceIncr(g4)\n\n\n\n\n\n\n\nFigure 33: Plotting deviance increment from a fitted GAMLSS model."
  },
  {
    "objectID": "gamlss-ggplots.html#fitted_leverage",
    "href": "gamlss-ggplots.html#fitted_leverage",
    "title": "The Functions in gamlss.ggplots",
    "section": "fitted_leverage()",
    "text": "fitted_leverage()\nThe function plots the linear leverage from a fitted GAMLSS model.\n\nfitted_leverage(m4)\n\n1969 observations with 5 variables \n\n\n\n\n\n\n\n\nFigure 34: Plotting linear leverage from a fitted GAMLSS model.\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nfitted_leverage is not working for gamlss2 objects"
  },
  {
    "objectID": "gamlss-ggplots.html#fitted_pdf",
    "href": "gamlss-ggplots.html#fitted_pdf",
    "title": "The Functions in gamlss.ggplots",
    "section": "fitted_pdf()",
    "text": "fitted_pdf()\nThe function fitted_pdf() plots individual pdf’s from a fitted model. It needs the argument obs indicating which observation number to plot.\nFirst a continuous distribution:\n\na1 &lt;- gamlss2(y~pb(x),sigma.fo=~x, data=abdom, family=LO)\n\nGAMLSS-RS iteration  1: Global Deviance = 4884.1282 eps = 0.342686     \nGAMLSS-RS iteration  2: Global Deviance = 4778.7262 eps = 0.021580     \nGAMLSS-RS iteration  3: Global Deviance = 4778.6342 eps = 0.000019     \nGAMLSS-RS iteration  4: Global Deviance = 4778.6342 eps = 0.000000     \n\nfitted_pdf(a1, obs=c(500,610),from=280, to=500)\n\n\n\n\n\n\n\nFigure 35: Probability functions for a continuous responses.\n\n\n\n\n\nFor a infinite count response:\n\np1 &lt;- gamlss2(y~pb(x)+qrt, data=aids, family=NBI) \n\nGAMLSS-RS iteration  1: Global Deviance = 366.3831 eps = 0.366850     \nGAMLSS-RS iteration  2: Global Deviance = 359.6389 eps = 0.018407     \nGAMLSS-RS iteration  3: Global Deviance = 359.6228 eps = 0.000044     \nGAMLSS-RS iteration  4: Global Deviance = 359.6228 eps = 0.000000     \n\nfitted_pdf(p1, obs=10:15, from=25, to=130, alpha=.9) \n\n\n\n\n\n\n\nFigure 36: Probability functions for a count responses.\n\n\n\n\n\nThis is a binomial example:\n\nh &lt;- gamlss(y~ward+loglos+year, ~year+ward, family=BB, data=aep)\n\nGAMLSS-RS iteration 1: Global Deviance = 4490.361 \nGAMLSS-RS iteration 2: Global Deviance = 4483.13 \nGAMLSS-RS iteration 3: Global Deviance = 4483.021 \nGAMLSS-RS iteration 4: Global Deviance = 4483.02 \nGAMLSS-RS iteration 5: Global Deviance = 4483.02 \n\nfitted_pdf(h, obs=c(10:15), alpha=.9)\n\n\n\n\n\n\n\nFigure 37: Probability functions for a binomial responses.\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nbinomial type distributions are not working for gamlss2 objects"
  },
  {
    "objectID": "gamlss-ggplots.html#fitted_cdf",
    "href": "gamlss-ggplots.html#fitted_cdf",
    "title": "The Functions in gamlss.ggplots",
    "section": "fitted_cdf()",
    "text": "fitted_cdf()\nThe function fitted_cdf() plots individual pdf’s from a fitted model. It needs the argument obs indicating which observation number to plot. %\nFirst a continuous distribution:\n\nfitted_cdf(a1, obs=c(500,610),from=280, to=500)\n\n\n\n\n\n\n\nFigure 38: Probability functions for a continuous responses.\n\n\n\n\n\nHere is a count response data example\n\nfitted_cdf(p1, obs=10:15, from=25, to=130, alpha=.9)\n\n\n\n\n\n\n\nFigure 39: Probability functions for a count responses.\n\n\n\n\n\nHere is a binomial response data example\n\nh&lt;-gamlss(y~ward+loglos+year, sigma.formula=~year+ward, family=BB, data=aep)\n\nGAMLSS-RS iteration 1: Global Deviance = 4490.361 \nGAMLSS-RS iteration 2: Global Deviance = 4483.13 \nGAMLSS-RS iteration 3: Global Deviance = 4483.021 \nGAMLSS-RS iteration 4: Global Deviance = 4483.02 \nGAMLSS-RS iteration 5: Global Deviance = 4483.02 \n\nfitted_cdf(h, obs=c(10:15),  alpha=.9)\n\n\n\n\n\n\n\nFigure 40: Probability functions for a binomial responses."
  },
  {
    "objectID": "gamlss-ggplots.html#fitted_cdf_data",
    "href": "gamlss-ggplots.html#fitted_cdf_data",
    "title": "The Functions in gamlss.ggplots",
    "section": "fitted_cdf_data()",
    "text": "fitted_cdf_data()\nThe function fitted_cdf_data() plots individual pdf’s from a fitted model but also add the data points. It needs the argument obs indicating which obs\n\nfitted_cdf_data(a1, obs=c(500,610),from=280, to=500)\n\n\n\n\n\n\n\nFigure 41: Probability functions for a binomial responses.\n\n\n\n\n\n\nfitted_centiles()\nSee Section Section 11.1."
  },
  {
    "objectID": "gamlss-ggplots.html#sec-fittedcentiles0",
    "href": "gamlss-ggplots.html#sec-fittedcentiles0",
    "title": "The Functions in gamlss.ggplots",
    "section": "fitted_centiles()",
    "text": "fitted_centiles()\nSee Section Section 9.2."
  },
  {
    "objectID": "gamlss-ggplots.html#model_gaic",
    "href": "gamlss-ggplots.html#model_gaic",
    "title": "The Functions in gamlss.ggplots",
    "section": "model_GAIC()",
    "text": "model_GAIC()\nThe functions model_GAIC() and model_GAIC_lollipop() are identical but the appearance is different.\n\ngg&lt;-model_GAIC(g0,g1,g2,g3,g4, g5,g6,g7,g8,g9)\ngg\n\n\n\n\n\n\n\nFigure 42: plotting the GAIC."
  },
  {
    "objectID": "gamlss-ggplots.html#model_gaic_lollipop",
    "href": "gamlss-ggplots.html#model_gaic_lollipop",
    "title": "The Functions in gamlss.ggplots",
    "section": "model_GAIC_lollipop()",
    "text": "model_GAIC_lollipop()\n\ngg1&lt;-model_GAIC_lollipop(g0,g1,g2,g3,g4, g5,g6,g7,g8,g9)\ngg1\n\n\n\n\n\n\n\nFigure 43: plotting the GAIC."
  },
  {
    "objectID": "gamlss-ggplots.html#model_density",
    "href": "gamlss-ggplots.html#model_density",
    "title": "The Functions in gamlss.ggplots",
    "section": "model_density()",
    "text": "model_density()\nTo plot the residuals densities of all m1, m2, m3, and m4 models use:\n\ngg &lt;-  model_density(g1, g2, g3, g4)\ngg\n\n\n\n\nA density plot of the residuals from different models.\n\n\n\n\nFor multiple plots use:\n\ngg+facet_grid(cut_number(rent$Fl, 3)~rent$loc)\n\n\n\n\nA density plot of the residuals from different models."
  },
  {
    "objectID": "gamlss-ggplots.html#model_qqplot",
    "href": "gamlss-ggplots.html#model_qqplot",
    "title": "The Functions in gamlss.ggplots",
    "section": "model_qqplot()",
    "text": "model_qqplot()\nThe function model_qqplot() can be use to get a QQ-plots for more that one fitted model residuals.\n\ngg &lt;- model_qqplot(g1, g2, g3, g4)\ngg\n\n\n\n\n\n\n\nFigure 44: QQ-plot of the residuals of models m1, m2, m3 and m4.\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nsomething is wrong here g2 and g3 are not appearing\n\n\nFor multiple plots use;\n\ngg+facet_grid(cut_number(rent$Fl, 3)~rent$loc)\n\n\n\n\n\n\n\nFigure 45: Multiple QQ-plots of the residuals of models m1, m2, m3 and m4.\n\n\n\n\n\nThe resulting plot is shown on the Figure 45."
  },
  {
    "objectID": "gamlss-ggplots.html#model_wp",
    "href": "gamlss-ggplots.html#model_wp",
    "title": "The Functions in gamlss.ggplots",
    "section": "model_wp()",
    "text": "model_wp()\nThe function model_wp() can be use to get a worm-plots for multiple fitted models residuals.\n\ngg &lt;- model_wp(g1, g2, g3, g4)\ngg\n\n\n\n\n\n\n\nFigure 46: Worm-plot of the residuals of models m1, m2, m3 and m4."
  },
  {
    "objectID": "gamlss-ggplots.html#model_wp_wrap",
    "href": "gamlss-ggplots.html#model_wp_wrap",
    "title": "The Functions in gamlss.ggplots",
    "section": "model_wp_wrap()",
    "text": "model_wp_wrap()\nFor model worm plots at differenbt values of the explnatory variables use;\n\ngg1 &lt;- model_wp_wrap(g1, g2, g3, g4, xvar=rent$A)\ngg1\n\n\n\n\n\n\n\nFigure 47: Multiple worm-plot of the residuals of models m1, m2, m3 and m4.\n\n\n\n\n\nThe resulting plot is shown on the right side of Figure Figure 47."
  },
  {
    "objectID": "gamlss-ggplots.html#bucket-plots",
    "href": "gamlss-ggplots.html#bucket-plots",
    "title": "The Functions in gamlss.ggplots",
    "section": "Bucket plots",
    "text": "Bucket plots\n\nmoment_bucket() and model_mom_bucket()\nThe function model_mom_bucket() can be use to get the moment bucket plot for one or more fitted model residuals. Note that moment_bucket() is synonymous to model_mom_bucket().\n\ngg &lt;- moment_bucket(g1, g2, g3, g4)\ngg\n\n\n\n\n\n\n\nFigure 47: Single moment bucket plots of the residuals of models m1, m2, m3 and m4.\n\n\n\n\n\n\n\nmoment_bucket_wrap()\nFor multiple plots use\n\ngg1 &lt;- moment_bucket_wrap(m1, m2, m3, m4, xvar=rent$A)\ngg1 \n\n\n\n\n\n\n\nFigure 48: Multiple moment bucket plots of the residuals of models g1, g2, g3 and g4.\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nNot working with gamlls2 objects\n\n\nThe resulting plot is shown on the right side of Figure Figure 48.\n\n\nmoment_gray_both() and moment_colour_both()\nNote that the background of the bucket plots is generated by the functions\n\nmoment_gray_both()\nmoment_colour_both()\n\n\n\n\n\n\n\nFigure 49: The background of the moment bucket plot.\n\n\n\n\n\n\n\n\n\n\n\nFigure 50: The background of the moment bucket plot.\n\n\n\n\n\n\n\nmodel_devianceIncr_diff()\nThe function plots the difference in deviance increment between two fitted gamlss models.\n\nmodel_devianceIncr_diff(m3,m4)\n\n\n\n\n\n\n\nFigure 51: Deviance difference between models m3 and m4.\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nNot working with gamlls2 objects\n\n\n\n\nmodel_pca()\nThis function it uses Principal Component Analysis (PCA) for the residual of multiple models and plots a bi-plot the first and second components.\n\ngg &lt;- model_pca(g1,g2,g3,g4)\ngg\n\n\n\n\n\n\n\nFigure 52: A PCA of different models residuals`.\n\n\n\n\n\n\n\nmodel_centiles()\nSee Section Section 4.28.4."
  },
  {
    "objectID": "gamlss-ggplots.html#model_devianceincr_diff",
    "href": "gamlss-ggplots.html#model_devianceincr_diff",
    "title": "The Functions in gamlss.ggplots",
    "section": "model_devianceIncr_diff()",
    "text": "model_devianceIncr_diff()\nThe function plots the difference in deviance increment between two fitted gamlss models.\n\nmodel_devianceIncr_diff(m3,m4)\n\n\n\n\n\n\n\nFigure 52: Deviance difference between models m3 and m4.\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nNot working with gamlls2 objects"
  },
  {
    "objectID": "gamlss-ggplots.html#model_pca",
    "href": "gamlss-ggplots.html#model_pca",
    "title": "The Functions in gamlss.ggplots",
    "section": "model_pca()",
    "text": "model_pca()\nThis function it uses Principal Component Analysis (PCA) for the residual of multiple models and plots a bi-plot the first and second components.\n\ngg &lt;- model_pca(g1,g2,g3,g4)\ngg\n\n\n\n\n\n\n\nFigure 53: A PCA of different models residuals`.\n\n\n\n\n\n\nmodel_centiles()\nSee Section Section 11.3."
  },
  {
    "objectID": "gamlss-ggplots.html#sec-modelcentiles0",
    "href": "gamlss-ggplots.html#sec-modelcentiles0",
    "title": "The Functions in gamlss.ggplots",
    "section": "",
    "text": "See Section Section 6.4."
  },
  {
    "objectID": "gamlss-ggplots.html#y_hist",
    "href": "gamlss-ggplots.html#y_hist",
    "title": "The Functions in gamlss.ggplots",
    "section": "y_hist()",
    "text": "y_hist()\nHere we generate a vector from a BCCG distribution and plot its histogram and its density function .\n\ny &lt;- rBCCG(1000, mu=3, sigma=.1, nu=-1)\ngg &lt;- y_hist(y)\ngg\n\n\n\n\n\n\n\nFigure 54: Plotting a histogram of single variable and its density function.\n\n\n\n\n\nHere we add the true pdf of the generated variable.\n\ngg + stat_function(fun = dBCT, args=list(mu=3, sigma=.1,  nu=-1, tau=5),\n      geom = \"area\", alpha=0.5, fill=\"lightblue\", color=\"black\", n=301)\n\n\n\n\n\n\n\nFigure 55: Plotting a histogram of single variable and the true distribution supperimpose"
  },
  {
    "objectID": "gamlss-ggplots.html#y_dots",
    "href": "gamlss-ggplots.html#y_dots",
    "title": "The Functions in gamlss.ggplots",
    "section": "y_dots()",
    "text": "y_dots()\nThe dots function is appropriate for long tail distributions be cause emphasise the the long left ot right tail of \\(y\\).\n\ny &lt;- rBCT(1000, mu=3, sigma=.1, nu=-1, tau=4)\ngg &lt;- y_dots(y)\ngg\n\n\n\n\n\n\n\nFigure 56: Plotting the \\(y\\) in dots to emphisise the long right tail."
  },
  {
    "objectID": "gamlss-ggplots.html#y_ecdf",
    "href": "gamlss-ggplots.html#y_ecdf",
    "title": "The Functions in gamlss.ggplots",
    "section": "y_ecdf()",
    "text": "y_ecdf()\nThe empirical cdf of a variables can be plotted using the function y_ecdf()\n\ny &lt;- rBCT(1000, mu=3, sigma=.1, nu=-1, tau=4)\ngg &lt;- y_ecdf(y)\ngg\n\n\n\n\n\n\n\nFigure 57: Plotting the empirical cdf of a variable.\n\n\n\n\n\nHere is how you can add the theoretical cdf.\n\ngg+ stat_function(fun = \n          pBCT, args=list(mu=3, sigma=.1,  nu=-1, tau=5), \n          color=\"red\", n=301)\n\n\n\n\n\n\n\nFigure 58: Plotting the empirical cdf of a variable plues its theoretical cdf."
  },
  {
    "objectID": "gamlss-ggplots.html#y_acf",
    "href": "gamlss-ggplots.html#y_acf",
    "title": "The Functions in gamlss.ggplots",
    "section": "y_acf()",
    "text": "y_acf()\nThe functions y_acf() take a single time series vector and plot its auto-correlation function.\n\ny_acf(diff(EuStockMarkets[,1])) \n\n\n\n\n\n\n\nFigure 59: Plotting ACF of the first difference of FTSE."
  },
  {
    "objectID": "gamlss-ggplots.html#y_pacf",
    "href": "gamlss-ggplots.html#y_pacf",
    "title": "The Functions in gamlss.ggplots",
    "section": "y_pacf()",
    "text": "y_pacf()\nThe functions y_pacf() take a single time series vector and plot its partial auto-correlation function.\n\ny_pacf(diff(EuStockMarkets[,1])) \n\n\n\n\n\n\n\nFigure 60: Plotting PACF of the first difference of FTSE."
  },
  {
    "objectID": "gamlss-ggplots.html#y_symmetry",
    "href": "gamlss-ggplots.html#y_symmetry",
    "title": "The Functions in gamlss.ggplots",
    "section": "y_symmetry()",
    "text": "y_symmetry()\nHere we plot the symmetry plot of the variable \\(y\\).\n\ngg&lt;-y_symmetry(rent$R)\ngg\n\n\n\n\n\n\n\nFigure 61: A symmetry plot of the rent data."
  },
  {
    "objectID": "gamlss-ggplots.html#pe_terms",
    "href": "gamlss-ggplots.html#pe_terms",
    "title": "The Functions in gamlss.ggplots",
    "section": "pe_terms()",
    "text": "pe_terms()\nThe function plots individual terms from a fitted gamlss model. It is equivalent to term.plot(). Note that the function will produce up to 9 term plots before ask for the next page.\n\ngamlss.ggplots:::pe_terms(m4, partial=T)\n\n\n\n\n\n\n\nFigure 62: Plotting terms from a fitted GAMLSS model.\n\n\n\n\n\nThe function pe_terms() uses the function lpred() to obtain the partial fitted terms. The function lpred() is using the parameter model.frame which has already evaluate the factors as a set of dummy variables. The partial terms are then evaluated by fixing the other terms on their means and after subtraction for the mean of the response.\n\n\n\n\n\n\nWarning\n\n\n\nNot working with gamlls2 objects"
  },
  {
    "objectID": "gamlss-ggplots.html#pe_param",
    "href": "gamlss-ggplots.html#pe_param",
    "title": "The Functions in gamlss.ggplots",
    "section": "pe_param()",
    "text": "pe_param()\nThe function pe_param() plots the partial effect of one or two specified term(s) given all other terms in the model remain fixed at predetermined values. Depending on length of the argument term the function pe_param() uses the function pe_1_param() or pe_2_param() to plot the partial effects. Here we plot it against a continuous variable A:\n\npe_param(g4, \"A\")\n\n\n\n\n\n\n\nFigure 63: Plotting partial terms from A.\n\n\n\n\n\nHere we plot it against a categorical variable loc:\n\npe_param(g4, \"loc\")\n\n\n\n\n\n\n\nFigure 64: Plotting partial terms from loc.\n\n\n\n\n\nHere we use two continuous variables A and Fl:\n\ngamlss.ggplots:::pe_param(g4, c(\"A\", \"Fl\"))\n\n\n\n\n\n\n\nFigure 65: Plotting partial terms from A and Fl.\n\n\n\n\n\nHere we use two continuous variables A and Fl but with the argument filled = TRUE:\n\ngamlss.ggplots:::pe_param(g4, c(\"A\", \"Fl\"), filled = TRUE)\n\n\n\n\n\n\n\nFigure 66: Plotting partial terms from A and Fl.\n\n\n\n\n\nHere we use one continuous A and one categorical variables loc:\n\ngamlss.ggplots:::pe_param(g4, c(\"A\", \"loc\"))\n\n\n\n\n\n\n\nFigure 67: Plotting partial terms from loc and A.\n\n\n\n\n\nHere we use two categorical variables H and loc:\n\ngamlss.ggplots:::pe_param(g4, c(\"H\", \"loc\"))\n\n\n\n\n\n\n\nFigure 68: Plotting partial terms from loc and H."
  },
  {
    "objectID": "gamlss-ggplots.html#pe_param_grid",
    "href": "gamlss-ggplots.html#pe_param_grid",
    "title": "The Functions in gamlss.ggplots",
    "section": "pe_param_grid()",
    "text": "pe_param_grid()\nThe function pe_param_grid() show multiple plots of partial effects (main effect or first order interactions) given all other terms in the model remain fixed at predetermined values.\n\ngamlss.ggplots:::pe_param_grid(g5, list(c(\"Fl\", \"A\"), c(\"H\", \"loc\")))\n\n\n\n\n\n\n\nFigure 69: Plotting partial terms from loc and H.\n\n\n\n\n\n\ngamlss.ggplots:::pe_param_grid(g5, list(c(\"Fl\", \"A\"), c(\"H\", \"loc\")))\n\n\n\n\n\n\n\nFigure 70: Plotting the partial effect of terms in model m6. Note that the plot on the left panel is a genuine first order interaction term fitted with a 2 dimensional smoother using the interface with the package mgcv while the plot on the right panel is created using the two factors H and loc is just a two dimensional plot of the two main effects for H and loc respectively, since no interaction was fitted for those two terms"
  },
  {
    "objectID": "gamlss-ggplots.html#predict_pdf",
    "href": "gamlss-ggplots.html#predict_pdf",
    "title": "The Functions in gamlss.ggplots",
    "section": "predict_pdf()",
    "text": "predict_pdf()\nNow with newdata we use `predict_pdf’:\n\npredict_pdf(a1, newdata=abdom[c(11,15,20,25),], from=38, to=130)\n\n\n\n\n\n\n\nFigure 71: Plotting linear leverage from a fitted GAMLSS model."
  },
  {
    "objectID": "gamlss-ggplots.html#classical-functions",
    "href": "gamlss-ggplots.html#classical-functions",
    "title": "The Functions in gamlss.ggplots",
    "section": "Classical functions",
    "text": "Classical functions\nThose the three classical GAMLSS centile functions;\n\ncentiles(m1)#  works\n#######################################################################\ncentiles.fan(m1)#  works\n#######################################################################\ncentiles.com(m1,m2,m3 )# works\n\nNote that you can actually can specify the x variable and produce identical results:\n\ncentiles(m1, xvar=db$age)# works\ncentiles(m1, xvar=age)# works"
  },
  {
    "objectID": "gamlss-ggplots.html#sec-fittedcentiles",
    "href": "gamlss-ggplots.html#sec-fittedcentiles",
    "title": "The Functions in gamlss.ggplots",
    "section": "fitted_centiles()",
    "text": "fitted_centiles()\nThe fitted_centiles() function, produces identical results with the function centiles() of gamlss.\n\nfitted_centiles(m2)\n\n\n\n\n\n\n\nFigure 72: Plotting the centiles for model m2.\n\n\n\n\n\nThe function fitted_centiles()'  can be used withfacet_wrap()`:\n\nfitted_centiles(m2)+\n  facet_wrap(cut_number(db$age, 3), scales = \"free_x\")\n\n\n\n\n\n\n\nFigure 73: Plotting the centiles for models m1, m2 and m3."
  },
  {
    "objectID": "gamlss-ggplots.html#sec-fittedcentileslegend",
    "href": "gamlss-ggplots.html#sec-fittedcentileslegend",
    "title": "The Functions in gamlss.ggplots",
    "section": "fitted_centiles_legend()",
    "text": "fitted_centiles_legend()\nTo have legend in the plot use the following which it takes more time.\n\nfitted_centiles_legend(m1)\n\n\n\n\n\n\n\nFigure 74: Plotting the centiles for model m2."
  },
  {
    "objectID": "gamlss-ggplots.html#sec-modelcentiles",
    "href": "gamlss-ggplots.html#sec-modelcentiles",
    "title": "The Functions in gamlss.ggplots",
    "section": "model_centiles()",
    "text": "model_centiles()\nIf you wnat to check the centiles curves for all models used;\n\nmodel_centiles(m1,m2,m3,  xvar=age)\n\n\n\n\n\n\n\nFigure 75: Plotting the centiles for models m1, m2 and m3.\n\n\n\n\n\nor\n\nmodel_centiles(m1,m2,m3,  xvar=age, in.one=TRUE)\n\n\n\n\n\n\n\nFigure 76: Plotting the centiles for models m1, m2 and m3.\n\n\n\n\n\nThe function model_centiles(..., in.one=TRUE)'  can be used withfacet_wrap()`:\n\nmodel_centiles(m1,m2,m3, xvar=age, in.one=TRUE)+\n  facet_wrap(cut_number(db$age, 4), scales = \"free_x\")# working\n\n\n\n\n\n\n\nFigure 77: Plotting the centiles for models m1, m2 and m3."
  },
  {
    "objectID": "gamlss-ggplots.html#sec-Boostrapping",
    "href": "gamlss-ggplots.html#sec-Boostrapping",
    "title": "The Functions in gamlss.ggplots",
    "section": "Boostrapping",
    "text": "Boostrapping\nto be continue"
  },
  {
    "objectID": "gamlss-ggplots.html#package-versions",
    "href": "gamlss-ggplots.html#package-versions",
    "title": "The Functions in gamlss.ggplots",
    "section": "",
    "text": "The latest versions of the key packages are:\n\nrm(list=ls())\n#getRversion()\nlibrary(gamlss)\nlibrary(gamlss2)\nlibrary(ggplot2)\nlibrary(gamlss.ggplots)\nlibrary(\"dplyr\") \npackageVersion(\"gamlss\")\n\n[1] '5.4.23'\n\npackageVersion(\"gamlss2\")\n\n[1] '0.1.0'\n\npackageVersion(\"gamlss.ggplots\")\n\n[1] '2.1.17'"
  },
  {
    "objectID": "gamlss-ggplots.html#packages",
    "href": "gamlss-ggplots.html#packages",
    "title": "The Functions in gamlss.ggplots",
    "section": "",
    "text": "This booklet provides an overview of the R package gamlss.ggplots and its functions, focusing on their use and the output they generate.\nThe gamlss.ggplots package offers a set of ggplot2-based visual tools for diagnostic and exploratory plots specifically tailored for models fitted using gamlss() and gamlss2(). Most of the functions in this package are compatible with both model types, making them broadly useful across the Generalized Additive Models for Location, Scale and Shape framework.\nSupported Model Objects\n\ngamlss — the original modeling interface for GAMLSS\ngamlss2 — a modern interface with cleaner formula syntax and streamlined model handling"
  },
  {
    "objectID": "gamlss-ggplots.html#overview-of-the-gamlss.ggplots-package",
    "href": "gamlss-ggplots.html#overview-of-the-gamlss.ggplots-package",
    "title": "The Functions in gamlss.ggplots",
    "section": "",
    "text": "Originally, the package gamlss.ggplots included all the functionality currently split between gamlss.ggplots and gamlss.prepdata. Due to maintenance challenges caused by the growing size and complexity of the package, it was divided into two separate components:\n- `gamlss.ggplots` — visualization and diagnostics\n\n- `gamlss.prepdata` — data preparation utilities\nAt this stage, gamlss.ggplots is experimental. Some functions are not yet exported (i.e., “hidden”) to allow for further testing and validation. These hidden functions can still be accessed using the triple-colon syntax:\n\ngamlss.ggplots:::function_name()\n\n\n\nFunctions in gamlss.ggplots can be broadly classified into three categories:\n\n\nMost if the before-fitting a model functions are moved to gamlss.prepdata package but few visualizing distribution families are remaining. Distribution relatated function names begin with family_Name() - Example: family_pdf(NO) which plots the density distribution from the Normal GAMLSS family.\n\n\n\nThese functions are used after a model has been fitted, providing diagnostics, interpretation aids, and predictions.\n\nResidual-based diagnostics - resid_NAME() – Diagnostics on residuals E.g., resid_wp() produces a worm plot for assessing the distribution.\nFitted value visualizations - fitted_NAME() – Visualizes fitted parameters (\\(\\mu\\), \\(\\sigma\\), etc.) from a fitted model.\nResponse variable plots resp_NAME() – Plots the response variable against various model-based quantities such as parameters or quantiles.\nModel interpretation\n\n\npe_NAME() – Plots partial effects to aid model interpretation.\ninfluence_NAME() – Shows influence measures of model terms.\npredict_NAME() – Provides predictions, often using a newdata argument.\n\n\nModel comparison diagnostics\n\nmodel_NAME() – Functions for comparing multiple models, typically using diagnostic statistics or GAIC.\n\n\n\n\n\nThese functions serve as helpers for data exploration and bootstrap analyses.\n\nSingle-vector visualizations • y_NAME() – For example, y_hist(y) plots a histogram, y_acf(y) plots the autocorrelation function.\nBootstrap summaries • boot_NAME() – Plots parameter estimates or fitted values across bootstrap samples.\nFunction imitations\n\nSome utility functions imitate existing gamlss functions. E.g., histSmo_plot() mimics histSmo() but uses ggplot2.\nBecause the package is at an experimental stage, some of the functions are hidden to allowed time for checking. The hidden functions can be accessed using gamlss.ggplots:::functionname().\nNote future functions that are not included in the package at the moment are:\n\nale_param() for accumulated local effects of a specific term on the parameter (Mikis)\npd_param() for partial dependants plots of a specific term on the parameter (Mikis)\npe_exceedance() (Julian)"
  },
  {
    "objectID": "gamlss-ggplots.html#packages-versions",
    "href": "gamlss-ggplots.html#packages-versions",
    "title": "The Functions in gamlss.ggplots",
    "section": "",
    "text": "The latest versions of the key packages are:\n\nrm(list=ls())\n#getRversion()\nlibrary(gamlss)\nlibrary(gamlss2)\nlibrary(ggplot2)\nlibrary(gamlss.ggplots)\nlibrary(\"dplyr\") \npackageVersion(\"gamlss\")\n\n[1] '5.4.23'\n\npackageVersion(\"gamlss2\")\n\n[1] '0.1.0'\n\npackageVersion(\"gamlss.ggplots\")\n\n[1] '2.1.17'"
  },
  {
    "objectID": "gamlss-ggplots.html#sec-Family",
    "href": "gamlss-ggplots.html#sec-Family",
    "title": "The Functions in gamlss.ggplots",
    "section": "",
    "text": "Here there are no fitted models, and the only requirement are values for the parameters. Note that those function may at a later stage move to gamlss.prepdata.\n\n\nThe function family_pdf() plots individual pdf’s from gamlss.family distribution. It needs the family argument.\n\n\n\nfamily_pdf(NO, from=-5,to=5, mu=0, sigma=c(.5,1,2))\n\n\n\n\n\n\n\nFigure 1: Continuous response example: plotting the pdf of a normal random variable.\n\n\n\n\n\n\n\n\n\nfamily_pdf(NBI, to=15, mu=1, sigma=c(.5,1,2), alpha=.9, size.seqment = 3)\n\n\n\n\n\n\n\nFigure 2: Count response example: plotting the pdf of a beta binomial.\n\n\n\n\n\n\n\n\n\nfamily_pdf(BB, to=15, mu=.5, sigma=c(.5,1,2),  alpha=.9, , size.seqment = 3)\n\n\n\n\n\n\n\nFigure 3: Beta binomial response example: plotting the pdf of a beta binomial.\n\n\n\n\n\n\n\n\n\nThe function plots individual cdf’s from gamlss.family distribution. It needs the family argument.\n\n\n\nfamily_cdf(NO, from=-5,to=5, mu=0, sigma=c(.5,1,2))\n\n\n\n\n\n\n\nFigure 4: Continuous response example: plotting the cdf of a normal random variable.\n\n\n\n\n\n\n\n\n\nfamily_cdf(NBI, to=15, mu=1, sigma=c(.5,1,2), alpha=.9, size.seqment = 3)\n\n\n\n\n\n\n\nFigure 5: Count response example: plotting the cdf of a negative binomial.\n\n\n\n\n\n\n\n\n\nfamily_cdf(BB, to=15, mu=.5, sigma=c(.5,1,2),  alpha=.9, , size.seqment = 3)\n\n\n\n\n\n\n\nFigure 6: Count response example: plotting the cdf of a beta binomial.\n\n\n\n\n\n\n\n\n\nThe function family_cor() provides a crude way of checking the inter correlation of parameters within any gamlss.family distribution. It\n\ngenerates 10000 values from the specified distribution,\nfits the distribution to the generared data and\nplots the correlation coefficients of the parameters.\n\nThose correlation coefficients are taken from the fitted variance covariance matrix.\n\n\n\n\n\n\nWarning\n\n\n\nThe method only provides an idea of how the correlations between parameters are at specified points of the distribution parameters. At different parameter points the distribution could behave completely different.\n\n\n\n#source(\"~/Dropbox/github/gamlss-ggplots/R/family_cor.R\")\ngamlss.ggplots:::family_cor(\"BCTo\", mu=1, sigma=0.11, nu=1, tau=5, no.sim=10000)\n\n\n\n\n\n\n\nFigure 7: Family correlation of a BCTo distribution at specified values of the parameters."
  },
  {
    "objectID": "gamlss-ggplots.html#sec-Fitting",
    "href": "gamlss-ggplots.html#sec-Fitting",
    "title": "The Functions in gamlss.ggplots",
    "section": "Fits",
    "text": "Fits\nFirst we fit different GAMLSS models. The models will to be used for demonstration later.\n\nThe null model using Normal distribution.\n\ndata(rent)\n# Null model \nm0&lt;-gamlss(R~1,family=GA,data=rent)\n\nGAMLSS-RS iteration 1: Global Deviance = 28611.58 \nGAMLSS-RS iteration 2: Global Deviance = 28611.58 \n\ng0&lt;-gamlss2(R~1,family=GA,data=rent)\n\nGAMLSS-RS iteration  1: Global Deviance = 28611.5819 eps = 0.056387     \nGAMLSS-RS iteration  2: Global Deviance = 28611.5819 eps = 0.000000     \n\n\n\n\nAdditive smooth model using Normal distribution.\nFit additive smooth terms for Fl and A in the \\(\\mu\\) using the Normal distribution.\n\n# Smooth terms for Fl  and A  in mu, normal\nm1&lt;-gamlss(R~pb(Fl)+pb(A),family=NO,data=rent)\n\nGAMLSS-RS iteration 1: Global Deviance = 28264.19 \nGAMLSS-RS iteration 2: Global Deviance = 28264.19 \n\ng1&lt;-gamlss2(R~s(Fl)+s(A),family=NO,data=rent)\n\nGAMLSS-RS iteration  1: Global Deviance = 28269.3496 eps = 0.024145     \nGAMLSS-RS iteration  2: Global Deviance = 28265.5187 eps = 0.000135     \nGAMLSS-RS iteration  3: Global Deviance = 28265.5116 eps = 0.000000     \n\n\nFit additive smooth terms for Fl and A and main effects for H and loc in the \\(\\mu\\) using the Normal distribution.\n\nm2&lt;-gamlss(R~pb(Fl)+pb(A)+H+loc, family=NO, data=rent)\n\nGAMLSS-RS iteration 1: Global Deviance = 28062.62 \nGAMLSS-RS iteration 2: Global Deviance = 28062.62 \n\ng2&lt;-gamlss2(R~s(Fl)+s(A)+H+loc, family=NO, data=rent)\n\nGAMLSS-RS iteration  1: Global Deviance = 28067.7199 eps = 0.031106     \nGAMLSS-RS iteration  2: Global Deviance = 28063.5704 eps = 0.000147     \nGAMLSS-RS iteration  3: Global Deviance = 28063.5654 eps = 0.000000     \n\n\n\n\nAdditive smooth model using gamma distribution.\nFit additive smooth terms for Fl and A and main effects for H and loc in the \\(\\mu\\) using Gamma distribution.\n\n# Smooth terms for Fl  and A main effects for H and loc in mu, gamma\nm3&lt;-gamlss(R~pb(Fl)+pb(A)+H+loc,family=GA,data=rent)\n\nGAMLSS-RS iteration 1: Global Deviance = 27683.22 \nGAMLSS-RS iteration 2: Global Deviance = 27683.22 \nGAMLSS-RS iteration 3: Global Deviance = 27683.22 \n\ng3&lt;-gamlss2(R~s(Fl)+s(A)+H+loc,family=GA,data=rent)\n\nGAMLSS-RS iteration  1: Global Deviance = 27693.1246 eps = 0.086677     \nGAMLSS-RS iteration  2: Global Deviance = 27686.3102 eps = 0.000246     \nGAMLSS-RS iteration  3: Global Deviance = 27686.309 eps = 0.000000     \n\n\nFit additive smooth terms for Fl and A and main effects for H and loc in both \\(\\mu\\) and \\(\\sigma\\) parameters using Gamma distribution.\n\nm4&lt;-gamlss(R~pb(Fl)+pb(A)+H+loc, sigma.fo=~pb(Fl)+pb(A)+H+loc, \n           family=GA,data=rent)\n\nGAMLSS-RS iteration 1: Global Deviance = 27572.14 \nGAMLSS-RS iteration 2: Global Deviance = 27570.29 \nGAMLSS-RS iteration 3: Global Deviance = 27570.28 \nGAMLSS-RS iteration 4: Global Deviance = 27570.28 \n\ng4&lt;-gamlss2(R~s(Fl)+s(A)+H+loc, sigma.fo=~s(Fl)+s(A)+H+loc, \n           family=GA,data=rent)\n\nGAMLSS-RS iteration  1: Global Deviance = 27621.6847 eps = 0.089033     \nGAMLSS-RS iteration  2: Global Deviance = 27613.1786 eps = 0.000307     \nGAMLSS-RS iteration  3: Global Deviance = 27613.1728 eps = 0.000000     \n\n\n\n\nAdditive smooth model using s() and the gamma distribution.\nWe bring the package gamlss.add for extra smoothers\n\nlibrary(gamlss.add)\n\nFit interaction smooth terms for Fl and A in \\(\\mu\\), additive main smooth effect for for Fl and A in \\(\\sigma\\) and main effects for H and loc in both \\(\\mu\\) and \\(\\sigma\\) using a gamma family.\n\nm5&lt;-gamlss(R~ga(~s(Fl, A))+H+loc, sigma.fo=~pb(Fl)+pb(A)+H+loc, \n           family=GA,data=rent)\n\nGAMLSS-RS iteration 1: Global Deviance = 27542.45 \nGAMLSS-RS iteration 2: Global Deviance = 27537.5 \nGAMLSS-RS iteration 3: Global Deviance = 27537.41 \nGAMLSS-RS iteration 4: Global Deviance = 27537.41 \n\ng5&lt;-gamlss2(R~s(Fl, A)+H+loc, sigma.fo=~s(Fl)+s(A)+H+loc, \n           family=GA,data=rent)\n\nGAMLSS-RS iteration  1: Global Deviance = 27654.2449 eps = 0.087960     \nGAMLSS-RS iteration  2: Global Deviance = 27584.1109 eps = 0.002536     \nGAMLSS-RS iteration  3: Global Deviance = 27584.0985 eps = 0.000000     \n\n\nFit interaction smooth terms for Fl and A, interaction for H and loc in \\(\\mu\\) and and main effects only in \\(\\sigma\\) using a gamma family.\n\nm6&lt;-gamlss(R~ga(~s(Fl, A))+H*loc, sigma.fo=~pb(Fl)+pb(A)+H+loc, \n           family=GA,data=rent)\n\nGAMLSS-RS iteration 1: Global Deviance = 27541.97 \nGAMLSS-RS iteration 2: Global Deviance = 27536.41 \nGAMLSS-RS iteration 3: Global Deviance = 27536.29 \nGAMLSS-RS iteration 4: Global Deviance = 27536.29 \nGAMLSS-RS iteration 5: Global Deviance = 27536.29 \n\ng6&lt;-gamlss2(R~s(Fl, A)+H*loc, sigma.fo=~s(Fl)+s(A)+H+loc, \n           family=GA,data=rent)\n\nGAMLSS-RS iteration  1: Global Deviance = 27654.5301 eps = 0.087950     \nGAMLSS-RS iteration  2: Global Deviance = 27583.6533 eps = 0.002562     \nGAMLSS-RS iteration  3: Global Deviance = 27583.6377 eps = 0.000000     \n\n\nFit a penalised varying coefficient model (a simpler form of smoothing in two dimensions that s(Fl, A)) in \\(\\mu\\). Additive main smooth effect for for Fl and A in \\(\\sigma\\) and main effects for H and loc in both \\(\\mu\\) and \\(\\sigma\\) using a gamma family.\n\nm7&lt;-gamlss(R~pvc(Fl, by=A)+H+loc, sigma.fo=~pb(Fl)+pb(A)+H+loc, \n           family=GA,data=rent)\n\nGAMLSS-RS iteration 1: Global Deviance = 28221.47 \nGAMLSS-RS iteration 2: Global Deviance = 28203.83 \nGAMLSS-RS iteration 3: Global Deviance = 28201.49 \nGAMLSS-RS iteration 4: Global Deviance = 28201.05 \nGAMLSS-RS iteration 5: Global Deviance = 28200.94 \nGAMLSS-RS iteration 6: Global Deviance = 28200.92 \nGAMLSS-RS iteration 7: Global Deviance = 28200.91 \nGAMLSS-RS iteration 8: Global Deviance = 28200.9 \nGAMLSS-RS iteration 9: Global Deviance = 28200.9 \n\ng7&lt;-gamlss2(R~s(Fl,by=A)+H+loc, sigma.fo=~s(Fl)+s(A)+H+loc, \n           family=GA,data=rent)\n\nGAMLSS-RS iteration  1: Global Deviance = 27732.5053 eps = 0.085379     \nGAMLSS-RS iteration  2: Global Deviance = 27731.6923 eps = 0.000029     \nGAMLSS-RS iteration  3: Global Deviance = 27731.6805 eps = 0.000000     \n\n\n\n\nNeural network using Gamma distribution\nFit Neural networks for Fl, A, H, and loc for both \\(\\mu\\) and \\(\\sigma\\) # gamma family\n\nm8&lt;-gamlss(R~nn(~Fl+A+H+loc,   size = 10,\n                     decay = 0.01,  maxit = 100), \n               sigma.fo=~nn(~Fl+A+H+loc, decay = 0.01), \n           family=GA,data=rent)\n\nWarning in additive.fit(x = X, y = wv, w = wt * w, s = s, who = who,\nsmooth.frame, : additive.fit convergence not obtained in 30 iterations\n\n\nGAMLSS-RS iteration 1: Global Deviance = 27561.3 \nGAMLSS-RS iteration 2: Global Deviance = 27542.02 \nGAMLSS-RS iteration 3: Global Deviance = 27541.35 \nGAMLSS-RS iteration 4: Global Deviance = 27541.35 \n\ng8&lt;-gamlss2(R~n(~Fl+A+H+loc,   size = 10, decay = 0.01), \n               sigma.fo=~n(~Fl+A+H+loc, size = 3,decay = 0.01), \n           family=GA,data=rent)\n\nGAMLSS-RS iteration  1: Global Deviance = 27456.7656 eps = 0.094473     \nGAMLSS-RS iteration  2: Global Deviance = 27421.9704 eps = 0.001267     \nGAMLSS-RS iteration  3: Global Deviance = 27421.4861 eps = 0.000017     \nGAMLSS-RS iteration  4: Global Deviance = 27421.4861 eps = 0.000000     \n\n\n\n\nRegression trees fits using Gamma distribution\nFit regression trees for Fl, A, H, and loc for both \\(\\mu\\) and \\(\\sigma\\)\n\nm9&lt;-gamlss(R~tr(~Fl+A+H+loc), \n               sigma.fo=~tr(~Fl+A+H+loc), \n           family=GA,data=rent)\n\nGAMLSS-RS iteration 1: Global Deviance = 27779.77 \nGAMLSS-RS iteration 2: Global Deviance = 27754.62 \nGAMLSS-RS iteration 3: Global Deviance = 27754.53 \nGAMLSS-RS iteration 4: Global Deviance = 27754.53 \n\ng9&lt;-gamlss2(R~tr(~Fl+A+H+loc), \n               sigma.fo=~tr(~Fl+A+H+loc), \n           family=GA,data=rent)\n\nGAMLSS-RS iteration  1: Global Deviance = 27759.7778 eps = 0.084479     \nGAMLSS-RS iteration  2: Global Deviance = 27755.0828 eps = 0.000169     \nGAMLSS-RS iteration  3: Global Deviance = 27755.0817 eps = 0.000000     \n\n\nChecking the models using different GAIC. The best model using AIC is m5 while using BIC is m4.\n\nT1 &lt;- GAIC.table(m1,m2,m3,m4,m5,m6,m7, m8, m9)\n\nminimum GAIC(k= 2 ) model: m5 \nminimum GAIC(k= 3.84 ) model: m4 \nminimum GAIC(k= 7.59 ) model: m4 \n\nT1\n\n          df      k=2   k=3.84   k=7.59\nm1  8.372701 28280.94 28296.34 28327.74\nm2 11.748554 28086.12 28107.74 28151.79\nm3 11.215475 27705.65 27726.29 27768.34\nm4 22.250355 27614.78 27655.72 27739.16\nm5 33.146682 27603.70 27664.69 27788.99\nm6 35.262213 27606.81 27671.70 27803.93\nm7 21.380016 28243.66 28283.00 28363.18\nm8 95.000000 27731.35 27906.15 28262.40\nm9 18.000000 27790.53 27823.65 27891.15\n\nmodel_GAIC(m1,m2,m3,m4,m5,m6,m7, m8, m9)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nGAIC.table is not working for gamlss2 objects\n\n\n\n# T2 &lt;- GAIC.table(g1,g2,g3,g4,g5,g6,g7, g8, g9)\n# T2\nmodel_GAIC(g1,g2,g3,g4,g5,g6,g7, g8, g9)"
  },
  {
    "objectID": "gamlss-ggplots.html#residuals-of-fitted-models",
    "href": "gamlss-ggplots.html#residuals-of-fitted-models",
    "title": "The Functions in gamlss.ggplots",
    "section": "",
    "text": "This section describes plots to do with the residuals of a single GAMLSS fitted model. Plotting, the residuals is very important, because if the model is correct, the residuals of the model should be like a white noise. In GAMLSS we use the (randomised) normalised residuals. Randomisation happens only if the distribution is discreet, or if it is censored. Normalisation means that if the model is correct or adequate, then the residuals should look like a normal distribution."
  },
  {
    "objectID": "gamlss-ggplots.html#residuals",
    "href": "gamlss-ggplots.html#residuals",
    "title": "The Functions in gamlss.ggplots",
    "section": "Residuals",
    "text": "Residuals\nThis section describes plots to do with the residuals of a single GAMLSS fitted model. Plotting, the residuals is very important, because if the model is correct, the residuals of the model should be like a white noise. In GAMLSS we use the (randomised) normalised residuals. Randomisation happens only if the distribution is discreet, or if it is censored. Normalisation means that if the model is correct or adequate, then the residuals should look like a normal distribution.\n\nresid_index()\nBy plotting, the residual against the data index we would expect that there is no pattern in the plot because of the assumption that the observations (given the explanatory variables) are independent. Figure 8 show the standard residual plot against the index of the data. The argument value has to do with the outliers hoghlighted in the plot and can be change to a higher cut-off point. Other type of residuals, suitable standardised, not necessarily GAMLSS model residuals, can be plotted by using the argument resid.\n\ngg &lt;-resid_index(g4)\ngg\n\n\n\n\n\n\n\nFigure 8: Residual plot.\n\n\n\n\n\n\nDifferent models\nResidual plots from different models can be plotted in different graphs see Figure 9.\n\ng11 &lt;-resid_index(g1)\ng12 &lt;-resid_index(g2)\ng13 &lt;-resid_index(g3)\ng14 &lt;-resid_index(g4)\nlibrary(gridExtra)\ngrid.arrange(g11,g12,g13,g14 )\n\n\n\n\n\n\n\nFigure 9: Residual plot.\n\n\n\n\n\n\n\nDifferent range of an x-variable\nHere the residual plot is split against one continuous explanatory variables Fl using the function facet_wrap(). The function splits in three different cut points because the cut_number(rent\\$Fl, 3) is used. Note that in order for facet_wrap() to work we had to suppress the horizontal likes using the argument no.lines=TRUE.\n\nresid_index(g1, no.lines=TRUE)+\n   facet_wrap(~cut_number(rent$Fl, 3))\n\n\n\n\n\n\n\nFigure 10: Resisduals plot against the x-variable split in tree sections.\n\n\n\n\n\nHere we split according to one continuous an one categorical x-variables.\n\nresid_index(g1, no.lines=TRUE) +\n  facet_grid(cut_number(rent$Fl, 3)~rent$loc)\n\n\n\n\n\n\n\nFigure 11: Residuals plot against two x-variable split in tree sections.\n\n\n\n\n\n\n\n\nresid_mu()\nA plot of the residual against the fitted values of the model, usually reveals whether there is a heterogeneity in the data. The resid_mu() plots the residual from model m1 against the fitted values for the model for \\(\\mu\\):\n\nresid_mu(g1)\n\n\n\n\n\n\n\nFigure 12: Residuals plot against the fitted values for models m1\n\n\n\n\n\nWe can observe a fan type of behaviour in the plot, that is the residuals are become bigger for larger fitted values of the response. This behaviour is typical for models with heterogeneity in the data. Remember m1 does not have a model for \\(\\sigma\\). In model m4 we fit an mode for \\(\\sigma\\) so the residuals are better.\n\nresid_mu(g4)\n\n\n\n\n\n\n\nFigure 13: Residuals plot against the fitted values for models m4\n\n\n\n\n\n\n\nresid_quantile()\nResidual plots from model m1 against the fitted quantile (including the median) values can be obtained using:\n\nresid_quantile(g4)\n\n\n\n\nResiduals plot against the fitted 50 percent fitted quantiles of model m4\n\n\n\n\n\n\nresid_xvar()\n\nAgainst a continuous explanatory variable\nResidual plots against a continuous explanatory variables can be plotted using: resid_xvar():\n\nresid_xvar(g1,Fl)\n\n\n\n\nResisduals plot against the x-variables\n\n\n\nresid_xvar(g1,A)\n\n\n\n\nResisduals plot against the x-variables\n\n\n\n\n\n\nAgainst a categorical explanatory variable\nThe function is working differently for categorical x-variables.\n\nresid_xvar(g1,H)\n\n\n\n\nResisduals plot against categorical x-variables for model m1\n\n\n\nresid_xvar(g1,loc)\n\n\n\n\nResisduals plot against categorical x-variables for model m1\n\n\n\n\nOr for a different models:\n\nresid_xvar(g4,loc)\n\n\n\n\nResisduals plot against categorical x-variables for model m4\n\n\n\n\n\n\n\nresid_qqplot()\nThe function resid_qqplot() can be use to get a QQ-plot of the residuals.\n\ngg &lt;-resid_qqplot(g1)\ngg\n\n\n\n\n\n\n\nFigure 14: QQ-plot for model m1\n\n\n\n\n\nThe plot appears in Figure 14.\nFor comparing two different models you can use\nTo add another model QQ-plot try\n\n\n\n\n\n\nWarning\n\n\n\nadd_resid_qqplot is not working for extra gamlss2 objects\n\n\n\ngg1 &lt;-add_resid_qqplot(gg, m4)\n#gg1\n\nor alternatively you can use the function model_qqplot() which can display more than two models.\n\nFor different values of x-variables\n\ngg+facet_grid(cut_number(rent$Fl, 3)~rent$loc)+ \n  ggtitle(\"qqplot of model m1 against Fl and A\") \n\n\n\n\n\n\n\nFigure 15: QQ-plot of the residuals of model m1 against Fl and `A’\n\n\n\n\n\n\ngg1+facet_grid(cut_number(rent$Fl, 3)~rent$loc)+ \n  ggtitle(\"qqplot of models m1 and m4 against Fl and A\")\n\n\n\n\n\n\n\nFigure 16: QQ-plot of the residuals of models m1 and m4 against Fl and `A’\n\n\n\n\n\n\n\n\nresid_wp(), Worm plots\nThe worm plot for a single model can be plotted using:\n\ngg&lt;-resid_wp(g4)\ngg\n\n\n\n\n\n\n\nFigure 17: Worm-plot of the residuals for model m4\n\n\n\n\n\nSee Figure 17.\n\nresid_wp_wrap()\nThe worm plot for a single model at different values of an explanatory variable can be plotted using:\n\ngg1 &lt;-resid_wp_wrap(g4, xvar=rent$A)\ngg1\n\n\n\n\n\n\n\nFigure 18: Worm-plot of the residuals for model m4 at different values of A\n\n\n\n\n\nSee Figure 18\n\n\n\nresid_density()\n\nThe single density\nThe function resid_density() plots the density of the residuals for one model while the function model_densitity() for more than one model.\nHere we plot the density of the residuals for model m4. See Figure Figure 19 for the plot.\n\ngg&lt;-resid_density(g4)\ngg\n\n\n\n\n\n\n\nFigure 19: A density plot of the residuals\n\n\n\n\n\n\n\nThe multiple densities\nHere we plot the density of the residuals for model m4 against two explanatory variables, Fl and A.\n\ngg+facet_grid(cut_number(rent$Fl, 3)~rent$loc)\n\n\n\n\n\n\n\nFigure 20: A density plot of the residuals against Fl and A\n\n\n\n\n\n\n\n\nresid_ecdf()\nFigure Figure 21 show the empirical cumulative distribution function of the residuals of model m4\n\ngg &lt;- resid_ecdf(g4)\ngg\n\n\n\n\n\n\n\nFigure 21: The ECDF of the residuals.\n\n\n\n\n\nBelow cdf of a normally distributed variable with \\(\\mu=0\\) and \\(\\sigma=1\\) is added to the previous plot.\n\ngg+stat_function(fun = pNO, args=list(mu=0, sigma=1), col=\"red\")\n\n\n\n\n\n\n\nFigure 22: The ECDF of the residuals with added normal distribution.\n\n\n\n\n\n\n\nresid_dtop()\nThe function resid_dtop() plot a de-trended empirical cdf plot. See Figure 23 for the plot.\n\ngg&lt;-resid_dtop(g4)\ngg\n\n\n\n\n\n\n\nFigure 23: A detrended Own’s plot.\n\n\n\n\n\n\n\nresid_plots()\nThe function resid_plots() tries to imitate the function plot.gamlss() of the gamlss package.\n\nresid_plots(g4)\n\n\n\n\n\n\n\nFigure 24: The plot of residuals.\n\n\n\n\n\n\nDifferent themes\nThere are also different themes in the plot. Next in Figure Figure 25 we are trying theme=\"new\":\n\nresid_plots(g4, theme=\"new\")\n\n\n\n\n\n\n\nFigure 25: The plot of residuals with `theme=new’.\n\n\n\n\n\nFigure 26 has theme=\"ecdf\":\n\nresid_plots(g4, theme=\"ecdf\")\n\n\n\n\n\n\n\nFigure 26: The plot of residuals with `theme=ecdf’.\n\n\n\n\n\nFigure 27 has theme=\"ecdf\":\n\n  resid_plots(g4, theme=\"ts\")\n\n\n\n\n\n\n\nFigure 27: The plot of residuals with `theme=ts’.\n\n\n\n\n\n\n\n\nresid_symmetry()\nA symmetry plot is useful for detecting asymmetry (skewnwess) in the residuals. Here we plot the symmetry plot of the residuals for model m4.\n\ngg&lt;-resid_symmetry(g4)\ngg\n\n\n\n\n\n\n\nFigure 28: A symmetry plot of the residuals."
  },
  {
    "objectID": "gamlss-ggplots.html#responce",
    "href": "gamlss-ggplots.html#responce",
    "title": "The Functions in gamlss.ggplots",
    "section": "Responce",
    "text": "Responce"
  },
  {
    "objectID": "gamlss-ggplots.html#fitted",
    "href": "gamlss-ggplots.html#fitted",
    "title": "The Functions in gamlss.ggplots",
    "section": "Fitted",
    "text": "Fitted"
  },
  {
    "objectID": "gamlss-ggplots.html#models",
    "href": "gamlss-ggplots.html#models",
    "title": "The Functions in gamlss.ggplots",
    "section": "Models",
    "text": "Models\n\nmodel_GAIC()\nThe functions model_GAIC() and model_GAIC_lollipop() are identical but the appearance is different.\n\ngg&lt;-model_GAIC(g0,g1,g2,g3,g4, g5,g6,g7,g8,g9)\ngg\n\n\n\n\n\n\n\nFigure 41: plotting the GAIC.\n\n\n\n\n\n\n\nmodel_GAIC_lollipop()\n\ngg1&lt;-model_GAIC_lollipop(g0,g1,g2,g3,g4, g5,g6,g7,g8,g9)\ngg1\n\n\n\n\n\n\n\nFigure 42: plotting the GAIC.\n\n\n\n\n\n\n\nmodel_density()\nTo plot the residuals densities of all m1, m2, m3, and m4 models use:\n\ngg &lt;-  model_density(g1, g2, g3, g4)\ngg\n\n\n\n\nA density plot of the residuals from different models.\n\n\n\n\nFor multiple plots use:\n\ngg+facet_grid(cut_number(rent$Fl, 3)~rent$loc)\n\n\n\n\nA density plot of the residuals from different models.\n\n\n\n\n\n\nmodel_qqplot()\nThe function model_qqplot() can be use to get a QQ-plots for more that one fitted model residuals.\n\ngg &lt;- model_qqplot(g1, g2, g3, g4)\ngg\n\n\n\n\n\n\n\nFigure 43: QQ-plot of the residuals of models m1, m2, m3 and m4.\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nsomething is wrong here g2 and g3 are not appearing\n\n\nFor multiple plots use;\n\ngg+facet_grid(cut_number(rent$Fl, 3)~rent$loc)\n\n\n\n\n\n\n\nFigure 44: Multiple QQ-plots of the residuals of models m1, m2, m3 and m4.\n\n\n\n\n\nThe resulting plot is shown on the Figure 44.\n\n\nmodel_wp()\nThe function model_wp() can be use to get a worm-plots for multiple fitted models residuals.\n\ngg &lt;- model_wp(g1, g2, g3, g4)\ngg\n\n\n\n\n\n\n\nFigure 45: Worm-plot of the residuals of models m1, m2, m3 and m4.\n\n\n\n\n\n\n\nmodel_wp_wrap()\nFor model worm plots at differenbt values of the explnatory variables use;\n\ngg1 &lt;- model_wp_wrap(g1, g2, g3, g4, xvar=rent$A)\ngg1\n\n\n\n\n\n\n\nFigure 46: Multiple worm-plot of the residuals of models m1, m2, m3 and m4.\n\n\n\n\n\nThe resulting plot is shown on the right side of Figure Figure 46."
  },
  {
    "objectID": "gamlss-ggplots.html#plots-for-y-vectors",
    "href": "gamlss-ggplots.html#plots-for-y-vectors",
    "title": "The Functions in gamlss.ggplots",
    "section": "Plots for \\(y\\) vectors",
    "text": "Plots for \\(y\\) vectors\n\ny_hist()\nHere we generate a vector from a BCCG distribution and plot its histogram and its density function .\n\ny &lt;- rBCCG(1000, mu=3, sigma=.1, nu=-1)\ngg &lt;- y_hist(y)\ngg\n\n\n\n\n\n\n\nFigure 53: Plotting a histogram of single variable and its density function.\n\n\n\n\n\nHere we add the true pdf of the generated variable.\n\ngg + stat_function(fun = dBCT, args=list(mu=3, sigma=.1,  nu=-1, tau=5),\n      geom = \"area\", alpha=0.5, fill=\"lightblue\", color=\"black\", n=301)\n\n\n\n\n\n\n\nFigure 54: Plotting a histogram of single variable and the true distribution supperimpose\n\n\n\n\n\n\n\ny_dots()\nThe dots function is appropriate for long tail distributions be cause emphasise the the long left ot right tail of \\(y\\).\n\ny &lt;- rBCT(1000, mu=3, sigma=.1, nu=-1, tau=4)\ngg &lt;- y_dots(y)\ngg\n\n\n\n\n\n\n\nFigure 55: Plotting the \\(y\\) in dots to emphisise the long right tail.\n\n\n\n\n\n\n\ny_ecdf()\nThe empirical cdf of a variables can be plotted using the function y_ecdf()\n\ny &lt;- rBCT(1000, mu=3, sigma=.1, nu=-1, tau=4)\ngg &lt;- y_ecdf(y)\ngg\n\n\n\n\n\n\n\nFigure 56: Plotting the empirical cdf of a variable.\n\n\n\n\n\nHere is how you can add the theoretical cdf.\n\ngg+ stat_function(fun = \n          pBCT, args=list(mu=3, sigma=.1,  nu=-1, tau=5), \n          color=\"red\", n=301)\n\n\n\n\n\n\n\nFigure 57: Plotting the empirical cdf of a variable plues its theoretical cdf.\n\n\n\n\n\n\n\ny_acf()\nThe functions y_acf() take a single time series vector and plot its auto-correlation function.\n\ny_acf(diff(EuStockMarkets[,1])) \n\n\n\n\n\n\n\nFigure 58: Plotting ACF of the first difference of FTSE.\n\n\n\n\n\n\n\ny_pacf()\nThe functions y_pacf() take a single time series vector and plot its partial auto-correlation function.\n\ny_pacf(diff(EuStockMarkets[,1])) \n\n\n\n\n\n\n\nFigure 59: Plotting PACF of the first difference of FTSE.\n\n\n\n\n\n\n\ny_symmetry()\nHere we plot the symmetry plot of the variable \\(y\\).\n\ngg&lt;-y_symmetry(rent$R)\ngg\n\n\n\n\n\n\n\nFigure 60: A symmetry plot of the rent data."
  },
  {
    "objectID": "gamlss-ggplots.html#partial-effects",
    "href": "gamlss-ggplots.html#partial-effects",
    "title": "The Functions in gamlss.ggplots",
    "section": "Partial effects",
    "text": "Partial effects\nThose functions should be called fitted_pe\\_\\... but we have dropped the fitted part to simplify the notation.\n\npe_terms()\nThe function plots individual terms from a fitted gamlss model. It is equivalent to term.plot(). Note that the function will produce up to 9 term plots before ask for the next page.\n\ngamlss.ggplots:::pe_terms(m4, partial=T)\n\n\n\n\n\n\n\nFigure 61: Plotting terms from a fitted GAMLSS model.\n\n\n\n\n\nThe function pe_terms() uses the function lpred() to obtain the partial fitted terms. The function lpred() is using the parameter model.frame which has already evaluate the factors as a set of dummy variables. The partial terms are then evaluated by fixing the other terms on their means and after subtraction for the mean of the response.\n\n\n\n\n\n\nWarning\n\n\n\nNot working with gamlls2 objects"
  },
  {
    "objectID": "gamlss-ggplots.html#interpretation",
    "href": "gamlss-ggplots.html#interpretation",
    "title": "The Functions in gamlss.ggplots",
    "section": "interpretation",
    "text": "interpretation\n\npe_param()\nThe function pe_param() plots the partial effect of one or two specified term(s) given all other terms in the model remain fixed at predetermined values. Depending on length of the argument term the function pe_param() uses the function pe_1_param() or pe_2_param() to plot the partial effects. Here we plot it against a continuous variable A:\n\npe_param(g4, \"A\")\n\n\n\n\n\n\n\nFigure 62: Plotting partial terms from A.\n\n\n\n\n\nHere we plot it against a categorical variable loc:\n\npe_param(g4, \"loc\")\n\n\n\n\n\n\n\nFigure 63: Plotting partial terms from loc.\n\n\n\n\n\nHere we use two continuous variables A and Fl:\n\ngamlss.ggplots:::pe_param(g4, c(\"A\", \"Fl\"))\n\n\n\n\n\n\n\nFigure 64: Plotting partial terms from A and Fl.\n\n\n\n\n\nHere we use two continuous variables A and Fl but with the argument filled = TRUE:\n\ngamlss.ggplots:::pe_param(g4, c(\"A\", \"Fl\"), filled = TRUE)\n\n\n\n\n\n\n\nFigure 65: Plotting partial terms from A and Fl.\n\n\n\n\n\nHere we use one continuous A and one categorical variables loc:\n\ngamlss.ggplots:::pe_param(g4, c(\"A\", \"loc\"))\n\n\n\n\n\n\n\nFigure 66: Plotting partial terms from loc and A.\n\n\n\n\n\nHere we use two categorical variables H and loc:\n\ngamlss.ggplots:::pe_param(g4, c(\"H\", \"loc\"))\n\n\n\n\n\n\n\nFigure 67: Plotting partial terms from loc and H.\n\n\n\n\n\n\n\npe_param_grid()\nThe function pe_param_grid() show multiple plots of partial effects (main effect or first order interactions) given all other terms in the model remain fixed at predetermined values.\n\ngamlss.ggplots:::pe_param_grid(g5, list(c(\"Fl\", \"A\"), c(\"H\", \"loc\")))\n\n\n\n\n\n\n\nFigure 68: Plotting partial terms from loc and H.\n\n\n\n\n\n\ngamlss.ggplots:::pe_param_grid(g5, list(c(\"Fl\", \"A\"), c(\"H\", \"loc\")))\n\n\n\n\n\n\n\nFigure 69: Plotting the partial effect of terms in model m6. Note that the plot on the left panel is a genuine first order interaction term fitted with a 2 dimensional smoother using the interface with the package mgcv while the plot on the right panel is created using the two factors H and loc is just a two dimensional plot of the two main effects for H and loc respectively, since no interaction was fitted for those two terms"
  },
  {
    "objectID": "gamlss-ggplots.html#prediction",
    "href": "gamlss-ggplots.html#prediction",
    "title": "The Functions in gamlss.ggplots",
    "section": "prediction",
    "text": "prediction\n\npredict_pdf()\nNow with newdata we use `predict_pdf’:\n\npredict_pdf(a1, newdata=abdom[c(11,15,20,25),], from=38, to=130)\n\n\n\n\n\n\n\nFigure 70: Plotting linear leverage from a fitted GAMLSS model."
  },
  {
    "objectID": "gamlss-ggplots.html#centile-estimation-utilities",
    "href": "gamlss-ggplots.html#centile-estimation-utilities",
    "title": "The Functions in gamlss.ggplots",
    "section": "Centile estimation (Utilities)",
    "text": "Centile estimation (Utilities)\nHere we use three different GAMLSS models. Note that for all centile functions the name of the x-variable should appear in the plotting function as in the actual data otherwise the function will not find it:\n\nfractional polynomials basis\n\nm1 =gamlss(head~bfp(age,c(-2,-1,-.5,0,.5,1,2,3)), \n           ~bfp(age,c(-2,-1,-.5,0,.5,1,2,3)),\n           ~bfp(age,c(-2,-1,-.5,0,.5,1,2,3)),\n           ~bfp(age,c(-2,-1,-.5,0,.5,1,2,3)),\n           data=db, family=BCTo, trace=FALSE)\n\n\n\nP-splines\n\nm2 =gamlss(head~pb(age^.3), \n           ~pb(age^.3), \n           ~pb(age^.3),\n           ~pb(age^.3),\n           data=db, family=BCTo, c.crit=0.01, \n           trace=FALSE, n.cyc=50)\n\n\n\nfractional polynomials\n\nm3 =gamlss(head~fp(age,3), \n           ~fp(age,3), \n           ~fp(age,3),\n           ~fp(age,3),\n           data=db, family=BCTo, c.crit=0.01, \n           trace=FALSE, n.cyc=50)\nGAIC.table(m1,m2, m3)\n\nminimum GAIC(k= 2 ) model: m2 \nminimum GAIC(k= 3.84 ) model: m2 \nminimum GAIC(k= 8.86 ) model: m2 \n\n\n         df      k=2   k=3.84   k=8.86\nm1 36.00000 26809.37 26875.61 27056.33\nm2 22.66598 26789.02 26830.73 26944.51\nm3 28.00000 26978.73 27030.25 27170.81\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis needs to be translated to gamlls2 objects\n\n\n\n\nClassical functions\nThose the three classical GAMLSS centile functions;\n\ncentiles(m1)#  works\n#######################################################################\ncentiles.fan(m1)#  works\n#######################################################################\ncentiles.com(m1,m2,m3 )# works\n\nNote that you can actually can specify the x variable and produce identical results:\n\ncentiles(m1, xvar=db$age)# works\ncentiles(m1, xvar=age)# works\n\n\n\nfitted_centiles()\nThe fitted_centiles() function, produces identical results with the function centiles() of gamlss.\n\nfitted_centiles(m2)\n\n\n\n\n\n\n\nFigure 71: Plotting the centiles for model m2.\n\n\n\n\n\nThe function fitted_centiles()'  can be used withfacet_wrap()`:\n\nfitted_centiles(m2)+\n  facet_wrap(cut_number(db$age, 3), scales = \"free_x\")\n\n\n\n\n\n\n\nFigure 72: Plotting the centiles for models m1, m2 and m3.\n\n\n\n\n\n\n\nfitted_centiles_legend()\nTo have legend in the plot use the following which it takes more time.\n\nfitted_centiles_legend(m1)\n\n\n\n\n\n\n\nFigure 73: Plotting the centiles for model m2.\n\n\n\n\n\n\n\nmodel_centiles()\nIf you wnat to check the centiles curves for all models used;\n\nmodel_centiles(m1,m2,m3,  xvar=age)\n\n\n\n\n\n\n\nFigure 74: Plotting the centiles for models m1, m2 and m3.\n\n\n\n\n\nor\n\nmodel_centiles(m1,m2,m3,  xvar=age, in.one=TRUE)\n\n\n\n\n\n\n\nFigure 75: Plotting the centiles for models m1, m2 and m3.\n\n\n\n\n\nThe function model_centiles(..., in.one=TRUE)'  can be used withfacet_wrap()`:\n\nmodel_centiles(m1,m2,m3, xvar=age, in.one=TRUE)+\n  facet_wrap(cut_number(db$age, 4), scales = \"free_x\")# working\n\n\n\n\n\n\n\nFigure 76: Plotting the centiles for models m1, m2 and m3."
  },
  {
    "objectID": "gamlss-ggplots.html#versions",
    "href": "gamlss-ggplots.html#versions",
    "title": "The Functions in gamlss.ggplots",
    "section": "",
    "text": "The latest versions of the key packages are:\n\nrm(list=ls())\n#getRversion()\nlibrary(gamlss)\nlibrary(gamlss2)\nlibrary(ggplot2)\nlibrary(gamlss.ggplots)\nlibrary(\"dplyr\") \npackageVersion(\"gamlss\")\n\n[1] '5.4.23'\n\npackageVersion(\"gamlss2\")\n\n[1] '0.1.0'\n\npackageVersion(\"gamlss.ggplots\")\n\n[1] '2.1.17'"
  },
  {
    "objectID": "gamlss-ggplots.html#overview",
    "href": "gamlss-ggplots.html#overview",
    "title": "The Functions in gamlss.ggplots",
    "section": "",
    "text": "Originally, the package gamlss.ggplots included all the functionality currently split between gamlss.ggplots and gamlss.prepdata. Due to maintenance challenges caused by the growing size and complexity of the package, it was divided into two separate components:\n- `gamlss.ggplots` — visualization and diagnostics\n\n- `gamlss.prepdata` — data preparation utilities\nAt this stage, gamlss.ggplots is experimental. Some functions are not yet exported (i.e., “hidden”) to allow for further testing and validation. These hidden functions can still be accessed using the triple-colon syntax:\n\ngamlss.ggplots:::function_name()\n\n\n\nFunctions in gamlss.ggplots can be broadly classified into three categories:\n\n\nMost if the before-fitting a model functions are moved to gamlss.prepdata package but few visualizing distribution families are remaining. Distribution relatated function names begin with family_Name() - Example: family_pdf(NO) which plots the density distribution from the Normal GAMLSS family.\n\n\n\nThese functions are used after a model has been fitted, providing diagnostics, interpretation aids, and predictions.\n\nResidual-based diagnostics - resid_NAME() – Diagnostics on residuals E.g., resid_wp() produces a worm plot for assessing the distribution.\nFitted value visualizations - fitted_NAME() – Visualizes fitted parameters (\\(\\mu\\), \\(\\sigma\\), etc.) from a fitted model.\nResponse variable plots resp_NAME() – Plots the response variable against various model-based quantities such as parameters or quantiles.\nModel interpretation\n\n\npe_NAME() – Plots partial effects to aid model interpretation.\ninfluence_NAME() – Shows influence measures of model terms.\npredict_NAME() – Provides predictions, often using a newdata argument.\n\n\nModel comparison diagnostics\n\nmodel_NAME() – Functions for comparing multiple models, typically using diagnostic statistics or GAIC.\n\n\n\n\n\nThese functions serve as helpers for data exploration and bootstrap analyses.\n\nSingle-vector visualizations • y_NAME() – For example, y_hist(y) plots a histogram, y_acf(y) plots the autocorrelation function.\nBootstrap summaries • boot_NAME() – Plots parameter estimates or fitted values across bootstrap samples.\nFunction imitations\n\nSome utility functions imitate existing gamlss functions. E.g., histSmo_plot() mimics histSmo() but uses ggplot2.\nBecause the package is at an experimental stage, some of the functions are hidden to allowed time for checking. The hidden functions can be accessed using gamlss.ggplots:::functionname().\nNote future functions that are not included in the package at the moment are:\n\nale_param() for accumulated local effects of a specific term on the parameter (Mikis)\npd_param() for partial dependants plots of a specific term on the parameter (Mikis)\npe_exceedance() (Julian)"
  },
  {
    "objectID": "gamlss-ggplots.html#models-1",
    "href": "gamlss-ggplots.html#models-1",
    "title": "The Functions in gamlss.ggplots",
    "section": "",
    "text": "The functions model_GAIC() and model_GAIC_lollipop() are identical but the appearance is different.\n\ngg&lt;-model_GAIC(g0,g1,g2,g3,g4, g5,g6,g7,g8,g9)\ngg\n\n\n\n\n\n\n\nFigure 41: plotting the GAIC.\n\n\n\n\n\n\n\n\n\ngg1&lt;-model_GAIC_lollipop(g0,g1,g2,g3,g4, g5,g6,g7,g8,g9)\ngg1\n\n\n\n\n\n\n\nFigure 42: plotting the GAIC.\n\n\n\n\n\n\n\n\nTo plot the residuals densities of all m1, m2, m3, and m4 models use:\n\ngg &lt;-  model_density(g1, g2, g3, g4)\ngg\n\n\n\n\nA density plot of the residuals from different models.\n\n\n\n\nFor multiple plots use:\n\ngg+facet_grid(cut_number(rent$Fl, 3)~rent$loc)\n\n\n\n\nA density plot of the residuals from different models.\n\n\n\n\n\n\n\nThe function model_qqplot() can be use to get a QQ-plots for more that one fitted model residuals.\n\ngg &lt;- model_qqplot(g1, g2, g3, g4)\ngg\n\n\n\n\n\n\n\nFigure 43: QQ-plot of the residuals of models m1, m2, m3 and m4.\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nsomething is wrong here g2 and g3 are not appearing\n\n\nFor multiple plots use;\n\ngg+facet_grid(cut_number(rent$Fl, 3)~rent$loc)\n\n\n\n\n\n\n\nFigure 44: Multiple QQ-plots of the residuals of models m1, m2, m3 and m4.\n\n\n\n\n\nThe resulting plot is shown on the Figure 44.\n\n\n\nThe function model_wp() can be use to get a worm-plots for multiple fitted models residuals.\n\ngg &lt;- model_wp(g1, g2, g3, g4)\ngg\n\n\n\n\n\n\n\nFigure 45: Worm-plot of the residuals of models m1, m2, m3 and m4.\n\n\n\n\n\n\n\n\nFor model worm plots at differenbt values of the explnatory variables use;\n\ngg1 &lt;- model_wp_wrap(g1, g2, g3, g4, xvar=rent$A)\ngg1\n\n\n\n\n\n\n\nFigure 46: Multiple worm-plot of the residuals of models m1, m2, m3 and m4.\n\n\n\n\n\nThe resulting plot is shown on the right side of Figure Figure 46."
  },
  {
    "objectID": "gamlss-ggplots.html#gamlss",
    "href": "gamlss-ggplots.html#gamlss",
    "title": "The Functions in gamlss.ggplots",
    "section": "",
    "text": "The general GAMLSS model can be written as \\[\n\\begin{split}\ny_i     &  \\stackrel{\\small{ind}}{\\sim }  \\mathcal{D}( \\theta_1, \\ldots, \\theta_k) \\nonumber \\\\\ng(\\theta_1) &= \\mathcal{ML}_1(x_{1i},x_{2i}, \\ldots,  x_{pi}) \\nonumber \\\\\n\\ldots &= \\ldots \\nonumber\\\\\ng(\\theta_k) &= \\mathcal{ML}_k(x_{1i},x_{2i}, \\ldots,  x_{pi}).\n\\end{split}\n  \\tag{1}\\] When only additive smoothing terms are fitted the model can be written; \\[\\begin{split}\ny_i     &  \\stackrel{\\small{ind}}{\\sim }  \\mathcal{D}( \\theta_1, \\ldots, \\theta_k) \\nonumber \\\\\ng(\\theta_1)  &= b_0 + s_1(x_{1i})  +  \\cdots,  +s_p(x_{pi}) \\nonumber\\\\\n\\cdots &=& \\cdots \\nonumber\\\\\ng(\\theta_k)  &= b_0 + s_1(x_{1i})  +   \\cdots,  +s_p(x_{pi}).\n\\end{split}\n\\tag{2}\\]"
  },
  {
    "objectID": "gamlss-prepdata.html#gamlss.prepdata",
    "href": "gamlss-prepdata.html#gamlss.prepdata",
    "title": "The Functions in the Package gamlss.prepdata",
    "section": "gamlss.prepdata",
    "text": "gamlss.prepdata\nThe gamlss.prepdata package originated from the gamlss.ggplots package. As gamlss.ggplots became too large for easy maintenance, it was split into two separate packages, and gamlss.prepdata was created.\nSince gamlss.prepdata is still at an experimental stage, some functions are hidden to allow time for thorough checking and validation. These hidden functions can still be accessed using the triple colon notation, for example: gamlss.prepdata:::.\nThe functions available in gamlss.prepdata are intended for pre-fitting — that is, to be used before applying the gamlss() or gamlss2() fitting functions. The available functions can be grouped into the following categories:\n\nInformation functions\nThese functions provide information about:\n- The size of the dataset\n\n- The presence and extent of missing values\n\n- The structure of the dataset\n\nWhether the variables are numeric or factors, and how they should be prepared for analysis\n\n\n\nPlotting functions\nThese functions allow plotting for:\n-   individual variables\n\n-   Pairwise relationships between variables.\n\n\nFeatures functions\nFunctions that assist in\n\nDetecting outliers\nApplying transformations\nScaling variables.\n\n\n\nData Partition functions\nFunctions that facilitate partitioning data to improve inference and avoid overfitting during model selection.\n\n\nPurpose and Usage\nThe information and plotting functions provide valuable insights that assist in building better models, including:\n\nUnderstanding the distribution of the response variable\nChoosing the appropriate type of analysis\nExamining explanatory variables, including:\n\nRange and spread of values\nPresence of missing values\nAssociations and interactions between explanatory variables\nNature of relationships between response and explanatory variables (linear or non-linear)\n\n\nThe features functions focus on handling outliers, scaling, and transforming explanatory variables (x-variables) before modeling.\nData partitioning is used to avoid overfitting by ensuring models are evaluated more reliably. It falls under the broader category of data manipulation, although merging datasets is not covered here — only partitioning for improved inference is addressed.\nMost of the pre-fitting functions are data-related, and their names typically start with data (e.g., data_NAME), indicating that they either print information, produce plots, or manipulate data.frames.\nThe gamlss.prepdata package is thus a useful tool for carrying out pre-analysis work before beginning the process of fitting a distributional regression model.\nNext, we define what we mean by distributional regression."
  },
  {
    "objectID": "gamlss-prepdata.html#distributional-regression",
    "href": "gamlss-prepdata.html#distributional-regression",
    "title": "The Functions in the Package gamlss.prepdata",
    "section": "Distributional Regression",
    "text": "Distributional Regression\nThe aim of this vignette is to demonstrate how to manipulate and prepare data before applying a distributional regression analysis.\nThe general form a distributional regression model can be written as; \\[\n\\begin{split}\n\\textbf{y}     &  \\stackrel{\\small{ind}}{\\sim }  \\mathcal{D}( \\boldsymbol{\\theta}_1, \\ldots, \\boldsymbol{\\theta}_k) \\nonumber \\\\\ng_1(\\boldsymbol{\\theta}_1) &= \\mathcal{ML}_1(\\textbf{x}_{11},\\textbf{x}_{21}, \\ldots,  \\textbf{x}_{p1}) \\nonumber \\\\\n\\ldots &= \\ldots \\nonumber\\\\\ng_k(\\boldsymbol{\\theta}_k) &= \\mathcal{ML}_k(\\textbf{x}_{1k},\\textbf{x}_{2k}, \\ldots,  \\textbf{x}_{pk}).\n\\end{split}\n  \\tag{1}\\] where we assume that the response variable \\(y_i\\) for \\(i=1,\\ldots, n\\), is independently distributed having a distribution \\(\\mathcal{D}( \\theta_1, \\ldots, \\theta_k)\\) with \\(k\\) parameters and where all parameters could be effected by the explanatory variables \\(\\textbf{x}_{1},\\textbf{x}_{2}, \\ldots,  \\textbf{x}_{p}\\). The \\(\\mathcal{ML}\\) represents any regression type machine learning algorithm i.e. LASSO, Neural networks etc.\nWhen only additive smoothing terms are used in the fitting the model can be written as; \\[\\begin{split}\n\\textbf{y}     &  \\stackrel{\\small{ind}}{\\sim }  \\mathcal{D}(  \\boldsymbol{\\theta}_1, \\ldots,  \\boldsymbol{\\theta}_k) \\nonumber \\\\\ng_1( \\boldsymbol{\\theta}_1)  &= b_{01} + s_1(\\textbf{x}_{11})  +  \\cdots,  +s_p(\\textbf{x}_{p1}) \\nonumber\\\\\n\\ldots &= \\ldots \\nonumber\\\\\ng_k( \\boldsymbol{\\theta}_k)  &= b_{0k} + s_1(\\textbf{x}_{1k})  +   \\cdots,  +s_p(\\textbf{x}_{pk}).\n\\end{split}\n\\tag{2}\\] which is the GAMLSS model introduced by Rigby and Stasinopoulos (2005).\nThere are three books on GAMLSS, D. M. Stasinopoulos et al. (2017), Rigby et al. (2019) and M. D. Stasinopoulos et al. (2024) and several ’GAMLSS lecture materials, available from GitHUb, https://github.com/mstasinopoulos/Porto_short_course.git and https://github.com/mstasinopoulos/ShortCourse.git. The latest R packages related to GAMLSS can be found in https://gamlss-dev.r-universe.dev/builds.\nThe aim of this package is to prepare data and extract useful information that can be utilized during the modeling stage."
  },
  {
    "objectID": "gamlss-ggplots.html#resid_symmetry",
    "href": "gamlss-ggplots.html#resid_symmetry",
    "title": "The Functions in gamlss.ggplots",
    "section": "resid_symmetry()",
    "text": "resid_symmetry()\nA symmetry plot is useful for detecting asymmetry (skewnwess) in the residuals. Here we plot the symmetry plot of the residuals for model m4.\n\ngg&lt;-resid_symmetry(g4)\ngg\n\n\n\n\n\n\n\nFigure 29: A symmetry plot of the residuals."
  },
  {
    "objectID": "gamlss-ggplots.html#resid_wp_wrap",
    "href": "gamlss-ggplots.html#resid_wp_wrap",
    "title": "The Functions in gamlss.ggplots",
    "section": "resid_wp_wrap()",
    "text": "resid_wp_wrap()\nThe worm plot for a single model at different values of an explanatory variable can be plotted using:\n\ngg1 &lt;-resid_wp_wrap(g4, xvar=rent$A)\ngg1\n\n\n\n\n\n\n\nFigure 19: Worm-plot of the residuals for model m4 at different values of A\n\n\n\n\n\nSee Figure 19"
  },
  {
    "objectID": "gamlss-ggplots.html#model_mom_bucket",
    "href": "gamlss-ggplots.html#model_mom_bucket",
    "title": "The Functions in gamlss.ggplots",
    "section": "`model_mom_bucket()",
    "text": "`model_mom_bucket()\nThe function model_mom_bucket() (also known as mement_bucket()) can be use to get the moment bucket plot for one or more fitted model residuals. Note that moment_bucket() is synonymous to model_mom_bucket().\n\ngg &lt;- moment_bucket(g1, g2, g3, g4)\ngg\n\n\n\n\n\n\n\nFigure 48: Single moment bucket plots of the residuals of models m1, m2, m3 and m4."
  },
  {
    "objectID": "gamlss-ggplots.html#moment_bucket_wrap",
    "href": "gamlss-ggplots.html#moment_bucket_wrap",
    "title": "The Functions in gamlss.ggplots",
    "section": "moment_bucket_wrap()",
    "text": "moment_bucket_wrap()\nFor multiple plots use\n\ngg1 &lt;- moment_bucket_wrap(m1, m2, m3, m4, xvar=rent$A)\ngg1 \n\n\n\n\n\n\n\nFigure 49: Multiple moment bucket plots of the residuals of models g1, g2, g3 and g4.\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nNot working with gamlls2 objects\n\n\nThe resulting plot is shown on the right side of Figure Figure 49.\n\nmoment_gray_both() and moment_colour_both()\nNote that the background of the bucket plots is generated by the functions\n\nmoment_gray_both()\nmoment_colour_both()\n\n\n\n\n\n\n\nFigure 50: The background of the moment bucket plot.\n\n\n\n\n\n\n\n\n\n\n\nFigure 51: The background of the moment bucket plot."
  },
  {
    "objectID": "gamlss-ggplots.html#xectors",
    "href": "gamlss-ggplots.html#xectors",
    "title": "The Functions in gamlss.ggplots",
    "section": "Xectors",
    "text": "Xectors\n\ny_hist()\nHere we generate a vector from a BCCG distribution and plot its histogram and its density function .\n\ny &lt;- rBCCG(1000, mu=3, sigma=.1, nu=-1)\ngg &lt;- y_hist(y)\ngg\n\n\n\n\n\n\n\nFigure 53: Plotting a histogram of single variable and its density function.\n\n\n\n\n\nHere we add the true pdf of the generated variable.\n\ngg + stat_function(fun = dBCT, args=list(mu=3, sigma=.1,  nu=-1, tau=5),\n      geom = \"area\", alpha=0.5, fill=\"lightblue\", color=\"black\", n=301)\n\n\n\n\n\n\n\nFigure 54: Plotting a histogram of single variable and the true distribution supperimpose\n\n\n\n\n\n\n\ny_dots()\nThe dots function is appropriate for long tail distributions be cause emphasise the the long left ot right tail of \\(y\\).\n\ny &lt;- rBCT(1000, mu=3, sigma=.1, nu=-1, tau=4)\ngg &lt;- y_dots(y)\ngg\n\n\n\n\n\n\n\nFigure 55: Plotting the \\(y\\) in dots to emphisise the long right tail.\n\n\n\n\n\n\n\ny_ecdf()\nThe empirical cdf of a variables can be plotted using the function y_ecdf()\n\ny &lt;- rBCT(1000, mu=3, sigma=.1, nu=-1, tau=4)\ngg &lt;- y_ecdf(y)\ngg\n\n\n\n\n\n\n\nFigure 56: Plotting the empirical cdf of a variable.\n\n\n\n\n\nHere is how you can add the theoretical cdf.\n\ngg+ stat_function(fun = \n          pBCT, args=list(mu=3, sigma=.1,  nu=-1, tau=5), \n          color=\"red\", n=301)\n\n\n\n\n\n\n\nFigure 57: Plotting the empirical cdf of a variable plues its theoretical cdf.\n\n\n\n\n\n\n\ny_acf()\nThe functions y_acf() take a single time series vector and plot its auto-correlation function.\n\ny_acf(diff(EuStockMarkets[,1])) \n\n\n\n\n\n\n\nFigure 58: Plotting ACF of the first difference of FTSE.\n\n\n\n\n\n\n\ny_pacf()\nThe functions y_pacf() take a single time series vector and plot its partial auto-correlation function.\n\ny_pacf(diff(EuStockMarkets[,1])) \n\n\n\n\n\n\n\nFigure 59: Plotting PACF of the first difference of FTSE.\n\n\n\n\n\n\n\ny_symmetry()\nHere we plot the symmetry plot of the variable \\(y\\).\n\ngg&lt;-y_symmetry(rent$R)\ngg\n\n\n\n\n\n\n\nFigure 60: A symmetry plot of the rent data."
  }
]