---
title: "The Functions in the Package `gamlss.prepdata`"
author:
  - name: De Bastiani, F.
    affiliation: Federal University of Pernambuco, Brazil  
  - name: Heller, G.
    affiliation: University of Syndey, Austalia  
  - name: Kneib, T.
    affiliation: Georg-August-Universitat Gottigan, Gernany
  - name: Mayr, A.
    affiliation: University of Marburg, Gernany  
  - name: Rigby, R. A.
    affiliation: University of Greenwich, UK
  - name: Stasinopoulos, M. D.
    affiliation: University of Greenwich, UK
    email: d.stasinopoulos@gre.ac.uk
  - name: Stauffer, Reto
    affiliation: University of Innsbruck, Austria
  - name: Umlauf, N,   
    affiliation: University of Innsbruck, Austria
  - name: Zeileis, A.  
    affiliation: University of Innsbruck, Austria
format:
  html: default 
  pdf: default  
toc-depth: 2
editor: visual
bibliography: book2024.bib
---

## Introduction {#sec-introduction}

This booklet introduces the `gamlss.prepdata` package and its functionality. It aims to describe the available functions and how they can be used.

<!--  At a later stage, we will try to describe their rational. -->

The latest versions of the packages **gamlss**, **gamlss2** and **gamlss.prepdata** are shown below:

```{r}
#| message: false
#| fig-show: hide
#| warning: false
rm(list=ls())
library(gamlss)
library(gamlss2)
library(ggplot2)
library(gamlss.ggplots)
library(gamlss.prepdata)
library("dplyr") 
packageVersion("gamlss")
packageVersion("gamlss2")
packageVersion("gamlss.prepdata")
```

## `gamlss.prepdata` 

The `gamlss.prepdata` package originated from the `gamlss.ggplots` package. As `gamlss.ggplots` became too large for easy maintenance, it was split into two separate packages, and `gamlss.prepdata` was created.

Since `gamlss.prepdata` is still at an experimental stage, some functions are hidden to allow time for thorough checking and validation. These hidden functions can still be accessed using the triple colon notation, for example: `gamlss.prepdata:::`.

The functions available in `gamlss.prepdata` are intended for pre-fitting — that is, to be used before applying the `gamlss()` or `gamlss2()` fitting functions. The available functions can be grouped into the following categories:

#### **Information** functions

These functions provide information about:

```         
- The size of the dataset

- The presence and extent of missing values

- The structure of the dataset
```

-   Whether the variables are numeric or factors, and how they should be prepared for analysis

#### **Plotting** functions

These functions allow plotting for:

```         
-   individual variables

-   Pairwise relationships between variables.
```

#### **Features** functions

Functions that assist in

-   Detecting outliers

-   Applying transformations

-   Scaling variables.

#### **Data Partition** functions

Functions that facilitate partitioning data to improve inference and avoid overfitting during model selection.

#### Purpose and Usage

The information and plotting functions provide valuable insights that assist in building better models, including:

-   Understanding the distribution of the *response* variable
-   Choosing the appropriate *type* of analysis
-   Examining explanatory variables, including:
    -   Range and spread of values
    -   Presence of missing values
    -   *Associations* and *interactions* between explanatory variables
    -   Nature of relationships between response and explanatory variables (*linear* or *non-linear*)

The features functions focus on handling outliers, scaling, and transforming explanatory variables (x-variables) before modeling.

Data partitioning is used to avoid overfitting by ensuring models are evaluated more reliably. It falls under the broader category of data manipulation, although merging datasets is not covered here — only partitioning for improved inference is addressed.

Most of the pre-fitting functions are data-related, and their names typically start with data (e.g., `data_NAME`), indicating that they either print information, produce plots, or manipulate `data.frame`s.

The `gamlss.prepdata` package is thus a useful tool for carrying out pre-analysis work before beginning the process of fitting a distributional regression model.

Next, we define what we mean by **distributional regression**.

## Distributional Regression

The aim of this vignette is to demonstrate how to manipulate and prepare data before applying a distributional regression analysis.

The general form a distributional regression model can be written as; $$
\begin{split}
\textbf{y}     &  \stackrel{\small{ind}}{\sim }  \mathcal{D}( \boldsymbol{\theta}_1, \ldots, \boldsymbol{\theta}_k) \nonumber \\
g_1(\boldsymbol{\theta}_1) &= \mathcal{ML}_1(\textbf{x}_{11},\textbf{x}_{21}, \ldots,  \textbf{x}_{p1}) \nonumber \\
 \ldots &= \ldots \nonumber\\
g_k(\boldsymbol{\theta}_k) &= \mathcal{ML}_k(\textbf{x}_{1k},\textbf{x}_{2k}, \ldots,  \textbf{x}_{pk}). 
 \end{split}
 $$ {#eq-GAMLSS_ML} where we assume that the response variable $y_i$ for $i=1,\ldots, n$, is independently distributed having a distribution $\mathcal{D}( \theta_1, \ldots, \theta_k)$ with $k$ parameters and where all parameters could be effected by the explanatory variables $\textbf{x}_{1},\textbf{x}_{2}, \ldots,  \textbf{x}_{p}$. The $\mathcal{ML}$ represents **any** regression type machine learning algorithm i.e. LASSO, Neural networks etc.

When only **additive smoothing** terms are used in the fitting the model can be written as; $$\begin{split}
\textbf{y}     &  \stackrel{\small{ind}}{\sim }  \mathcal{D}(  \boldsymbol{\theta}_1, \ldots,  \boldsymbol{\theta}_k) \nonumber \\
g_1( \boldsymbol{\theta}_1)  &= b_{01} + s_1(\textbf{x}_{11})  +  \cdots,  +s_p(\textbf{x}_{p1}) \nonumber\\
 \ldots &= \ldots \nonumber\\
g_k( \boldsymbol{\theta}_k)  &= b_{0k} + s_1(\textbf{x}_{1k})  +   \cdots,  +s_p(\textbf{x}_{pk}).
\end{split}
$$ {#eq-GAMLSS} which is the GAMLSS model introduced by @RigbyStasinopoulos05.

There are three books on GAMLSS, @Stasinopoulosetal2017, @Rigbyetal2019 and @stasinopoulos2024generalized and several 'GAMLSS lecture materials, available from GitHUb, <https://github.com/mstasinopoulos/Porto_short_course.git> and <https://github.com/mstasinopoulos/ShortCourse.git>. The latest **R** packages related to GAMLSS can be found in <https://gamlss-dev.r-universe.dev/builds>.

The aim of this package is to prepare data and extract useful information that can be utilized during the modeling stage.

## Ιnformation functions {#sec-Information}

The functions for obtaining information from a dataset are described in this section and summarized in @tbl-infromation. These functions can provide:

-   General information about the dataset such as the dimensions (number of rows and columns) and the percentage of omitted (missing) observations

-   Information about the variables in the dataset

-   Information about the observations in the dataset

All functions have a `data.frame` as their first argument.

Here is a table of the information functions;

| Functions | Usage |
|:-----------------------------------|-----------------------------------:|
| [`data_dim()`](#sec-data-dim) | Returns the number of rows and columns, and calculates the percentage of omitted (missing) observations |
| [`data_names()`](#sec-data-names) | Lists the names of the variables in the dataset |
| [`data_rename()`](#sec-data-rename) | Allows renaming of one or more variables in the dataset |
| [`data_shorter_names()`](#sec-data-shorter-names) | Allows shortening the names of one or more variables in the dataset |
| [`data_distinct()`](#sec-data-distinct) | Displays the number of distinct values for each variable in the dataset |
| [`data_which_na()`](#sec-data-which-na) | Displays the count of NA (missing) values for each variable in the dataset |
| [`data_omit()`](#sec-data-omit) | Removes all rows (observations) that contain any NA (missing) values |
| [`data_str()`](#sec-data-str) | Displays the class of each variable (e.g., numeric, factor, etc.) along with additional details about the variables |
| [`data_cha2fac()`](#sec-data-cha2fac) | Converts all character variables in the dataset to factor type |
| [`data_few2fac()`](#sec-data-few2fac) | Converts variables with a small number of distinct values (e.g., binary or categorical) to factor type |
| [`data_int2num()`](#sec-data-int2num) | Converts integer variables with several distinct values to numeric type to allow for continuous analysis |
| [`data_rm()`](#sec-data-rm) | Removes one or more variables (columns) from the dataset as specified by the user |
| [`data_rmNAvars()`](#sec-data-rmNAvars) | Removes variables which have NA's as all its values. |
| [`data_rm1val()`](#sec-data-rm1val) | Removes factors that have only a single level (no variability) in the dataset |
| [`data_select()`](#sec-data-select) | Allows the user to select one or more specific variables (columns) from the dataset for further analysis |
| [`data_exclude_class()`](#sec-data-exclude-class) | Removes all variables of a specified class (e.g., factor, numeric) from the dataset |
| [`data_only_continuous()`](#sec-data-only-continuous) | Retains only the numeric variables (columns) in the dataset, excluding factors or other variable types |

: A summary table of the functions to obtain information {#tbl-infromation} {tbl-colwidths="\[50,50\]"}

Next we give examples of the functions.

#### `data_dim()` {#sec-data-dim}

This function provides detailed information about the dimensions of a `data.frame`. It is similar to the R function `dim()`, but with additional details. The output is the original data frame, allowing it to be used in a series of piping commands.

```{r}
rent99 |> data_dim()
```

[back to table](#tbl-infromation)

#### `data_names()` {#sec-data-names}

This function provides the names of the variables in the `data.frame`, similar to the `R` function `names()`.

```{r}
rent99 |> data_names()
```

The output of the function is the original data frame. [back to table](#tbl-infromation)

#### `data_rename()` {#sec-data-rename}

Renames one or more variables (columns) in the `data.frame` using the function `data_rename()`;

```{r}
da<- rent99 |> data_rename(oldname="rent", newname="R")
head(da)
```

The output of the function is the original data frame with new names.

[back to table](#tbl-infromation)

#### `data_shorter_names()` {#sec-data-shorter-names}

If the variables in the dataset have very long names, they can be difficult to handle in formulae during modelling. The function `data_shorter_names()` abbreviates the names of the explanatory variables, making them easier to use in formulas.

```{r}
rent99 |> data_shorter_names()
```

If no long variable names exist in the dataset, the function data_shorter_names() does nothing. However, when applicable, the function abbreviates long names and returns the original data frame with the new shortened names.

::: callout-warning
Note that there is a risk when using a small value for the `max` option, as it may result in identical names for different variables. This could lead to confusion or errors in the modelling process. It is important to carefully choose an appropriate value for `max` to avoid this issue.
:::

[back to table](#tbl-infromation)

#### `data_distinct()` {#sec-data-distinct}

TThe distinct values for each variable are shown below.

```{r}
rent99 |> data_distinct()
```

The output of the function is the original data frame.

[back to table](#tbl-infromation)

#### `data_which_na()` {#sec-data-which-na}

This function provides information about which variables have missing observations and how many missing values there are for each variable;

```{r}
rent99 |> data_which_na()
```

The output of the function is the original data frame nothing is changing.

[back to table](#tbl-infromation)

#### `data_omit` {#sec-data-omit}

The function `data_str()` (similar to the `R` function `str()`) provides information about the types of variables present in the dataset.

```{r}
rent99 |> data_omit()
```

The output of the function is a new data frame with all NA's omitted.

::: callout-warning
It is important to select the relevant variables before using the data_omit() function, as some unwanted variables may contain many missing values that could lead to unnecessary row omissions.
:::

[back to table](#tbl-infromation)

#### `data_str()` {#sec-data-str}

The function `data_str()` (similar to the **R** function `str()`) provides information about the types of variable exist in the data.

```{r}
rent99 |> data_str()
```

The output of the function is the original data frame.

The next set of functions manipulate variables withing `data.frame`s.

[back to table](#tbl-infromation)

#### `data_cha2fac()` {#sec-data-cha2fac}

Often, variables in datasets are read as character vectors, but for analysis, they may need to be treated as factors. This function transforms any character vector (with a relatively small number of distinct values) into a factor.

```{r}
rent99 |> data_cha2fac()  -> da 
```

Since no character were found nothing have changed. The output of the function is a new data frame.

[back to table](#tbl-infromation)

#### `data_few2fac()` {#sec-data-few2fac}

There are occasions when some variables have very few distinct observations, and it may be better to treat them as factors. The function `data_few2fac()` converts vectors with a small number of distinct values into factors.

```{r}
rent99 |>  data_few2fac() -> da 
str(da)
```

The output of the function is a new data frame. This can be seen by using the **R** function `str()`;

[back to table](#tbl-infromation)

#### `data_int2num()` {#sec-data-int2num}

Occasionally, we need to convert integer variables with a very large range of values into numeric vectors, especially for graphics. The function `data_int2num()` performs this conversion.

```{r}
rent99 |> data_int2num() -> da
```

The output of the function is a new data frame.

[back to table](#tbl-infromation)

#### `data_rm()` {#sec-data-rm}

For plotting datasets, it’s easier to keep only the relevant variables. The function `data_rm()` allows you to remove unnecessary variables, making the dataset more manageable for visualization.

```{r}
data_rm(rent99, c(2,9)) -> da
dim(rent99)
dim(da)
```

The output of the function is a new data frame. Note that this could be also done using the function `select()` of the package **dplyr**, or our own `data_select()` function.

[back to table](#tbl-infromation)

#### `data_rmNAvars()` {#sec-data-rmNAvars}

Occutionally when we read data from files in `R`some extra variables are accidentaly produced with no values but NA's. The function `data_rmNAvars()` removes those variables from the data;

```{r}
data_rmNAvars(da)
```

[back to table](#tbl-infromation)

#### `data_rm1val()` {#sec-data-rm1val}

This function searches for variables with only a single distinct value (often factors left over from a previous `subset()` operation) and removes them from the dataset.

```{r}
da |> data_rm1val()
```

The output of the function is a new data frame.

[back to table](#tbl-infromation)

#### `data_select()` {#sec-data-select}

This function selects specified variables from a dataset.

```{r}
da1<-rent |> data_select( vars=c("R", "Fl", "A"))
head(da1)
```

The output of the function is a new data frame.

[back to table](#tbl-infromation)

#### `data_exclude_class()` {#sec-data-exclude-class}

This function searches for variables (columns) of a specified `R` class and removes them from the dataset. By default, the class to remove is factor.

```{r}
#| eval: true 
da |> data_exclude_class()  -> da1
head(da1)
```

The output of the function is a new data frame.

#### `data_only_continuous()` {#sec-data-only-continuous}

This function keeps only the continuous variables in the dataset, removing all non-continuous variables.

```{r}
da |> data_only_continuous()  -> da1
head(da1)
```

The output of the function is a new data frame.

[back to table](#tbl-infromation)

## Graphical functions {#sec-Graphical-functions}

Graphical methods are used in pre-analysis to examine individual variables or pair-wise relationships between variables.

| Functions | Usage |
|:-----------------------------------|-----------------------------------:|
| [`data_plot()`](#sec-data-plot) | Plots each variable in the dataset to visualize its distribution or other important characteristics (see also @sec-Outliers |
| [`data_bucket()`](#sec-data-bucket) | Generates bucket plots for all numerical variables in the dataset to visualize their skewness and kurtosis |
| [`data_response()`](#sec-data-response) | Plots the response variable alongside its z-score, providing a standardized version of the response for comparison |
| [`data_zscores()`](#sec-data-zscores) | Plots the z-scores for all continuous variables in the dataset, allowing for easy visualization of the standardized values (see also @sec-Outliers) |
| [`data_xyplot()`](#sec-data-xyplot) | Generates pairwise plots of the response variable against all other variables in the dataset to visualize their relationships |
| [`data_cor()`](#sec-data-cor) | Calculates and plots the pairwise correlations for all continuous variables in the dataset to assess linear relationships between them |
| [`data_void()`](#sec-data-void) | Searches for pairwise empty spaces across all continuous variables in the dataset to identify problems with interpretation or prediction |
| [`data_pcor()`](#sec-data-pcor) | Calculates and plots the pairwise patrial-correlations for all continuous variables in the dataset |
| [`data_inter()`](#sec-data-inter) | Searches for potential pairwise interactions between variables in the dataset to identify relationships or dependencies that may be useful for modelling |
| [`data_leverage()`](#sec-data-leverage) | Detects outliers in the continuous explanatory variables (x-variables) as a group to highlight unusual observations |
| [`data_Ptrans_plot()`](#sec-data-Ptrans-plot) | Plots the response variable against various power transformations of the continuous x-variables to explore potential relationships and model suitability |

: A summary table of the graphical functions {#tbl-graphical} {tbl-colwidths="\[50,50\]"}

Next we give examples of the graphical functions.

#### `data_plot()` {#sec-data-plot}

The function `data_plot` plots all the variables of the data individually; It plots the continuous variable as *histograms* with a density plots superimposed, see the plot for `rent` and `yearc`. As an alternative a `dot plots` can be requested see for an example in @sec-data_response. For integers the function plots *needle* plots, see `area` below and for categorical the function plots *bar* plots, see `location`, `bath` `kitchen` and `cheating` below.

```{r}
da |> data_plot()
```

The function could saves the `ggplot2` figures.

[back to table](#tbl-graphical)

#### `data_bucket()` {#sec-data-bucket}

The function `data_bucket` plots bucket plots (which identify skewness and kurosid) for all variables in the data set.

```{r}
#| warning: false 
da |> data_bucket()
```

[back to table](#tbl-graphical)

#### `data_response()` {#sec-data-response}

The function `data_response()` plots four different plots related to a continuous response variable. It plots; i) a histogram (and the density function); ii) a dot plot of the response in the original scale iii) a histogram (and the density function) of the response at the z-scores scale and vi) s dot-plot in the z-score scale.

The z-score scale is defined by fitting a distribution to the variable (normal or SHASH) and then taking the residuals, see also next section. The dot plots are good in identify highly skew variables and unusual observations. They display the median and inter quantile range of the data. The y-axis of a dot plot is a randomised uniform variable (therefore the plot could look slightly different each time.)

```{r}
#| warning: false
da |> data_response(, response=rent)
```

[back to table](#tbl-graphical)

#### `data_zscores()` {#sec-data-zscores}

One could fit any four parameter (GAMLSS) distribution, defined on $-\infty$ to $\infty$, to any continuous variable, where skewness and kurtosis is suspected and take the quantile residuals (or z-scores) as the transformed values x-values. The function `y_zscores()` performs just this. It takes a continuous variable and fits a continuous four parameter distribution and gives the z-scores. The fitting distribution can be specified by the user, but as default we use the SHASHo distribution. Below we demonstrate that the function y_zscores() is equivalent to fitting the SHASHo distribution to a continuous variable and then take the quqntile residuals from the fitted model as the result.

```{r}
#| warning: false
da |> data_zscores()
```

In order to see how the z-scores are calculated consider the function `y_zscores()` which is taking an individual variables z-scores;

```{r}
 z <- y_zscores(rent99$rent, plot=FALSE)
```

The function is equivalent of fitting a constant model to all the parameters of a given distribution and than taking the quantile residuals (or z=scores) as he variable of interest. The default distribution is the four parameter `SHASHo` distribution.

```{r}
library(gamlss2)
m1 <- gamlssML(rent99$rent, family=SHASHo) # fitting a 4 parameter distribution 
cbind(z,resid(m1))[1:5,]#  and taking the residuals
```

[back to table](#tbl-graphical)

#### `data_xyplot()` {#sec-data-xyplot}

The functions `data_xyplot()` plots the response variable against each of the independent explanatory variables. It plots the continuous against continuous as *scatter* plots and continuous variables against categorical as *box* plot.

::: callout-note
At the moment there is no provision for categorical response variables.
:::

```{r}
da |> data_xyplot(response=rent )
```

The output of the function saves the `ggplot2` figures.

Note hat the package `gamlss.prepdata` do not provides pairwise plot of the explanatory variables themself but the package `GGally` does. here is an example ;

```{r}
library(GGally) 
dac <- gamlss.prepdata:::data_only_continuous(da) 
ggpairs(dac, lower=list(continuous = "cor",
      combo = "box_no_facet", discrete = "count", 
      na = "na"),upper=list(continuous = "points", 
      combo = "box_no_facet", discrete = "count", na = "na")) + 
theme_bw(base_size =15)
```

[back to table](#tbl-graphical)

#### `data_cor()` {#sec-data-cor}

The function `data_corr()` is taking a `data.frame` object and plot the correlation coefficients of all its continuous variables.

```{r}
data_cor(da, lab=TRUE)
```

A different type of plot can be produce if we use;

```{r}
data_cor(da, method="circle", circle.size = 40)
```

To get the variables with higher, say, than \$0.4 \$ correlation values use;

```{r}
Tabcor <- data_cor(da, plot=FALSE)
high_val(Tabcor, val=0.4)
```

We can plot the path of those variables using the package `corr`;

```{r}
library(corrr)
network_plot(Tabcor)
```

[back to table](#tbl-graphical)

#### `data_association()`

The function `data_association()` is taking a `data.frame` object and plot the pair-wise association of all its variables. The pair-wise association for two continuous variables is given by default by the absolute value of the Spearman's correlation coefficient. For two categorical variables by the (adjusted for bias) Cramers' `v`-coefficient. For a continuous against a categorical variables by the $\sqrt R^2$ obtained by regressing the continuous variable against the categorical variable.

```{r}
data_association(da, lab=TRUE)
```

A different type of plot can be produce if we use;

```{r}
data_association(da, method="circle", circle.size = 40)
```

To get the variables with higher, say, than \$0.4 \$ association values use;

```{r}
Tabcor <- data_cor(da, plot=FALSE)
high_val(Tabcor, val=0.4)
```

A different plot can be acheived using the funtion `network_plot()` of package `corr`;

```{r}
pp=gamlss.prepdata:::data_association(rent[,-c(4,5,8)], plot=F)
library(corrr)
network_plot(pp, min_cor = 0, colors = c("red", "green"), legend = "range")

```

[back to table](#tbl-graphical)

#### `data_void()` {#sec-data-void}

::: callout-warning
This function is new. Its theoretical foundation are not proven yet. The function needs testing and therefore it should be used with causion.
:::

The idea behind the functions `void()` and its equivalent `data.frame` version `data_void()` is to be able to identify whether the data in the direction of two continuous variables say $x_i$ and $x_j$ have a lot of empty spaces. The reason is that empty spaces effect prediction since interpolation at empty spaces is dengerous. The function `data_void()` is taking a `data.frame` object and plot the percentage of empty spaces for all pair-wise continuous variables. The function used the `foreach()` function of the package **foreach** to allow parallel processing.

```{r}
registerDoParallel(cores = 9)
data_void(da)
```

A different type of plot can be produce if we use;

```{r}
data_void(da, method="circle", circle.size = 40)
stopImplicitCluster()
```

To get the variables with highter than \$0.4 \$ values use;

```{r}
Tabvoid <- data_void(da, plot=FALSE)
high_val(Tabvoid, val=0.4)
```

To plot their paths use;

```{r}
library(corrr)
network_plot(Tabvoid)
```

[back to table](#tbl-graphical)

#### `data_pcor()` {#sec-data-pcor}

The function `data_copr()` is taking a `data.frame` object and plot the partial correlation coefficients of all its continuous variables.

```{r}
data_pcor(da, lab=TRUE)
```

To get the variables with highter than \$0.4 \$ partial correlation values use;

```{r}
Tabpcor <- data_pcor(da, plot=FALSE)
high_val(Tabpcor, val=0.4)
high_val(Tabpcor, val=0.4, plot=T)
```

For plotting you can use;

```{r}
library(corrr)
network_plot(Tabpcor)
```

[back to table](#tbl-graphical)

#### `data_inter()` {#sec-data-inter}

The function `data_inter()` takes a `data.frame`, fits all pair-wise interactions of the explanatory variables against the response (using a normal model) and produce a graph displaying their significance levels. The idea behind this is to identify possible first order interactions at an early stage of the analysis.

```{r}
da |> gamlss.prepdata:::data_inter(response= rent)
```

```{r}
tinter <-gamlss.prepdata:::data_inter(da,  response= rent, plot=FALSE)
tinter
```

To get the variables with lower than \$0.05 \$ significant interactions use;

```{r}
Tabinter <- gamlss.prepdata:::data_inter(da, response= rent, plot=FALSE)
low_val(Tabinter)
low_val(Tabinter, plot=T)
```

[back to table](#tbl-graphical)

NOT DOCUMENTED IN HELP FOR THIS FUNCTION


#### `data_leverage()` {#sec-data-leverage}

The function `data_leverage()` uses the linear model methodology to identify posible unusual observations within the explanatory variables as a group (not indivisually). It fit a linear (normal) model with response, the response of the data, and all explanatory variables in the data, as x's. It then calculates the leverage points and plots them. A leverage is a number between zero and 1. Large leverage correspond to extremes in the x's.

```{r}
rent99[, -c(2,9)] |> data_leverage( response=rent)
```

::: callout-note
The horizontal line is the plot is at point $2 \times (r/n)$, which is the threshold suggested in the literature; values beyond this point could be identified as extremes. It looks that the point $2 \times (r/n)$ is too low (at least for our data). Instead the plot identify only observation which are in the one per cent upper quantile. That is a quantile value of the leverage at value 0.99.
:::

@sec-data-leverage-1 show how the information of the function `data_leverage()` can be combined with the information given by `data_outliers()` in order to confirm hight outliers in the data.

::: callout-important
What happen if multiple response are in the data?
:::

[back to table](#tbl-graphical)

#### `data_Ptrans_plot()` {#sec-data-Ptrans-plot}

The essence of this plot is to visually decide whether certain values of $\lambda$ in a power transformation are appropriate or not. @sec-Transformations describes the power transformation, $T=X^\lambda$ for $\lambda>0$, in more details. For each continuous explanatory variable $X$, the function show the response against $X$, ($\lambda=1$), the response against the square root of $X$, ($\lambda=1/2$) and the response against the log of $X$ ($\lambda \rightarrow 0$). The user then can decide whether any of those transformation are appropriate.

```{r}
gamlss.prepdata:::data_Ptrans_plot(da, rent) 
```

It look that no transformation is needed for the continuous explanatory variables `area` of `yearc`.

[back to table](#tbl-graphical)

## Feature functions  {#sec-Features}

The features functions are functions to help with the continuous explanatory x-variables; We divide them here in five sections; Functions for

-   `outliers`;

-  `correlation` between variables;

-   `scaling`;

-   `transformations` and

-   function approproate for `time series`

The available functions are;

| Functions | Usage |
|:-----------------------------------|-----------------------------------:|
| [`y_outliers()`](#sec-y-outliers) | identify possible outliers in a continuous variable (see below) |
| [`y_both_outliers()`](#sec-y-both-outliers) | identify possible outliers using both `zscores` and `quaniles` methodology|
| [`data_outliers()`](#sec-data-outliers) | identify possible outliers for all continuous x-variable (see also `data_zscores` in @sec-data-zscores) |
| [`data_leverage()`](#sec-data-leverage-1) | this is a repeat of the function from @sec-data-leverage |
| [`data_index_cor()`](#sec-data-index-cor) | this function gives an index  of continuous variables to be eliminated because of their high correlation with others continuous variables
| [`data_index_association()`](#sec-data-index-cor) | this function gives an index  of variables to be eliminated because of their high association with others continuous or factor variables 
| [`data_scale()`](#sec-data-scale) | scaling all continuous x-variables either to zero mean and one s.d. or to the range \[0,1\] |
| [`xy_Ptrans()`](#sec-xy-Ptrans) | looking for appropriate power transformation for response against one of the x-variable |
| [`data_Ptrans()`](#sec-data-Ptrans) | looking for appropriate power transformation for response against all x-variable |
| [`data_Ptrans_plot()`](#data-Ptrans-plot) | plotting all x's agains the response using identiti ty square root ans log transormations |

: The outliers, scalling, transformation and time series functions {#tbl-features} {tbl-colwidths="\[50,50\]"}

### Outliers {#sec-Outliers}

Outliers in the response variable, within a distributional regression framework like GAMLSS, are better handled by selecting an appropriate distribution for the response. For example, an outlier under the assumption of a normal distribution for the response may no longer be considered an outlier if a distribution from the t-family is assumed. The function `data_response()`, discussed in @sec-data-response provides guidelines on appropriate actions. This section focuses on outliers in the explanatory variables.

Outliers in the continuous explanatory variables can potentially affect curve fitting, whether using linear or smooth non-parametric terms. In such cases, removing outliers from the x-variables can make the model more robust. Outliers are observations that deviate significantly from the rest of the data, but the concept depends on the dimension being considered. An single observation value might be an outlier within an explanatory variable. A pair observation might be an outlier examining within a pair-wise relationships. In a one-dimensional search, we identify extreme observations in individual continuous variables, based on how far they lie from the majority of the data. In a two-dimensional search, we look for outliers in pairwise plots of continuous variables, examining how observations deviate from typical relationships between variable pairs. Leverage points are useful to identify outliers when we look for outliers in $r$ dimension when $r$ is the number of explanatory variables.

::: callout-note
At this preliminary stage of the analysis, where no model has yet been fitted, it is difficult to identify outliers among factors. This is because outliers in factors typically do not appear unusual when examining individual variables. Their outlier status usually becomes apparent only when considered in combination with other variables or factors. Identifying such outliers is a task better suited for the modelling stage of the analysis.
:::

The function `y_outliers()` in @sec-y-outliers uses two methodologies to identify outliers; The default uses simple algorithm to identify potential one-dimensional outliers in continuous explanatory variables. It is designed specifically for continuous variables.

-   For variables defined on the full real line, (i.e., from $-\infty$ to $+\infty$, the algorithm fits a parametric distribution—by default, the four-parameter `SHASHo` distribution—and computes z-scores (quantile residuals) to assess outlier status.

-   For variables defined on the positive real line (i.e., from $0$ to $+ \infty$), an additional transformation step is included. These variables are often highly right-skewed, and extreme values in the right tail can dominate any outlier detection process. To address this, the function attempts to find a power transformation $T = X^\lambda$ that minimizes the Jarque-Bera test statistic—a measure of deviation from normality. To find the optimal power parameters $\lambda$ the function `y_outliers()` uses the internal function `x_Ptrans()`. After this transformation, the `SHASHo` distribution is fitted, and z-scores are computed. The Jarque-Bera test statistic is always non-negative; the further it is from zero, the stronger the evidence against normality. The goal is to reduce skewness and kurtosis in the data before assessing outliers.

Outliers are identify for observations with very low or hight z-scores. For example, if a z-scores is greater in absolute value to a specified value say 3. The methodology is appropriate for identifying outliers in one dimension. Pair-wise scatter plots of all continuous variables in the data can be use to identify outliers in two dimensions. To identify outliers in $p$ dimensions the function `data_leverage()`, see @sec-data-leverage-1, can be used.

::: callout-note
Several of the graphical functions described in @sec-Graphical-functions are good for identify visualy outliers. In fact we recommend that the function `data_outliers()` should be used in combination with functions like `data_plot()` or `data_zscores()`.
:::

#### `y_outliers()` {#sec-y-outliers}

The function `y_outliers()` identify outliers in one $X$ variable. There are two methodologies to do that;

-  `zscores`: A  distribution is fitted to the data (usually `SHASHo`) then the residuals (z-scores) are taken and observations with large residuals are identified (see also the above comment).

- `quantile`: This is the classical methodology and it is based on sample quantiles.  An observation is identified if it far from the median. How far usually is taken  as three time the semi-inter-quantiles range. The option `value` is set to 4 as a default in the function. 

Using the zscores;
```{r}
y_outliers(da$rent)
```
or  using the `quantile` 

```{r}
y_outliers(da$rent, type="quantile", value=4)
```

Note that the `y_outliers()` is used by `data_outliers()` to identify outliers in all continuous variables in the data.

::: callout-note
For very skew data the `value` in the option `type="quantile"` need to be increased.  
:::


[back to table](#tbl-features)


#### `y_both_outliers()` {#sec-y-both-outliers}

The function `y_both_outliers()` uses both `zscores` and `quantiles` and reports the obsrarvations appearing in both sets. Note that the defaule `value` is 4 for both cases.

```{r}
y_both_outliers(da$rent)
```

[back to table](#tbl-features)

#### `data_outliers()` {#sec-data-outliers}

The function `data_outliers()` uses the *z-scores* technique described above in order to detect outliers in the continuous variables in the data. It fits a `SHASHo` distribution to the continuous variable in the data and uses the z-scores (quantile residuals) to identify (one dimension) outliers for those continuous variables.

```{r}
data_outliers(da)
```

As it happen no individual variable outliers were highlighted in the `rent` data using the z-scores methodology. In general we recommend the function `data_outliers()` to be used in combination of the graphical functions `data_plot()` and `data_zscores()`

[back to table](#tbl-features)

#### `data_leverage()` (repeat) {#sec-data-leverage-1}

The function `data_leverage()`, fisrt describe in @sec-data-leverage, can be used to detect extremes in the continuous variables in the data by fiting a linear model with all the continuous variables in the data and then useing the higher leverage points to identify (multi-dimension) outliers for the observations for all continuous variables. By default, it identifies one percent of the observations with high leverage in the data. Here we use the function as a way to identify the obsrvation (not plottoing).

```{r}
data_leverage(da, response=rent, plot=FALSE)
```

::: callout-note
The function`data_leverage()` always identifies the top one percent of observations with the highest leverage, which may or may not be outliers. These high-leverage points can have a strong influence on model estimates. To better assess their impact, the list of high-leverage observations returned by `data_leverage()` should be compared with the list of outliers identified by the `data_outliers()` function. Observations that appear in both lists are particularly noteworthy, as they may be influential outliers that warrant further investigation.
:::

Next we are trying the process of identifying if data with high leverage are also coincide with outliers identified using the function `data_outliers()` function. The function `intersect()` will flash out the intersection of the two set of observations. First use `data_outliers()`

```{r}
ul <- unlist(data_outliers(da))
```

then `data_leverage()`;

```{r}
ll<-data_leverage(da, response=rent, plot=FALSE)
```

and finally we inspect the intersection of the two sets using `intersect()`;

```{r}
intersect(ll,ul)
```

Here we have zero outliers but in general we would expect to find common observations for further checking.

[back to table](#tbl-features)


### Correlations {#sec-correlations}

The idea here is to identify variables with high correlation or association with the possibility to eliminated them from the data. The first function is appropriate for continuous variables while the second for both continuous and factors.

#### `data_index_cor()` {#sec-data-index-cor}

The function  `data_index_cor()`, It takes a `data.frame` as input calculates correlation coefficients  of the continuous variables in the data and creates an index for variables which could be possible to eliminate from the further analysis.      

```{r}
gamlss.prepdata:::data_index_cor(rent99[,-1])
```

[back to table](#tbl-features)

#### `data_index_association()` {#sec-data-index-association}

The function  `data_index_association()`, It takes a `data.frame` as input calculates the association coefficients  of all the variables in the data and creates an index for variables which could be possible to eliminate from the further analysis.      

```{r}
gamlss.prepdata:::data_index_association(rent99[,-1])
```

[back to table](#tbl-features)


### Scaling {#sec-Scaling}

Scaling is another form of transformation for the continuous explanatory variables in the data which brings the explanatory variables in the same scale. It is a form of **standardization**. Standardization means bringing all continuous explanatory variables to similar range of values. For some machine learning techniques, i.e., principal component regression or neural networks standardization is mandatory in other like LASSO is recommended. There are two types of standardisation

i)  *scaling to normality* and

ii) *scaling to a range from zero to one*.

Scaling to normality it means that the variable should have a mean zero and a standard deviation one. Note that, this is equivalent of fitting a Normal distribution to the variables and then taking the z-scores (residuals) of the fit as the transformed variable. The problem with this type of scaling is that skewness and kurtosis in the standardised data persist. One could go further and fit a four parameter distribution instead of a normal where the two extra parameters could account for skewness and kurtosis. The function `data_scale()` described in @sec-data-scale it gives this option with te argument `family` in which can used to specify a different distribution.

#### `data_scale()` {#sec-data-scale}

The function `data_scale()` perform scaling to all continuous variables an a data set. Note that factors in the data set are left untouched because when fitted within a model they will be transform to dummy variable which take values 0 or 1 (a kind of standardization). The response is left also untouched because it is assumed that an appropriate distribution will be fitted. The response variable has to be declared using the argument `response.` First we use `data_scale()` to scale to normality.

```{r}
head(data_scale(da, response=rent))
```

As we mention before scaling to normality is equivalent of fitting a Normal variable first. The problem with scaling to normality is that if the variables are highly skew or kurtotic scaling them to normality does correct for skewness or kurtosis. Using a parametric distribution with four parameters some of which are skewness and kurtosis parameters it may correct that. Next we standardised using the `SHASHo` distribution;

```{r}
head(data_scale(da, response=rent,  family="SHASHo"))
```

The second form od standardisation is to transform all continuous x-variables to a range zero to one. Here is how this is done;

```{r}
head(data_scale(da, response=rent,  scale.to="0to1"))
```

[back to table](#tbl-features)

### Transformations {#sec-Transformations}

In the context of outliers, the focus on how far a single observation deviates from the rest of the data. However, when considering **transformations**, the goal shifts toward a better capturing of the relationship between a continuous predictor and the response variable. Specifically, we aim to model **peaks and troughs** in the relationship between a single x-variable and the response better. To achieve this, it is sometimes sufficient to stretch the x-axis so that the predictor variable is more evenly distributed within its its range. A common approach for this is to use a power transformation of the form $T = X^\lambda$. This transoformation can reduce the curvature in the response and make the curve fitting easier. Importantly, the power transformation smoothly transitions into a logarithmic transformation as $\lambda \to 0$, since: $\frac{X^{\lambda} - 1} {\lambda} \rightarrow \log(X) \quad \text{as } \lambda \to 0$. Thus, in the limit as $\lambda \to 0$ is the power transformation is the log transformation $T = \log(X)$.

<!-- In the context of outliers, we focus on how far an observation is from the rest of the data. However, in the case of transformations, we are more concerned with how to best capture the relationship between a continuous variable in the data and the response. More precisely, we are interest to be able to capture  better peaks and troughs in the relation between one single x and the response.  To do that, sometime it is sufficient to stretch the x-axis in such a way the x-variable is more evenly distributed within its range. A power transformation, $T=X^\lambda$ on the x-variable can achieve this. Often the pawer  transformation makes the  hight curvative in the response to disappear.  Now since $(X^{\lambda} - 1) / \lambda \rightarrow \log X$ as $\lambda \rightarrow 0$ one can assume at the limit of the transformation while lambda goes to zero   the log transformation  $T = \log(X).$ -->

::: callout-note
The power transformation is a subclass of the shifted power transformation\
$T = (\alpha + X)^\lambda$ which is not consider here
:::

The aim of the power transformation is to make the model fitting process more robust and reliable and maximizing the information extracted from the data. There are three different functions in `gamlss.prepdata` dealling with power transformations, `xy_Ptrans()`, `data_Ptrans()` and `data_Ptrans_plot()`

::: callout-note
It’s important to distinguish between the concepts of *transformations* (the subject of this section) and of *feature extraction*, a term often used in machine learning. We refer to transformations as functions applied to a individual explanatory variables while we use feature extraction when multiple explanatory variables are involved. In both cases, the goal is to enhance the model capabilities.
:::

#### `xy_Ptrans()` {#sec-xy-Ptrans}

```{r}
pp<-gamlss.prepdata:::xy_Ptrans(da$area, da$rent) 
pp
```

```{r}
gamlss.prepdata:::xy_Ptrans(da$area, da$rent, prof=TRUE, k=4) 
```

#### `data_Ptrans()` {#sec-data-Ptrans}

```{r}
gamlss.prepdata:::data_Ptrans(da, response=rent)
```

[back to table](#tbl-features)

#### `data_Ptrans_plot()` (repeat) {#sec-data-Ptrans-1}

```{r}
gamlss.prepdata:::data_Ptrans_plot(da, rent) 
```

[back to table](#tbl-features)

### Time series {#sec-Timeseries}

Time series data sets consist of observations collected at consistent time intervals—e.g., daily, weekly, or monthly. These data require special treatment because observations that are closer together in time tend to be more similar, violating the usual assumption of independence between observations. A similar issue arises in spatial data sets, where observations that are geographically closer often exhibit spatial correlation, again violating independence.

Many data sets include spatial or temporal features —such as longitude and latitude or timestamps (e.g., 12/10/2012)— those variables are can be used for for modeling directly, but also for interpreting or visualization of the results. If temporal or spatial information exists in the data, it is essential to ensure these features are correctly read and interpreted in `R`. For instance, dates in `R` can be handled using the `as.Date()` function. To understand its functionality and options associated with the function please consult the help documentation via `help("as.Date")`. Below, we provide two simple functions of how to work with date-time variables.

#### `time_dt2dhour()` {#sec-time-dt2dhour}

Suppose the variable `dt` contains both a date and a time. We may want to extract and separate this into two components: one containing the date, and another capturing the hour of the day.

```{r}
dt <- c("01/01/2011 00:00", "01/01/2011 01:00", "01/01/2011 02:00", "01/01/2011 03:00", "01/01/2011 04:00", "01/01/2011 05:00") 
```

The function to do this separation can lok like;

```{r}
time_dt2dhour <- function(datetime, format=NULL) 
  { 
          X <- t(as.data.frame(strsplit(datetime,' '))) 
rownames(X) <- NULL 
colnames(X) <- c("date", "time") 
       hour <- as.numeric(sub(":",".",X[,2])) 
      date <- as.Date(X[,1],format=format) 
      data.frame(date, hour) 
  } 
```

```{r}
P <- time_dt2dhour(dt, "%d/%m/%Y") 
P
```

#### `time_2num()` {#sec-time-2num}

The second example contains time given in its common format, `c("12:35", "10:50")` and we would like t change it to numeric which we can use in the model.

```{r}
t <- c("12:35", "10:50")
```

The function to change this to numeric;

```{r}
time_2num <- function(time, pattern=":") 
  { 
  t <- gsub(pattern, ".", time) 
   as.numeric(t) 
  } 
time_2num(t) 
```

For user who deal with a lot of time series try the function `POSXct()` and also the function `select()` from the `dplur` package.

## Data Partition {#sec-Partition}

###  Introduction to data partition {#sec-Intro-Partition}

Partitioning the data is essential for comparing different models and checking for overfitting. @fig-data_split illustrates the different ways the data can be split. A dataset can be split multiple times or just once, depending on the size of the dataset. For large datasets, it is common to split the data into a training set and a test set, and, if necessary, a validation set. Observations in the training set are referred to as `in-bag`, while observations in the test or validation set are termed `out-of-bag`.

The training set is used for fitting the model, the test set is used for prediction and comparison between models, and the validation set is used for tuning the model’s `hyperparameters`. In additive smoothing regression models, for instance, the hyperparameters refer to the `smoothing` parameters. Data partitioning allows for performance evaluation, model assumption checks, and detection of overfitting.

If the fitted model performs reasonably well on both the training and test data sets, one can be confident that overfitting has been avoided. Overfitting occurs when a model fails to generalize well to test data, typically because it is too closely fitted to the training data. In contrast, underfitting refers to a poor model that fails to capture the essential patterns in both training and test data, resulting to a model that is far from both the sample and the population intended to represent.

<!-- Partitioning the data is useful for comparing different models and check for overfitting. @fig-data_split shows different ways the data can be split. A data set can be either split multiple times or split once. For big data sets often the sample data is split once into `training`, and `test` data sets and if needed into a `validation` set. Observations in the the training set are often called **in-bag** and observations in the test set (or validation set) called **out-of-bag**. The training data set is used for fitting the model, test data for prediction and possible comparing the different models and validation for tuning hyper parameters of the model. Note that in additive smoothing regression models the hyper parameters are the smoothing parameters in the model. Splitting the data allows checking the performance, the suitability of the assumptions of the model and for overfitting. If the fitted model, in the training, data set perform reasonable also in the `test` data set then one could feel that **overfitting** has been avoided. Overfitting occurs when the model do not generalised well in new data. This often means that the model is too close to the sample data. By **underfitting**, we mean a bad model, a model which is unable to capture the most important feature in the data. A model which is far from both the sample but also from the population which suppose to represent. -->

Various goodness-of-fit measures can be used to check and compare different distributional regression models. One of the most popular methods when there is  no portioning of data is the Generalized Akaike Information Criterion (GAIC). To calculate the GAIC, only the in-bag data are required. However, calculating the GAIC requires an accurate measure of the degrees of freedom used to fit the model. For mathematical (stochastic) models, the degrees of freedom are straight forward to define as they correspond  the number of estimated parameters. In contrast, for over-parametrised algorithmic models  like neural networks, estimating he degrees of freedom is  challenging. Note that the degrees of freedom serve as a measure of the model’s complexity. We expect more complex models to be closer to the training data, which may reduce their ability to generalize well to new data (overfitting).

::: callout-note
Note that measure of goodness-of-fit used for the classical regression models  when data partition is involves are mostly not appropriate for distributional regression model like  GAMLSS. This  because those measures concentrate in the behaviour at the centre of the distribution of the response which may or not be of interest to a practitioner.
:::

The measures of goodness-of-fit used in classical regression models when partinioning of the data is involve are;

- *RMSE*: root mean squared error; 

- *MAE*: mean absolute error; 

- *$R^2$*: and 

- *MAPE*:  mean absolute percentage error.

All of the above measures of goodness-of-fit are NOT generalize well when the distribution of the response is not symmetric (like the Normal distribution), and  also if the focus of the study is not the location parameter (mean or median) of the response distribution. If the focus of the study is on the tail of the distribution (how often a threshold is bridged) those measure are not appropriate. The following  two measures are appropriate for  distributional regression model and can be use for both into bag and out of bag data sets;

- $\ell=\log L$; the log-likelihood, or equivalent the *deviance* $d=-2 \ell$  

- *CRPS*; the continuous rank probability scores.


<!-- There are different goodness of fit for checking and comparing different distributional regression models. The generalised Akaike criterion, **GAIC** is a very popular method. To calculate the GAIC only the *in-bag* data are needed there is no need for data partition. Calculating the GAIC needs though an accurate measure of the *degrees of freedom* used to fit the model. In mathematical (stochatic) models the degrees of freedom are easy to define because they are the number of estimated parameters. In algorithmic based models, especially the ones overparametrised like neural networks, the degrees of freedom are more difficult to guess. The degrees of freedoms (at least for mathematical models) are a measure of the *complexity* of the model. More complex models are usually closer to the training data, therefore maybe do not generalised well. Apart from the `GAIC` other goodness of fit measures, used in the literature for regression models are; the deviance; the $R^2$; the mean square errors, MSE, e.t.c. All of them are prone to overfitting if they are evaluated in the *in-bag* data set rather than in the *out-of-bag*. One could also argue that $R^2$ is not generalised well to non-normal errors and that MSE and its variations inappropriate for distributional regression models because their focus is in the location parameters of the response distribution ans not in other characteristics of the distribution of the response. Often other part of the distribution are of interest to the practioner. -->

```{mermaid}
%%| label: fig-data_split
%%| fig-cap: Different ways of splitting data to get in order to get more information"
flowchart TB
  A[Data] --> B{Multi-split} 
  A --> C{Split once}
  C --> D[Training]
  D --> E[Validate]
  E --> O[Test]
  B --> F(Rearranged)
  B --> G(Reweighed)
  G--> H(Bayesian \n Bootstrap)
  G--> J(Boosting)
  F --> K(Non-param. \n Bootstrap)
  F --> L(Cross \n Validation)
  L --> M(k-Fold) 
  L --> N(LOO) 
```

Partitioning the data into training, test, and validation sets should be done randomly. A potential risk when splitting the data once arises when the investigation focuses on the tails of the response distribution. In such cases, the few extreme observations may end up in only one of the partitions, leading to potential issues when evaluating the model. Repeatedly reusing parts of the data, such as through bootstrapping or cross-validation, can help mitigate this risk. Multi-splitting the data (shown on the left side of Figure @fig-data_split) enables the evaluation of appropriate goodness-of-fit measures using out-of-bag data. There are two conceptual ways to multi-split a dataset: the first involves rearranging the data, and the second involves reweighting the observations. \[As we will see later, the distinction between these two approaches is not always as clear-cut.\] Rearranging the data is typically accomplished by creating appropriate indices that select different parts of the data. For example data in this case are partitioned as `data[,index]`. This leads to methods such as **bootstrapping** or **cross-validation**. On the other hand, reweighting the observations during the fitting process is a technique employed in the **Bayesian Bootstrap** and **boosting**.

<!-- Partitioning the data into training, test and validation has to be done randomly. An inherent risk for splitting the data once comes when the interest of the investigation lies on the tail of the distribution of the response. In this, case the few extreme observations, could fall into only one of the partitioning sets creating problems when the model is evaluated. Repetitively reusing (parts) of the data, for example, bootstrapping or cross validation could avoid that. Multi-split of the data (left size of @fig-data_split) allow the evaluation of appropriate measures of goodness of fit in out of bag data. There are conceptuality two different ways of multi-split the data set; the first is by rearrange them and the second is by reweighed. \[We will see at a later stage that this difference is not so clear cut\]. Rearranging the data usually is done by creating appropriate indexes which pick different part of the data. This leads to **bootstrapping** or **cross validation**. Reweighed the observations, during the fit, is done in the **Bayesian Bootstrap** or in **boosting**. -->

In classical bootstrapping, the data are re-sampled randomly with replacement $B$ times, and the model is refitted the same number of times. While computationally expensive, this process provides additional insights into the model and in particular regarding the variability of all its parameters. For distributional regression models, both classical and Bayesian bootstrapping offer extra information about all the fitted parameters of the model which includes the distributional parameters, the fitted coefficients, the  hyperparameters, and the random effects. The Bayesian bootstrap re-weights the data as samples from a multinomial distribution and fits the re-weighted models $B$ times. Like the classical bootstrap, it provides valuable information about the variability of all the model parameters. In boosting, each weak learner (i.e., each simple fitted model) is reweighted using the residuals from the previous fits. The results of all these learners are then aggregated into a final model. Boosting can be seen as a method of iteratively improving model accuracy by focusing on the previously mis-fitted observations.

<!-- In classical *bootstrapping* the data are re-sampled randomly with replacement $B$ times and the model is refitted similar times. The process, while computational expensive, could provides extra information about the model ans especially about the variability of its parameters. For distributional regression models, both *classical* and *Bayesian* bootstrap, provides extra information for all the parameters involved in the model, i.e. the distributional parameters, the fitted coefficients, the hyper parameters and als for the random effects. Bayesian bootstrap re-weights the data as samples from a a multinomial distribution and fits the re-weighted models $B$ times and provides the extra information about the variability of the parameters, the same way like the classical bootstrap does. In *boosting* each week learner, that is, each simple fitted model, is reweighed using as weights the residuals of the previous fits, then all learners are aggregated together for the final model. -->

<!-- Bootstrapping helps to evaluate how far the sample is from the population by giving an estimate of the variability between the different sub-samples. It also helps to calculate standard errors from a fitted model when there is no an easy way to do so with the conventional methods. It helps us to evaluate the variability of all of the parameters of a fitted GAMLSS by providing standard errors not only for the coefficient of the model but also for the random effects and hyper parameters, see @sec-ParametersforEstimation for the definition of different parameters.  -->

In $K$-fold cross-validation, the data are split into $K$ sub-samples. The model is then fitted $K$ times, each time using $K-1$ sub-samples for training and the remaining one sub-sample as test data set. By the end of the process, all $n$ observations will have been used as test data exactly once. The main advantage of $K$-fold cross-validation is that it provides reliable test data for model evaluation, with the cost of fitting $K$ models. Leave-One-Out Cross-Validation (LOO) is a special case of K-fold cross-validation, where the model is trained on $n-1$ observations and tested on the remaining single observation, iterating this process $n$ times. Consequently, $n$ different models are refitted. While computationally intensive, LOO ensures that each data point is used for testing, providing an unbiased evaluation of model performance. Both bootstrapping and $K$-fold cross-validation, when appropriate performance metrics are used for the specific problem, can help in several ways: i) by facilitating model comparison , ii) by avoiding overfitting, and iii) by assisting in the selection of hyper-parameters.

Note that the  partition of the data in any distributional regression model, can be achieved different ways;

1. by *splitting* the original data frame in different subsets;  

2. by using a *factor* which classifies the partitions;

3. by *indexing* the original data i.e. `data[index,]`,

4. by using *prior weights* in the fitting.

<!-- A $k$-fold *cross validation* splits the data in $k$ sub-samples. The model is then fitted repeatedly $k$ times, each time the $k-1$-fold sets are used for training and the remaining one-fold as test data. By the time that all the $k$ models are fitted we would have $n$ observations fitted as test data fits. So the K-fold cross-validation allows us to get `test` data with the additinal cost of having to fit $K$ models. Leave one out, **LOO**, is an extreme version of the K-fold partition in which we pick $n-1$ observations for training and leave one out for test. In this case we have to refit $n$ different models. Both bootstrapping and $k$-fold cross validation when appropriate measures are used can help i) with model comparison (on test data) ii) by avoiding overfitting and iii) possible selecting the hyper parameters. -->

So there is no inherent need for physically partition into subgroups when performing a single or multiple partitions  since  the same results can be achieved with different methods which could be more appropriate for the data in hand.  



For example for prior weights in $K$-fold cross-validation, we need $K$ dummy vectors   $i_k$, where \$k = 1, \ldots, K. These dummy vectors take the value of 1 for the $k$-th set of observations and $0$ for the rest. The dummy vectors $i_k$ can be used in two ways:

1.  To select the observations from the `data.frame` for fitting.

2.  As prior weights when fitting the model.

Let `da` represent the `data.frame` used to fit the model, and $i$ the $i_k$ a dummy index. vector. For each of the $K$ fits, the model fitting process would use the command $data=da[,i]$ to select the appropriate data, and `newdata=da[,i==0]` for evaluating the out-of-bag measure. Alternatively, one can use `weights=i` during the model fitting and `newdata=da[,i==0]` for evaluating any out-of-bag measures. Note that the appropriate measures for comparing models include predictive deviance (PD) and continuous rank probability scores (CRPS). The function data_Kfold() in the package generates the correct dummy vectors as a matrix of dimensions n \times B, which can be used for the cross-validation process.

<!-- Notice that performing a single or multiple partitions in any distributional regression model there is no actually need to split the original data in subclass. The same results can be achieved either by indexing the original `data.frame` or by using prior weights to fit the different models. For example, in the $K$-th fold cross validation, we need $K$ dummy vectors, $i_k$, for $k=1,\ldots,K$, with value $1$ for the $k-1$ samples and $0$ for the rest. The dummy vectors $i_k$ can be used either to select observations from the `data.frame` or as prior weights in the fit. Let us denote `da` the `data.frame` used to fit the model and, `i`, the $i_k$ dummy vector. Then each of the $K$ fits it should have the options `data=da[,i]` when fitting the model and `newdata=da[,i==0]` for evaluation of of the out-of-bag measure. Equivalently, one could use `weights=i` for fitting using prior weights and `newdata=da[,i==0]` for any measure needed to be evaluated at the out-of-bag data. Note that possible measure to use for comparing models are the predictive deviance, **PD** and continuous rank probability scores, **CRPS**. The function in the package which can generate the right dummy vectors as a matrix of dimensions $n \times B$ is `data_Kfold()`, see -->

For the classical bootstrap, we need $B$ vectors. Unlike cross-validation (CV), where the vectors for indexing and prior weights are identical, bootstrapping requires different vectors. The function `data_boot_index()` creates the appropriate vectors for indexing, while `data_boot_weights()` generates the vectors for prior weighting. These vectors should differ whether used as indices to select the relevant columns from the data matrix `da` or as prior weights, which indicate how many times each observation is selected. Typically, a vector like $(1, 0, 2, \ldots, 3, 2)$ means that observation $1$ is picked once, observation $2$ is excluded from the fit, observation $3$ is selected twice, and so on. For Bayesian bootstrap, the $B$ vectors of length $n$ represent weights that sum to $n$. An out-of-bag observation, in this case, is one with zero weight. This distinction makes the split between rearranged and reweighed in @fig-data_split somewhat artificial, as both methods can be interpreted as refitting approaches using prior weights.

<!-- For the classical bootstrap, we also need $B$ vectors. Contrary to CV where the vectors for indexing and prior weights are identical, bootstrapping needs different vectors. The function `data_boot_index()` creates appropriate vectors for indexing while `data_boot_weihghts()` for prior weighing. they should be different whether we used them as indices to select the right columls form the data matrix `da` or as prior weights as either for of length $n$ indicating which observations to pick and how many times. Typically a vectors say $(1,0,2,\ldots,3, 2)$, indicates that observation $1$ is picked once, observation $2$ is excluded from the fit, observation $3$ is picked up 2 times e.t.c. For Bayesian bootstrap the $B$ vectors of length $N$ are weights summing up to $n$. A out-of-bag observation in this case can be defined as an observation with zero weights. This note alsmost make the split between **rearranged** and **reweighed** in @fig-data_split rather artificial since both can be thought as refitting methods using prior weights. -->

<!-- -partioning the data loke Cross Validation, **CV**,all need data partition and sould be el -->

<!-- For bootstrapping the prior weights will be $0$ for observations that were not selected. $1$ for observations that were selected once in the re-sampling, $2$ for observations selected twice, e.t.c. The vector of prior weights in bootstrapping should sum up to $n$, the number of observations.  In Bayesian bootstrapping, the weights, samples from a multinomial distribution, should be sum up to $n$. -->

| Functions | Usage |
|:-----------------------------------|-----------------------------------:|
| [`data_part()`](#sec-data-part) | Creates a single or multiple (CV) partitions by introducing a factor with different levels |
| [`data_part_list()`](#sec-data-part-list) | Creates a single or multiple (CV) partitions with output a list of `data.frame`s |
| [`data_boot_index()`](#sec-data_boot-index) | Creates two lists. The in-bag,`IB`, indices for fitting and the out-of-bag, `OOB`, indices for prediction |
| [`data_boot_weights()`](#sec-data_boot-weights) | Create a $n \times B$ matrix with columns weights for bootstrap fits |
| [`data_Kfold_index()`](#sec-Kfold-index) | Creates a $n \times K$ matrix of variables to be used for cross validation data indexing |
| [`data_Kfold_weights()`](#sec-Kfold-weights) | Creates a $n \times K$ matrix of dummy variables to be used for cross validation weighted fits |
| [`data_cut()`](#sec-data-cut) | This is not a partition funtion but randomly select a specified porpotion of the data |

: The data partition functions {#tbl-partition} {tbl-colwidths="\[50,50\]"}

Here are the data partition functions.

#### `data_part()` {#sec-data-part}

The function `data_part()` it does not partitioning the data as such but adds a factor called `partition` to the data indicating the partition. By default the data are partitioned into two sets the training ,`train`, and the test data, `test`.

```{r}
dap <- gamlss.prepdata:::data_part(da)
head(dap)
```

If the user would like to split the data in two separate `data.frame`'s she/he can use;

```{r}
daTrain <- subset(dap, partition=="train")
daTest <- subset(dap, partition=="test")
dim(daTrain)
dim(daTest)
```

Note, that use of the option `partition` has the following behaviour;

i)  `partition=2L` (the default) the factor has two levels `train`, and `test`.

ii) `partition=3L` the factor has three levels `train`, `val` (for validation) and `test`.

iii) `partition > 4L` say `K` then the levels are `1`, `2`...`K`. The factor then can be used to identify K-fold cross validation, see also the function `data_Kfols_CV()`.

[back to table](#tbl-partition)

#### `data_part_list()` {#sec-data-part-list}

The function `data_part_list()` creates a list of `dataframe`s which can be used either for single partition or for cross validation. The argument `partition` allows the splitting of the data to up to 20 subsets. The default is a list of 2 with elements `training` and `test` (single partition).

Here is a single partition in `train` and `test` data sets.

```{r}
allda <-  data_part_list(rent) 
length(allda)
dim(allda[["train"]]) # training data
dim(allda[["test"]]) # test data
```

Here is a multiple partition for cross validation.

```{r}
allda <-  data_part_list(rent, partition=10) 
length(allda)
names(allda)
```

[back to table](#tbl-partition)

#### `data_boot_index()` {#sec-data_boot-index}

The function `data_boot_index()` create two lists. The first called `IB` (for in-bag) and the second `OOB`, (out-of-bag). Each ellement of the list `IB` can be use to select a bootstrap sample from the original `data.frame` while each element of `OOB` can be use to select the out of sample data points. Note that each bootstap sample approximately contains 2/3 of the original data so we expact on avarage to 1/3 to be in `OOB` sample.

```{r}
DD <- data_boot_index(rent, B=10)
```

The class, length and names of the created lists;

```{r}
class(DD)
length(DD)
names(DD)
```

the first obsevations of the

```{r}
head(DD$IB[[1]]) # in bag
head(DD$OOB[[1]]) # out-of-bag
```

[back to table](#tbl-partition)

#### `data_boot_weights()` {#sec-data_boot-weights}

The function `data_boot_weights()` create a $n \times B$. matrix with columns possible weights for bootstap fits. Note that each bootstap sample approximately contains .666 of the original obsbation so we in general we expect 0.333 of the data to have zero weights.

```{r}
MM <- data_boot_weights(rent, B=10)
```

The `M` is a matrix which columns can be used as weights in bootstrap fits; Here is a plot of the first column

```{r}
plot(MM[,1])
```

and here is the first 6 rows os the matrix;

```{r}
head(MM)
```

[back to table](#tbl-partition)

#### `data_Kfold_index()` {#sec-Kfold-index}

The function `data_Kfold()` creates a matrix which columns can be use for cross validation either as indices to select data for fitting or as prior weights.

```{r}
PP <- data_Kfold_index(rent, K=10)
head(PP)
```

[back to table](#tbl-partition)

#### `data_Kfold_weights()` {#sec-Kfold-weights}

The function `data_Kfold()` creates a matrix which columns can be use for cross validation either as indices to select data for fitting or as prior weights.

```{r}
PP <- data_Kfold_weights(rent, K=10)
head(PP)
```

[back to table](#tbl-partition)

#### `data_cut()` {#sec-data-cut}

The function `data_cut()` is not included in the partition Section for its data partitioning properties. It is designed to select a random subset of the data specifically for plotting purposes. This is especially useful when working with large datasets, where plotting routines—such as those from the `ggplot2` package—can become very slow.

The `data_cut()` function can either: • Automatically reduce the data size based on the total number of observations, or • Subset the data according to a user-specified percentage.

Here is an example of the function assuming that that the user requires $50%$ of the data;

```{r}
da1 <- data_cut(rent99, percentage=.5)
dim(rent99)
dim(da1)
```

The function `data_cut()` is used extensively in many plotting routines within the `gamlss.ggplots` package. When the percentage option is not specified, a default rule is applied to determine the proportion of the data used for plotting. This approach balances performance and visualization quality when working with large datasets.

Let $n$ denote the total number of observations. Then:

-   if $n \le 50,000$: All data are used for plotting.
-   if $50,000 < n \le 100,000$: 50% of the data are used.
-   If $100,000 < n \le 1,000,000$: 20% of the data are used.
-   If $n>1,000,000$ : 10% of the data are used.

This default behaviour ensures that plotting remains efficient even with very large datasets.

[back to table](#tbl-partition)

## Distribution Families {#sec-Family}

::: callout-note
All functions listed below belong to the `gamlss.ggplots` package, not the `gamlss.prepdata` package. However, since they can be useful during the pre-modeling stage, they are presented here. The later version of the `gamlss.ggplots` can be found in <https://github.com/gamlss-dev/gamlss.ggplots>
:::

In the following function, no fitted model is required—only values for the parameters need to be specified.

#### `family_pdf()`

The function `family_pdf()` plots individual probability density functions (PDFs) from distributions in the `gamlss.family` package. Although it typically requires the family argument, it defaults to the Normal distribution (NO) if none is provided.

```{r}
#| fig-height: 5
#| label: fig-fam_pdf_continuous
#| fig.cap: "Continuous response example: plotting the pdf of a normal random variable."
#| warning: false
family_pdf(from=-5,to=5, mu=0, sigma=c(.5,1,2))
```

The following demonstrates how discrete distributions are displayed. We begin with a distribution that can take infinitely many count values—the Negative Binomial distribution;

```{r}
#| fig-height: 5
#| label: fig-fam_pdf_discrete
#| fig.cap: "Count response example: plotting the pdf of a beta binomial."
#| warning: false
family_pdf(NBI, to=15, mu=1, sigma=c(.5,1,2), alpha=.9, size.seqment = 3)
```

Here we use a discrete distribution with a finite range of possible values: the Beta-Binomial distribution.

```{r}
#| fig-height: 5
#| label: fig-fam_pdf_binomial
#| fig.cap: "Beta binomial response example: plotting the pdf of a beta binomial."
#| warning: false
family_pdf(BB, to=15, mu=.5, sigma=c(.5,1,2),  alpha=.9, , size.seqment = 3)
```

#### `family_cdf`

\subsection{The function \texttt{family\_cdf()}}

The function `family_cdf()` plots cumulative distribution functions (CDFs) from the **gamlss.family** distributions. The primary argument required is the family specifying the distribution to be used.

```{r}
#| fig-height: 5
#| label: fig-fam_cdf_discrete
#| fig.cap: "Count response example: plotting the cdf of a negative binomial."
#| warning: false
family_cdf(NBI, to=15, mu=1, sigma=c(.5,1,2), alpha=.9, size.seqment = 3)
```

The CDf of the negative binomial;

```{r}
#| fig-height: 5
#| label: fig-fam_cdf_binomial
#| fig.cap: "Count response example: plotting the cdf of a beta binomial."
#| warning: false
family_cdf(BB, to=15, mu=.5, sigma=c(.5,1,2),  alpha=.9, , size.seqment = 3)
```

#### `family_cor()`

The function `family_cor()` offers a basic method for examining the inter-correlation among the parameters of any distribution from the **gamlss.family**. It performs the following steps:

-   Generates 10,000 random values from the specified distribution.
-   Fits the same distribution to the generated data.
-   Extracts and plots the correlation coefficients of the distributional parameters.

These correlation coefficients are derived from the variance-covariance matrix of the fitted model.

::: callout-warning
This method provides only a rough indication of how the parameters are correlated at specific values of the distribution’s parameters. The correlation structure may vary significantly at different points in the parameter space, as the distribution can behave quite differently depending on those values.
:::

```{r}
#| fig-height: 5
#| label: fig-fam_correlation
#| fig.cap: "Family correlation of a BCTo distribution at specified values of the parameters."
#| warning: false
#source("~/Dropbox/github/gamlss-ggplots/R/family_cor.R")
gamlss.ggplots:::family_cor("BCTo", mu=1, sigma=0.11, nu=1, tau=5, no.sim=10000)
```

<!-- \subsection{Time Series and spatial feature} -->

<!-- \label{Time Series and spatial feature} -->

<!-- ``` -->
