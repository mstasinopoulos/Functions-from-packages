[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Extra material for GAMLSS packages",
    "section": "",
    "text": "This website contains information about the functions in packages gamlss.prepdata and gamlss.ggplots\n\ngamlss.prepdata\ngamlss.ggplots"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About extra materrial for GAMLSS packages",
    "section": "",
    "text": "This site is created to provide a platform to explain some of the function in GAMLSS packages in R software."
  },
  {
    "objectID": "gamlss_prepdata.html",
    "href": "gamlss_prepdata.html",
    "title": "The Functions in the package gamlss.prepdata",
    "section": "",
    "text": "This booklet introduces the gamlss.prepdata package and its functionality. It aims to describe the available functions and how they can be used.\n\nThe latest versions of the packages gamlss, gamlss2 and gamlss.prepdata are shown below:\n\nrm(list=ls())\nlibrary(gamlss)\nlibrary(gamlss2)\nlibrary(ggplot2)\nlibrary(gamlss.ggplots)\nlibrary(gamlss.prepdata)\nlibrary(\"dplyr\") \npackageVersion(\"gamlss\")\n\n[1] '5.4.23'\n\npackageVersion(\"gamlss2\")\n\n[1] '0.1.0'\n\npackageVersion(\"gamlss.prepdata\")\n\n[1] '0.1.6'"
  },
  {
    "objectID": "gamlss_prepdata.html#sec-introduction",
    "href": "gamlss_prepdata.html#sec-introduction",
    "title": "The Functions in the package gamlss.prepdata",
    "section": "",
    "text": "This booklet introduces the gamlss.prepdata package and its functionality. It aims to describe the available functions and how they can be used.\n\nThe latest versions of the packages gamlss, gamlss2 and gamlss.prepdata are shown below:\n\nrm(list=ls())\nlibrary(gamlss)\nlibrary(gamlss2)\nlibrary(ggplot2)\nlibrary(gamlss.ggplots)\nlibrary(gamlss.prepdata)\nlibrary(\"dplyr\") \npackageVersion(\"gamlss\")\n\n[1] '5.4.23'\n\npackageVersion(\"gamlss2\")\n\n[1] '0.1.0'\n\npackageVersion(\"gamlss.prepdata\")\n\n[1] '0.1.6'"
  },
  {
    "objectID": "gamlss_prepdata.html#introduction-to-the-gamlss.prepdata-package",
    "href": "gamlss_prepdata.html#introduction-to-the-gamlss.prepdata-package",
    "title": "The Functions in the package gamlss.prepdata",
    "section": "Introduction to the gamlss.prepdata Package",
    "text": "Introduction to the gamlss.prepdata Package\nThe gamlss.prepdata package originated from the gamlss.ggplots package. As gamlss.ggplots became too large for easy maintenance, it was split into two separate packages, and gamlss.prepdata was created.\nSince gamlss.prepdata is still at an experimental stage, some functions are hidden to allow time for thorough checking and validation. These hidden functions can still be accessed using the triple colon notation, for example: gamlss.prepdata:::.\nThe functions available in gamlss.prepdata are intended for pre-fitting — that is, to be used before applying the gamlss() or gamlss2() fitting functions. The available functions can be grouped into the following categories:\n\nInformation functions\nThese functions provide information about:\n- The size of the dataset\n\n- The presence and extent of missing values\n\n- The structure of the dataset\n\nWhether the variables are numeric or factors, and how they should be prepared for analysis\n\n\n\nPlotting functions\nThese functions allow plotting for:\n-   individual variables\n\n-   Pairwise relationships between variables.\n\n\nFeatures functions\nFunctions that assist in\n\nDetecting outliers\nApplying transformations\nScaling variables.\n\n\n\nData Partition functions\nFunctions that facilitate partitioning data to improve inference and avoid overfitting during model selection.\n\n\nPurpose and Usage\nThe information and plotting functions provide valuable insights that assist in building better models, including:\n\nUnderstanding the distribution of the response variable\nChoosing the appropriate type of analysis\nExamining explanatory variables, including:\n\nRange and spread of values\nPresence of missing values\nAssociations and interactions between explanatory variables\nNature of relationships between response and explanatory variables (linear or non-linear)\n\n\nThe features functions focus on handling outliers, scaling, and transforming explanatory variables (x-variables) before modeling.\nData partitioning is used to avoid overfitting by ensuring models are evaluated more reliably. It falls under the broader category of data manipulation, although merging datasets is not covered here — only partitioning for improved inference is addressed.\nMost of the pre-fitting functions are data-related, and their names typically start with data (e.g., data_NAME), indicating that they either print information, produce plots, or manipulate data.frames.\nThe gamlss.prepdata package is thus a useful tool for carrying out pre-analysis work before beginning the process of fitting a distributional regression model.\nNext, we define what we mean by distributional regression."
  },
  {
    "objectID": "gamlss_prepdata.html#distributional-regression-model",
    "href": "gamlss_prepdata.html#distributional-regression-model",
    "title": "The Functions in the package gamlss.prepdata",
    "section": "Distributional Regression model",
    "text": "Distributional Regression model\nThe aim of this vignette is to demonstrate how to manipulate and prepare data before applying a distributional regression analysis.\nThe general form a distributional regression model can be written as; \\[\n\\begin{split}\n\\textbf{y}     &  \\stackrel{\\small{ind}}{\\sim }  \\mathcal{D}( \\boldsymbol{\\theta}_1, \\ldots, \\boldsymbol{\\theta}_k) \\nonumber \\\\\ng_1(\\boldsymbol{\\theta}_1) &= \\mathcal{ML}_1(\\textbf{x}_{11},\\textbf{x}_{21}, \\ldots,  \\textbf{x}_{p1}) \\nonumber \\\\\n\\ldots &= \\ldots \\nonumber\\\\\ng_k(\\boldsymbol{\\theta}_k) &= \\mathcal{ML}_k(\\textbf{x}_{1k},\\textbf{x}_{2k}, \\ldots,  \\textbf{x}_{pk}).\n\\end{split}\n  \\tag{1}\\] where we assume that the response variable \\(y_i\\) for \\(i=1,\\ldots, n\\), is independently distributed having a distribution \\(\\mathcal{D}( \\theta_1, \\ldots, \\theta_k)\\) with \\(k\\) parameters and where all parameters could be effected by the explanatory variables \\(\\textbf{x}_{1},\\textbf{x}_{2}, \\ldots,  \\textbf{x}_{p}\\). The \\(\\mathcal{ML}\\) represents any regression type machine learning algorithm i.e. LASSO, Neural networks etc.\nWhen only additive smoothing terms are used in the fitting the model can be written as; \\[\\begin{split}\n\\textbf{y}     &  \\stackrel{\\small{ind}}{\\sim }  \\mathcal{D}(  \\boldsymbol{\\theta}_1, \\ldots,  \\boldsymbol{\\theta}_k) \\nonumber \\\\\ng_1( \\boldsymbol{\\theta}_1)  &= b_{01} + s_1(\\textbf{x}_{11})  +  \\cdots,  +s_p(\\textbf{x}_{p1}) \\nonumber\\\\\n\\ldots &= \\ldots \\nonumber\\\\\ng_k( \\boldsymbol{\\theta}_k)  &= b_{0k} + s_1(\\textbf{x}_{1k})  +   \\cdots,  +s_p(\\textbf{x}_{pk}).\n\\end{split}\n\\tag{2}\\] which is the GAMLSS model introduced by Rigby and Stasinopoulos (2005).\nThere are three books on GAMLSS, D. M. Stasinopoulos et al. (2017), Rigby et al. (2019) and M. D. Stasinopoulos et al. (2024) and several ’GAMLSS lecture materials, available from GitHUb, https://github.com/mstasinopoulos/Porto_short_course.git and https://github.com/mstasinopoulos/ShortCourse.git. The latest R packages related to GAMLSS can be found in https://gamlss-dev.r-universe.dev/builds.\nThe aim of this package is to prepare data and extract useful information that can be utilized during the modeling stage."
  },
  {
    "objectID": "gamlss_prepdata.html#sec-Information",
    "href": "gamlss_prepdata.html#sec-Information",
    "title": "The Functions in the package gamlss.prepdata",
    "section": "Ιnformation functions",
    "text": "Ιnformation functions\nThe functions for obtaining information from a dataset are described in this section and summarized in Table 1. These functions can provide:\n\nGeneral information about the dataset such as the dimensions (number of rows and columns) and the percentage of omitted (missing) observations\nInformation about the variables in the dataset\nInformation about the observations in the dataset\n\nAll functions have a data.frame as their first argument.\nHere is a table of the information functions;\n\n\n\nTable 1: A summary table of the functions to obtain information\n\n\n\n\n\n\n\n\n\nFunctions\nUsage\n\n\n\n\ndata_dim()\nReturns the number of rows and columns, and calculates the percentage of omitted (missing) observations\n\n\ndata_names()\nLists the names of the variables in the dataset\n\n\ndata_rename()\nAllows renaming of one or more variables in the dataset\n\n\ndata_shorter_names()\nAllows shortening the names of one or more variables in the dataset\n\n\ndata_distinct()\nDisplays the number of distinct values for each variable in the dataset\n\n\ndata_which_na()\nDisplays the count of NA (missing) values for each variable in the dataset\n\n\ndata_omit()\nRemoves all rows (observations) that contain any NA (missing) values\n\n\ndata_str()\nDisplays the class of each variable (e.g., numeric, factor, etc.) along with additional details about the variables\n\n\ndata_cha2fac()\nConverts all character variables in the dataset to factor type\n\n\ndata_few2fac()\nConverts variables with a small number of distinct values (e.g., binary or categorical) to factor type\n\n\ndata_int2num()\nConverts integer variables with several distinct values to numeric type to allow for continuous analysis\n\n\ndata_rm()\nRemoves one or more variables (columns) from the dataset as specified by the user\n\n\ndata_rmNAvars()\nRemoves variables which have NA’s as all its values.\n\n\ndata_rm1val()\nRemoves factors that have only a single level (no variability) in the dataset\n\n\ndata_select()\nAllows the user to select one or more specific variables (columns) from the dataset for further analysis\n\n\ndata_exclude_class()\nRemoves all variables of a specified class (e.g., factor, numeric) from the dataset\n\n\ndata_only_continuous()\nRetains only the numeric variables (columns) in the dataset, excluding factors or other variable types\n\n\n\n\n\n\nNext we give examples of the functions.\n\ndata_dim()\nThis function provides detailed information about the dimensions of a data.frame. It is similar to the R function dim(), but with additional details. The output is the original data frame, allowing it to be used in a series of piping commands.\n\nrent99 |&gt; data_dim()\n\n************************************************************** \n************************************************************** \nthe R class of the data is: data.frame \nthe dimensions of the data are: 3082 by 9 \nnumber of observations with missing values: 0 \n% of NA's in the data: 0 % \n************************************************************** \n************************************************************** \n\n\nback to table\n\n\ndata_names()\nThis function provides the names of the variables in the data.frame, similar to the R function names().\n\nrent99 |&gt; data_names()\n\n************************************************************** \n************************************************************** \nthe names of variables \n[1] \"rent\"     \"rentsqm\"  \"area\"     \"yearc\"    \"location\" \"bath\"     \"kitchen\" \n[8] \"cheating\" \"district\"\n************************************************************** \n************************************************************** \n\n\nThe output of the function is the original data frame. back to table\n\n\ndata_rename()\nRenames one or more variables (columns) in the data.frame using the function data_rename();\n\nda&lt;- rent99 |&gt; data_rename(oldname=\"rent\", newname=\"R\")\nhead(da)\n\n         R   rentsqm area yearc location bath kitchen cheating district\n1 109.9487  4.228797   26  1918        2    0       0        0      916\n2 243.2820  8.688646   28  1918        2    0       0        1      813\n3 261.6410  8.721369   30  1918        1    0       0        1      611\n4 106.4103  3.547009   30  1918        2    0       0        0     2025\n5 133.3846  4.446154   30  1918        2    0       0        1      561\n6 339.0256 11.300851   30  1918        2    0       0        1      541\n\n\nThe output of the function is the original data frame with new names.\nback to table\n\n\ndata_shorter_names()\nIf the variables in the dataset have very long names, they can be difficult to handle in formulae during modelling. The function data_shorter_names() abbreviates the names of the explanatory variables, making them easier to use in formulas.\n\nrent99 |&gt; data_shorter_names()\n\n************************************************************** \n************************************************************** \nthe names of variables \n[1] \"rent\"  \"rents\" \"area\"  \"yearc\" \"locat\" \"bath\"  \"kitch\" \"cheat\" \"distr\"\n************************************************************** \n************************************************************** \n\n\nIf no long variable names exist in the dataset, the function data_shorter_names() does nothing. However, when applicable, the function abbreviates long names and returns the original data frame with the new shortened names.\n\n\n\n\n\n\nWarning\n\n\n\nNote that there is a risk when using a small value for the max option, as it may result in identical names for different variables. This could lead to confusion or errors in the modelling process. It is important to carefully choose an appropriate value for max to avoid this issue.\n\n\nback to table\n\n\ndata_distinct()\nTThe distinct values for each variable are shown below.\n\nrent99 |&gt; data_distinct()\n\n    rent  rentsqm     area    yearc location     bath  kitchen cheating \n    2723     3053      132       68        3        2        2        2 \ndistrict \n     336 \n\n\nThe output of the function is the original data frame.\nback to table\n\n\ndata_which_na()\nThis function provides information about which variables have missing observations and how many missing values there are for each variable;\n\nrent99 |&gt; data_which_na()\n\n    rent  rentsqm     area    yearc location     bath  kitchen cheating \n       0        0        0        0        0        0        0        0 \ndistrict \n       0 \n\n\nThe output of the function is the original data frame nothing is changing.\nback to table\n\n\ndata_omit\nThe function data_str() (similar to the R function str()) provides information about the types of variables present in the dataset.\n\nrent99 |&gt; data_omit()\n\n************************************************************** \n************************************************************** \nthe R class of the data is: data.frame \nthe dimensions of the data before omition are: 3082 x 9 \nthe dimensions of the data saved after omition are: 3082 x 9 \nthe number of observations omited: 0 \n************************************************************** \n************************************************************** \n\n\nThe output of the function is a new data frame with all NA’s omitted.\n\n\n\n\n\n\nWarning\n\n\n\nIt is important to select the relevant variables before using the data_omit() function, as some unwanted variables may contain many missing values that could lead to unnecessary row omissions.\n\n\nback to table\n\n\ndata_str()\nThe function data_str() (similar to the R function str()) provides information about the types of variable exist in the data.\n\nrent99 |&gt; data_str()\n\n************************************************************** \n************************************************************** \nthe structure of the data \n'data.frame':   3082 obs. of  9 variables:\n $ rent    : num  110 243 262 106 133 ...\n $ rentsqm : num  4.23 8.69 8.72 3.55 4.45 ...\n $ area    : int  26 28 30 30 30 30 31 31 32 33 ...\n $ yearc   : num  1918 1918 1918 1918 1918 ...\n $ location: Factor w/ 3 levels \"1\",\"2\",\"3\": 2 2 1 2 2 2 1 1 1 2 ...\n $ bath    : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ kitchen : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 2 1 1 ...\n $ cheating: Factor w/ 2 levels \"0\",\"1\": 1 2 2 1 2 2 1 2 1 1 ...\n $ district: int  916 813 611 2025 561 541 822 1713 1812 152 ...\n************************************************************** \n************************************************************** \ntable of the class of variabes \n\n factor integer numeric \n      4       2       3 \n************************************************************** \n************************************************************** \ndistinct values in variables \n    rent  rentsqm     area    yearc location     bath  kitchen cheating \n    2723     3053      132       68        3        2        2        2 \ndistrict \n     336 \nconsider to make those characters vectors into factors: \nlocation bath kitchen cheating \n************************************************************** \n************************************************************** \n\n\nThe output of the function is the original data frame.\nThe next set of functions manipulate variables withing data.frames.\nback to table\n\n\ndata_cha2fac()\nOften, variables in datasets are read as character vectors, but for analysis, they may need to be treated as factors. This function transforms any character vector (with a relatively small number of distinct values) into a factor.\n\nrent99 |&gt; data_cha2fac()  -&gt; da \n\n************************************************************** \nnot character vector was found \n\n\nSince no character were found nothing have changed. The output of the function is a new data frame.\nback to table\n\n\ndata_few2fac()\nThere are occasions when some variables have very few distinct observations, and it may be better to treat them as factors. The function data_few2fac() converts vectors with a small number of distinct values into factors.\n\nrent99 |&gt;  data_few2fac() -&gt; da \n\n************************************************************** \n    rent  rentsqm     area    yearc location     bath  kitchen cheating \n    2723     3053      132       68        3        2        2        2 \ndistrict \n     336 \n************************************************************** \n4 vectors with fewer number of values than 5 were transformed to factors \n************************************************************** \n************************************************************** \n\nstr(da)\n\n'data.frame':   3082 obs. of  9 variables:\n $ rent    : num  110 243 262 106 133 ...\n $ rentsqm : num  4.23 8.69 8.72 3.55 4.45 ...\n $ area    : int  26 28 30 30 30 30 31 31 32 33 ...\n $ yearc   : num  1918 1918 1918 1918 1918 ...\n $ location: Factor w/ 3 levels \"1\",\"2\",\"3\": 2 2 1 2 2 2 1 1 1 2 ...\n $ bath    : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 1 1 1 ...\n $ kitchen : Factor w/ 2 levels \"0\",\"1\": 1 1 1 1 1 1 1 2 1 1 ...\n $ cheating: Factor w/ 2 levels \"0\",\"1\": 1 2 2 1 2 2 1 2 1 1 ...\n $ district: int  916 813 611 2025 561 541 822 1713 1812 152 ...\n\n\nThe output of the function is a new data frame. This can be seen by using the R function str();\nback to table\n\n\ndata_int2num()\nOccasionally, we need to convert integer variables with a very large range of values into numeric vectors, especially for graphics. The function data_int2num() performs this conversion.\n\nrent99 |&gt; data_int2num() -&gt; da\n\n************************************************************** \n    rent  rentsqm     area    yearc location     bath  kitchen cheating \n    2723     3053      132       68        3        2        2        2 \ndistrict \n     336 \n2 integer vectors with more number of values than 50 were transformed to numeric \n************************************************************** \n\n\nThe output of the function is a new data frame.\nback to table\n\n\ndata_rm()\nFor plotting datasets, it’s easier to keep only the relevant variables. The function data_rm() allows you to remove unnecessary variables, making the dataset more manageable for visualization.\n\ndata_rm(rent99, c(2,9)) -&gt; da\ndim(rent99)\n\n[1] 3082    9\n\ndim(da)\n\n[1] 3082    7\n\n\nThe output of the function is a new data frame. Note that this could be also done using the function select() of the package dplyr, or our own data_select() function.\nback to table\n\n\ndata_rmNAvars()\nOccutionally when we read data from files in Rsome extra variables are accidentaly produced with no values but NA’s. The function data_rmNAvars() removes those variables from the data;\n\ndata_rmNAvars(da)\n\nback to table\n\n\ndata_rm1val()\nThis function searches for variables with only a single distinct value (often factors left over from a previous subset() operation) and removes them from the dataset.\n\nda |&gt; data_rm1val()\n\nThe output of the function is a new data frame.\nback to table\n\n\ndata_select()\nThis function selects specified variables from a dataset.\n\nda1&lt;-rent |&gt; data_select( vars=c(\"R\", \"Fl\", \"A\"))\nhead(da1)\n\n       R Fl    A\n1  693.3 50 1972\n2  422.0 54 1972\n3  736.6 70 1972\n4  732.2 50 1972\n5 1295.1 55 1893\n6 1195.9 59 1893\n\n\nThe output of the function is a new data frame.\nback to table\n\n\ndata_exclude_class()\nThis function searches for variables (columns) of a specified R class and removes them from the dataset. By default, the class to remove is factor.\n\nda |&gt; data_exclude_class()  -&gt; da1\nhead(da1)\n\n      rent area yearc\n1 109.9487   26  1918\n2 243.2820   28  1918\n3 261.6410   30  1918\n4 106.4103   30  1918\n5 133.3846   30  1918\n6 339.0256   30  1918\n\n\nThe output of the function is a new data frame.\n\n\ndata_only_continuous()\nThis function keeps only the continuous variables in the dataset, removing all non-continuous variables.\n\nda |&gt; data_only_continuous()  -&gt; da1\nhead(da1)\n\n      rent area yearc\n1 109.9487   26  1918\n2 243.2820   28  1918\n3 261.6410   30  1918\n4 106.4103   30  1918\n5 133.3846   30  1918\n6 339.0256   30  1918\n\n\nThe output of the function is a new data frame.\nback to table"
  },
  {
    "objectID": "gamlss_prepdata.html#sec-Graphical-functions",
    "href": "gamlss_prepdata.html#sec-Graphical-functions",
    "title": "The Functions in the package gamlss.prepdata",
    "section": "Graphical functions",
    "text": "Graphical functions\nGraphical methods are used in pre-analysis to examine individual variables or pair-wise relationships between variables.\n\n\n\nTable 2: A summary table of the graphical functions\n\n\n\n\n\n\n\n\n\nFunctions\nUsage\n\n\n\n\ndata_plot()\nPlots each variable in the dataset to visualize its distribution or other important characteristics (see also Section 6.1\n\n\ndata_bucket()\nGenerates bucket plots for all numerical variables in the dataset to visualize their skewness and kurtosis\n\n\ndata_response()\nPlots the response variable alongside its z-score, providing a standardized version of the response for comparison\n\n\ndata_zscores()\nPlots the z-scores for all continuous variables in the dataset, allowing for easy visualization of the standardized values (see also Section 6.1)\n\n\ndata_xyplot()\nGenerates pairwise plots of the response variable against all other variables in the dataset to visualize their relationships\n\n\ndata_cor()\nCalculates and plots the pairwise correlations for all continuous variables in the dataset to assess linear relationships between them\n\n\ndata_void()\nSearches for pairwise empty spaces across all continuous variables in the dataset to identify problems with interpretation or prediction\n\n\ndata_pcor()\nCalculates and plots the pairwise patrial-correlations for all continuous variables in the dataset\n\n\ndata_inter()\nSearches for potential pairwise interactions between variables in the dataset to identify relationships or dependencies that may be useful for modelling\n\n\ndata_leverage()\nDetects outliers in the continuous explanatory variables (x-variables) as a group to highlight unusual observations\n\n\ndata_Ptrans_plot()\nPlots the response variable against various power transformations of the continuous x-variables to explore potential relationships and model suitability\n\n\n\n\n\n\nNext we give examples of the graphical functions.\n\ndata_plot()\nThe function data_plot plots all the variables of the data individually; It plots the continuous variable as histograms with a density plots superimposed, see the plot for rent and yearc. As an alternative a dot plots can be requested see for an example in ?@sec-data_response. For integers the function plots needle plots, see area below and for categorical the function plots bar plots, see location, bath kitchen and cheating below.\n\nda |&gt; data_plot()\n\n 100 % of data are saved, \nthat is, 3082 observations. \n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\nRemoved 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\n\nThe function could saves the ggplot2 figures.\nback to table\n\n\ndata_bucket()\nThe function data_bucket plots bucket plots (which identify skewness and kurosid) for all variables in the data set.\n\nda |&gt; data_bucket()\n\n 100 % of data are saved, \nthat is, 3082 observations. \n    rent     area    yearc location     bath  kitchen cheating \n    2723      132       68        3        2        2        2 \n\n\n\n\n\n\n\n\n\nback to table\n\n\ndata_response()\nThe function data_response() plots four different plots related to a continuous response variable. It plots; i) a histogram (and the density function); ii) a dot plot of the response in the original scale iii) a histogram (and the density function) of the response at the z-scores scale and vi) s dot-plot in the z-score scale.\nThe z-score scale is defined by fitting a distribution to the variable (normal or SHASH) and then taking the residuals, see also next section. The dot plots are good in identify highly skew variables and unusual observations. They display the median and inter quantile range of the data. The y-axis of a dot plot is a randomised uniform variable (therefore the plot could look slightly different each time.)\n\nda |&gt; data_response(, response=rent)\n\n 100 % of data are saved, \nthat is, 3082 observations. \nthe class of the response is numeric is this correct? \na continuous distribution on (0,inf) could be used \n\n\n\n\n\n\n\n\n\nback to table\n\n\ndata_zscores()\nOne could fit any four parameter (GAMLSS) distribution, defined on \\(-\\infty\\) to \\(\\infty\\), to any continuous variable, where skewness and kurtosis is suspected and take the quantile residuals (or z-scores) as the transformed values x-values. The function y_zscores() performs just this. It takes a continuous variable and fits a continuous four parameter distribution and gives the z-scores. The fitting distribution can be specified by the user, but as default we use the SHASHo distribution. Below we demonstrate that the function y_zscores() is equivalent to fitting the SHASHo distribution to a continuous variable and then take the quqntile residuals from the fitted model as the result.\n\nda |&gt; data_zscores()\n\n\n\n\n\n\n\n\nIn order to see how the z-scores are calculated consider the function y_zscores() which is taking an individual variables z-scores;\n\n z &lt;- y_zscores(rent99$rent, plot=FALSE)\n\nThe function is equivalent of fitting a constant model to all the parameters of a given distribution and than taking the quantile residuals (or z=scores) as he variable of interest. The default distribution is the four parameter SHASHo distribution.\n\nlibrary(gamlss2)\nm1 &lt;- gamlssML(rent99$rent, family=SHASHo) # fitting a 4 parameter distribution \ncbind(z,resid(m1))[1:5,]#  and taking the residuals\n\n          z          \n1 -2.496097 -2.496097\n2 -1.340464 -1.340464\n3 -1.182014 -1.182014\n4 -2.526326 -2.526326\n5 -2.295069 -2.295069\n\n\nback to table\n\n\ndata_xyplot()\nThe functions data_xyplot() plots the response variable against each of the independent explanatory variables. It plots the continuous against continuous as scatter plots and continuous variables against categorical as box plot.\n\n\n\n\n\n\nNote\n\n\n\nAt the moment there is no provision for categorical response variables.\n\n\n\nda |&gt; data_xyplot(response=rent )\n\n 100 % of data are saved, \nthat is, 3082 observations. \n\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nThe output of the function saves the ggplot2 figures.\nNote hat the package gamlss.prepdata do not provides pairwise plot of the explanatory variables themself but the package GGally does. here is an example ;\n\nlibrary(GGally) \n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\ndac &lt;- gamlss.prepdata:::data_only_continuous(da) \nggpairs(dac, lower=list(continuous = \"cor\",\n      combo = \"box_no_facet\", discrete = \"count\", \n      na = \"na\"),upper=list(continuous = \"points\", \n      combo = \"box_no_facet\", discrete = \"count\", na = \"na\")) + \ntheme_bw(base_size =15)\n\n\n\n\n\n\n\n\nback to table\n\n\ndata_cor()\nThe function data_corr() is taking a data.frame object and plot the correlation coefficients of all its continuous variables.\n\ndata_cor(da, lab=TRUE)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n4 factors have been omited from the data \n\n\nWarning in data_cor(da, lab = TRUE):\n\n\n\n\n\n\n\n\n\nA different type of plot can be produce if we use;\n\ndata_cor(da, method=\"circle\", circle.size = 40)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n4 factors have been omited from the data \n\n\nWarning in data_cor(da, method = \"circle\", circle.size = 40):\n\n\n\n\n\n\n\n\n\nTo get the variables with higher, say, than $0.4 $ correlation values use;\n\nTabcor &lt;- data_cor(da, plot=FALSE)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n4 factors have been omited from the data \n\n\nWarning in data_cor(da, plot = FALSE):\n\nhigh_val(Tabcor, val=0.4)\n\n     name1  name2  corrs  \n[1,] \"rent\" \"area\" \"0.585\"\n\n\nWe can plot the path of those variables using the package corr;\n\nlibrary(corrr)\nnetwork_plot(Tabcor)\n\n\n\n\n\n\n\n\nback to table\n\n\ndata_association()\nThe function data_association() is taking a data.frame object and plot the pair-wise association of all its variables. The pair-wise association for two continuous variables is given by default by the absolute value of the Spearman’s correlation coefficient. For two categorical variables by the (adjusted for bias) Cramers’ v-coefficient. For a continuous against a categorical variables by the \\(\\sqrt R^2\\) obtained by regressing the continuous variable against the categorical variable.\n\ndata_association(da, lab=TRUE)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n\n\n\n\n\n\n\n\n\nA different type of plot can be produce if we use;\n\ndata_association(da, method=\"circle\", circle.size = 40)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n\n\n\n\n\n\n\n\n\nTo get the variables with higher, say, than $0.4 $ association values use;\n\nTabcor &lt;- data_cor(da, plot=FALSE)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n4 factors have been omited from the data \n\n\nWarning in data_cor(da, plot = FALSE):\n\nhigh_val(Tabcor, val=0.4)\n\n     name1  name2  corrs  \n[1,] \"rent\" \"area\" \"0.585\"\n\n\nA different plot can be acheived using the funtion network_plot() of package corr;\n\npp=gamlss.prepdata:::data_association(rent[,-c(4,5,8)], plot=F)\n\n 100 % of data are saved, \nthat is, 1969 observations. \n\nlibrary(corrr)\nnetwork_plot(pp, min_cor = 0, colors = c(\"red\", \"green\"), legend = \"range\")\n\n\n\n\n\n\n\n\nback to table\n\n\ndata_void()\n\n\n\n\n\n\nWarning\n\n\n\nThis function is new. Its theoretical foundation are not proven yet. The function needs testing and therefore it should be used with causion.\n\n\nThe idea behind the functions void() and its equivalent data.frame version data_void() is to be able to identify whether the data in the direction of two continuous variables say \\(x_i\\) and \\(x_j\\) have a lot of empty spaces. The reason is that empty spaces effect prediction since interpolation at empty spaces is dengerous. The function data_void() is taking a data.frame object and plot the percentage of empty spaces for all pair-wise continuous variables. The function used the foreach() function of the package foreach to allow parallel processing.\n\nregisterDoParallel(cores = 9)\ndata_void(da)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n4 factors have been omited from the data \n\n\nWarning in data_void(da):\n\n\n\n\n\n\n\n\n\nA different type of plot can be produce if we use;\n\ndata_void(da, method=\"circle\", circle.size = 40)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n4 factors have been omited from the data \n\n\nWarning in data_void(da, method = \"circle\", circle.size = 40):\n\n\n\n\n\n\n\n\nstopImplicitCluster()\n\nTo get the variables with highter than $0.4 $ values use;\n\nTabvoid &lt;- data_void(da, plot=FALSE)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n4 factors have been omited from the data \n\n\nWarning in data_void(da, plot = FALSE):\n\nhigh_val(Tabvoid, val=0.4)\n\n     name1  name2   corrs  \n[1,] \"rent\" \"area\"  \"0.661\"\n[2,] \"rent\" \"yearc\" \"0.627\"\n[3,] \"area\" \"yearc\" \"0.51\" \n\n\nTo plot their paths use;\n\nlibrary(corrr)\nnetwork_plot(Tabvoid)\n\n\n\n\n\n\n\n\nback to table\n\n\ndata_pcor()\nThe function data_copr() is taking a data.frame object and plot the partial correlation coefficients of all its continuous variables.\n\ndata_pcor(da, lab=TRUE)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n4 factors have been omited from the data \n\n\nWarning in data_pcor(da, lab = TRUE):\n\n\n\n\n\n\n\n\n\nTo get the variables with highter than $0.4 $ partial correlation values use;\n\nTabpcor &lt;- data_pcor(da, plot=FALSE)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n4 factors have been omited from the data \n\n\nWarning in data_pcor(da, plot = FALSE):\n\nhigh_val(Tabpcor, val=0.4)\n\n     name1  name2  corrs  \n[1,] \"rent\" \"area\" \"0.638\"\n\n\nFor plotting you can use;\n\nlibrary(corrr)\nnetwork_plot(Tabpcor)\n\n\n\n\n\n\n\n\nback to table\n\n\ndata_inter()\nThe function data_inter() takes a data.frame, fits all pair-wise interactions of the explanatory variables against the response (using a normal model) and produce a graph displaying their significance levels. The idea behind this is to identify possible first order interactions at an early stage of the analysis.\n\nda |&gt; gamlss.prepdata:::data_inter(response= rent)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n\n\n\n\n\n\n\n\n\n\ntinter &lt;-gamlss.prepdata:::data_inter(da,  response= rent, plot=FALSE)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n\ntinter\n\n         area yearc location  bath kitchen cheating\narea       NA 0.014    0.001 0.119   0.000    0.000\nyearc      NA    NA    0.000 0.027   0.154    0.226\nlocation   NA    NA       NA 0.000   0.000    0.578\nbath       NA    NA       NA    NA   0.868    0.989\nkitchen    NA    NA       NA    NA      NA    0.719\ncheating   NA    NA       NA    NA      NA       NA\n\n\nTo get the variables with lower than $0.05 $ significant interactions use;\n\nTabinter &lt;- gamlss.prepdata:::data_inter(da, response= rent, plot=FALSE)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n\nlow_val(Tabinter)\n\n     name1      name2      value  \n[1,] \"area\"     \"yearc\"    \"0.014\"\n[2,] \"area\"     \"location\" \"0.001\"\n[3,] \"yearc\"    \"location\" \"0\"    \n[4,] \"yearc\"    \"bath\"     \"0.027\"\n[5,] \"location\" \"bath\"     \"0\"    \n[6,] \"area\"     \"kitchen\"  \"0\"    \n[7,] \"location\" \"kitchen\"  \"0\"    \n[8,] \"area\"     \"cheating\" \"0\"    \n\n\nback to table\nNOT DOCUMENTED IN HELP FOR THIS FUNCTION\n\n\ndata_leverage()\nThe function data_leverage() uses the linear model methodology to identify posible unusual observations within the explanatory variables as a group (not indivisually). It fit a linear (normal) model with response, the response of the data, and all explanatory variables in the data, as x’s. It then calculates the leverage points and plots them. A leverage is a number between zero and 1. Large leverage correspond to extremes in the x’s.\n\nrent99[, -c(2,9)] |&gt; data_leverage( response=rent)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe horizontal line is the plot is at point \\(2 \\times (r/n)\\), which is the threshold suggested in the literature; values beyond this point could be identified as extremes. It looks that the point \\(2 \\times (r/n)\\) is too low (at least for our data). Instead the plot identify only observation which are in the one per cent upper quantile. That is a quantile value of the leverage at value 0.99.\n\n\nSection 6.1.3 show how the information of the function data_leverage() can be combined with the information given by data_outliers() in order to confirm hight outliers in the data.\n\n\n\n\n\n\nImportant\n\n\n\nWhat happen if multiple response are in the data?\n\n\nback to table\n\n\ndata_Ptrans_plot()\nThe essence of this plot is to visually decide whether certain values of \\(\\lambda\\) in a power transformation are appropriate or not. Section 6.3 describes the power transformation, \\(T=X^\\lambda\\) for \\(\\lambda&gt;0\\), in more details. For each continuous explanatory variable \\(X\\), the function show the response against \\(X\\), (\\(\\lambda=1\\)), the response against the square root of \\(X\\), (\\(\\lambda=1/2\\)) and the response against the log of \\(X\\) (\\(\\lambda \\rightarrow 0\\)). The user then can decide whether any of those transformation are appropriate.\n\ngamlss.prepdata:::data_Ptrans_plot(da, rent) \n\n\n\n\n\n\n\n\nIt look that no transformation is needed for the continuous explanatory variables area of yearc.\nback to table"
  },
  {
    "objectID": "gamlss_prepdata.html#sec-Features",
    "href": "gamlss_prepdata.html#sec-Features",
    "title": "The Functions in the package gamlss.prepdata",
    "section": "Features",
    "text": "Features\nThe features functions are functions to help with the continuous explanatory x-variables; We divide them here in four sections; Functions for\n\noutliers;\nscaling;\ntransformations and\nfunction approproate for time series\n\nThe available functions are;\n\n\n\nTable 3: The outliers, scalling, transformation and time series functions\n\n\n\n\n\n\n\n\n\nFunctions\nUsage\n\n\n\n\ny_outliers()\nidentify possible outliers in a continuous variable (see below)\n\n\ndata_outliers()\nidentify possible outliers for all continuous x-variable (see also data_zscores in Section 5.0.4)\n\n\ndata_leverage()\nthis is a repear of the function from Section 5.0.11\n\n\ndata_scale()\nscaling all continuous x-variables either to zero mean and one s.d. or to the range [0,1]\n\n\nxy_Ptrans()\nlooking for appropriate power transformation for response against one of the x-variable\n\n\ndata_Ptrans()\nlooking for appropriate power transformation for response against all x-variable\n\n\ndata_Ptrans_plot()\nplotting all x’s agains the response using identiti ty square root ans log transormations\n\n\n\n\n\n\n\nOutliers\nOutliers in the response variable, within a distributional regression framework like GAMLSS, are better handled by selecting an appropriate distribution for the response. For example, an outlier under the assumption of a normal distribution for the response may no longer be considered an outlier if a distribution from the t-family is assumed. The function data_response(), discussed in Section 5.0.3 provides guidelines on appropriate actions. This section focuses on outliers in the explanatory variables.\nOutliers in the continuous explanatory variables can potentially affect curve fitting, whether using linear or smooth non-parametric terms. In such cases, removing outliers from the x-variables can make the model more robust. Outliers are observations that deviate significantly from the rest of the data, but the concept depends on the dimension being considered. An single observation value might be an outlier within an explanatory variable. A pair observation might be an outlier examining within a pair-wise relationships. In a one-dimensional search, we identify extreme observations in individual continuous variables, based on how far they lie from the majority of the data. In a two-dimensional search, we look for outliers in pairwise plots of continuous variables, examining how observations deviate from typical relationships between variable pairs. Leverage points are useful to identify outliers when we look for outliers in \\(r\\) dimension when \\(r\\) is the number of explanatory variables.\n\n\n\n\n\n\nNote\n\n\n\nAt this preliminary stage of the analysis, where no model has yet been fitted, it is difficult to identify outliers among factors. This is because outliers in factors typically do not appear unusual when examining individual variables. Their outlier status usually becomes apparent only when considered in combination with other variables or factors. Identifying such outliers is a task better suited for the modelling stage of the analysis.\n\n\nThe function y_outliers() in Section 6.1.1 uses a simple algorithm to identify potential one-dimensional outliers in continuous explanatory variables. It is designed specifically for continuous variables.\n\nFor variables defined on the full real line, (i.e., from \\(-\\infty\\) to \\(+\\infty\\), the algorithm fits a parametric distribution—by default, the four-parameter SHASHo distribution—and computes z-scores (quantile residuals) to assess outlier status.\nFor variables defined on the positive real line (i.e., from \\(0\\) to \\(+ \\infty\\)), an additional transformation step is included. These variables are often highly right-skewed, and extreme values in the right tail can dominate any outlier detection process. To address this, the function attempts to find a power transformation \\(T = X^\\lambda\\) that minimizes the Jarque-Bera test statistic—a measure of deviation from normality. To find the optimal power parameters \\(\\lambda\\) the function y_outliers() uses the internal function x_Ptrans(). After this transformation, the SHASHo distribution is fitted, and z-scores are computed. The Jarque-Bera test statistic is always non-negative; the further it is from zero, the stronger the evidence against normality. The goal is to reduce skewness and kurtosis in the data before assessing outliers.\n\nOutliers are identify for observations with very low or hight z-scores. For example, if a z-scores is greater in absolute value to a specified value say 3. The methodology is appropriate for identifying outliers in one dimension. Pair-wise scatter plots of all continuous variables in the data can be use to identify outliers in two dimensions. To identify outliers in \\(p\\) dimensions the function data_leverage(), see Section 6.1.3, can be used.\n\n\n\n\n\n\nNote\n\n\n\nSeveral of the graphical functions described in Section 5 are good for identify visualy outliers. In fact we recommend that the function data_outliers() should be used in combination with functions like data_plot() or data_zscores().\n\n\n\ny_outliers()\nThe function y_outliers() identify outliers in one \\(X\\) variable.\n\ny_outliers(da$rent)\n\nnamed numeric(0)\n\n\nNote that the y_outliers() is used by data_outliers() to identify outliers in all continuous variables in the data.\nback to table\n\n\ndata_outliers()\nThe function data_outliers() uses the z-scores technique described above in order to detect outliers in the continuous variables in the data. It fits a SHASHo distribution to the continuous variable in the data and uses the z-scores (quantile residuals) to identify (one dimension) outliers for those continuous variables.\n\ndata_outliers(da)\n\n    rent     area    yearc location     bath  kitchen cheating \n    2723      132       68        3        2        2        2 \n\n\n$rent\nnamed numeric(0)\n\n$area\nnamed integer(0)\n\n$yearc\nnamed numeric(0)\n\n\nAs it happen no individual variable outliers were highlighted in the rent data using the z-scores methodology. In general we recommend the function data_outliers() to be used in combination of the graphical functions data_plot() and data_zscores()\nback to table\n\n\ndata_leverage() (repeat)\nThe function data_leverage(), fisrt describe in Section 5.0.11, can be used to detect extremes in the continuous variables in the data by fiting a linear model with all the continuous variables in the data and then useing the higher leverage points to identify (multi-dimension) outliers for the observations for all continuous variables. By default, it identifies one percent of the observations with high leverage in the data. Here we use the function as a way to identify the obsrvation (not plottoing).\n\ndata_leverage(da, response=rent, plot=FALSE)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n\n\n [1]   38  162  176  204  206  242  454  496  541  651  696  794  910  942 1094\n[16] 1296 1384 1388 1782 1837 1875 1907 1970 2530 2540 2727 2810 2890 2977 3065\n[31] 3070\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe functiondata_leverage() always identifies the top one percent of observations with the highest leverage, which may or may not be outliers. These high-leverage points can have a strong influence on model estimates. To better assess their impact, the list of high-leverage observations returned by data_leverage() should be compared with the list of outliers identified by the data_outliers() function. Observations that appear in both lists are particularly noteworthy, as they may be influential outliers that warrant further investigation.\n\n\nNext we are trying the proceess of identifing if data with high leverage are also coincide with outliers identified using the function data_outliers() function. The function intersect() will flash out the intersection of the two set of observations. First use data_outliers()\n\nul &lt;- unlist(data_outliers(da))\n\n    rent     area    yearc location     bath  kitchen cheating \n    2723      132       68        3        2        2        2 \n\n\nthen data_leverage();\n\nll&lt;-data_leverage(da, response=rent, plot=FALSE)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n\n\nand finally we inspect the intersection of the two sets using intersect();\n\nintersect(ll,ul)\n\nnumeric(0)\n\n\nHere we have zero outliers but in general we would expect to find common observations for further checking.\nback to table\n\n\n\nScaling\nScaling is another form of transformation for the continuous explanatory variables in the data which brings the explanatory variables in the same scale. It is a form of standardization. Standardization means bringing all continuous explanatory variables to similar range of values. For some machine learning techniques, i.e., principal component regression or neural networks standardization is mandatory in other like LASSO is recommended. There are two types of standardisation\n\nscaling to normality and\nscaling to a range from zero to one.\n\nScaling to normality it means that the variable should have a mean zero and a standard deviation one. Note that, this is equivalent of fitting a Normal distribution to the variables and then taking the z-scores (residuals) of the fit as the transformed variable. The problem with this type of scaling is that skewness and kurtosis in the standardised data persist. One could go further and fit a four parameter distribution instead of a normal where the two extra parameters could account for skewness and kurtosis. The function data_scale() described in Section 6.2.1 it gives this option with te argument family in which can used to specify a different distribution.\n\ndata_scale()\nThe function data_scale() perform scaling to all continuous variables an a data set. Note that factors in the data set are left untouched because when fitted within a model they will be transform to dummy variable which take values 0 or 1 (a kind of standardization). The response is left also untouched because it is assumed that an appropriate distribution will be fitted. The response variable has to be declared using the argument response. First we use data_scale() to scale to normality.\n\nhead(data_scale(da, response=rent))\n\n    area    yearc location     bath  kitchen cheating \n     132       68        3        2        2        2 \n\n\n      rent       area      yearc location bath kitchen cheating\n1 109.9487  0.7009962  0.7035881        2    0       0        0\n2 243.2820 -0.4796117  0.7484206        2    0       0        1\n3 261.6410 -0.2266243  0.6587556        1    0       0        1\n4 106.4103 -0.3531180 -1.2242096        2    0       0        0\n5 133.3846  0.1950214 -1.7173671        2    0       0        1\n6 339.0256 -0.5639408  1.6450707        2    0       0        1\n\n\nAs we mention before scaling to normality is equivalent of fitting a Normal variable first. The problem with scaling to normality is that if the variables are highly skew or kurtotic scaling them to normality does correct for skewness or kurtosis. Using a parametric distribution with four parameters some of which are skewness and kurtosis parameters it may correct that. Next we standardised using the SHASHo distribution;\n\nhead(data_scale(da, response=rent,  family=\"SHASHo\"))\n\n    area    yearc location     bath  kitchen cheating \n     132       68        3        2        2        2 \n\n\n      rent       area      yearc location bath kitchen cheating\n1 109.9487  0.7390434  0.6323846        2    0       0        0\n2 243.2820 -0.3940193  0.6795499        2    0       0        1\n3 261.6410 -0.1175129  0.5864434        1    0       0        1\n4 106.4103 -0.2526941 -1.0598183        2    0       0        0\n5 133.3846  0.2949483 -1.6281459        2    0       0        1\n6 339.0256 -0.4916986  2.0610405        2    0       0        1\n\n\nThe second form od standardisation is to transform all continuous x-variables to a range zero to one. Here is how this is done;\n\nhead(data_scale(da, response=rent,  scale.to=\"0to1\"))\n\n    area    yearc location     bath  kitchen cheating \n     132       68        3        2        2        2 \n\n\n      rent       area yearc location bath kitchen cheating\n1 109.9487 0.04285714     0        2    0       0        0\n2 243.2820 0.05714286     0        2    0       0        1\n3 261.6410 0.07142857     0        1    0       0        1\n4 106.4103 0.07142857     0        2    0       0        0\n5 133.3846 0.07142857     0        2    0       0        1\n6 339.0256 0.07142857     0        2    0       0        1\n\n\nback to table\n\n\n\nTransformations\nIn the context of outliers, the focus on how far a single observation deviates from the rest of the data. However, when considering transformations, the goal shifts toward a better capturing of the relationship between a continuous predictor and the response variable. Specifically, we aim to model peaks and troughs in the relationship between a single x-variable and the response better. To achieve this, it is sometimes sufficient to stretch the x-axis so that the predictor variable is more evenly distributed within its its range. A common approach for this is to use a power transformation of the form \\(T = X^\\lambda\\). This transoformation can reduce the curvature in the response and make the curve fitting easier. Importantly, the power transformation smoothly transitions into a logarithmic transformation as \\(\\lambda \\to 0\\), since: \\(\\frac{X^{\\lambda} - 1} {\\lambda} \\rightarrow \\log(X) \\quad \\text{as } \\lambda \\to 0\\). Thus, in the limit as \\(\\lambda \\to 0\\) is the power traformation is the log transformation \\(T = \\log(X)\\).\n\n\n\n\n\n\n\nNote\n\n\n\nThe power transformation is a subclass of the shifted power transformation\n\\(T = (\\alpha + X)^\\lambda\\) which is not consider here\n\n\nThe aim of the power transformation is to make the model fitting process more robust and reliable and maximizing the information extracted from the data. There are three different functions in gamlss.prepdata dealling with power transformations, xy_Ptrans(), data_Ptrans() and data_Ptrans_plot()\n\n\n\n\n\n\nNote\n\n\n\nIt’s important to distinguish between the concepts of transformations (the subject of this section) and of feature extraction, a term often used in machine learning. We refer to transformations as functions applied to a individual explanatory variables while we use feature extraction when multiple explanatory variables are involved. In both cases, the goal is to enhance the model capabilities.\n\n\n\nxy_Ptrans()\n\ngamlss.prepdata:::xy_Ptrans(da$area, da$rent) \n\n\ngamlss.prepdata:::xy_Ptrans(da$area, da$rent, prof=TRUE, k=4) \n\n\n\n\n\n\n\n\n\n\ndata_Ptrans()\n\ngamlss.prepdata:::data_Ptrans(da, response=rent)\n\n 100 % of data are saved, \nthat is, 3082 observations. \n\n\n$area\n[1] 1.06212\n\n$yearc\n[1] 1.499942\n\n\nback to table\n\n\ndata_Ptrans_plot() (repeat)\n\ngamlss.prepdata:::data_Ptrans_plot(da, rent) \n\n\n\n\n\n\n\n\nback to table\n\n\n\nTime series\nTime series data sets consist of observations collected at consistent time intervals—e.g., daily, weekly, or monthly. These data require special treatment because observations that are closer together in time tend to be more similar, violating the usual assumption of independence between observations. A similar issue arises in spatial data sets, where observations that are geographically closer often exhibit spatial correlation, again violating independence.\nMany data sets include spatial or temporal features —such as longitude and latitude or timestamps (e.g., 12/10/2012)— those variables are can be used for for modeling directly, but also for interpreting or visualization of the results. If temporal or spatial information exists in the data, it is essential to ensure these features are correctly read and interpreted in R. For instance, dates in R can be handled using the as.Date() function. To understand its functionality and options associated with the function please consult the help documentation via help(\"as.Date\"). Below, we provide two simple functions of how to work with date-time variables.\n\ntime_dt2dhour()\nSuppose the variable dt contains both a date and a time. We may want to extract and separate this into two components: one containing the date, and another capturing the hour of the day.\n\ndt &lt;- c(\"01/01/2011 00:00\", \"01/01/2011 01:00\", \"01/01/2011 02:00\", \"01/01/2011 03:00\", \"01/01/2011 04:00\", \"01/01/2011 05:00\") \n\nThe function to do this separation can lok like;\n\ntime_dt2dhour &lt;- function(datetime, format=NULL) \n  { \n          X &lt;- t(as.data.frame(strsplit(datetime,' '))) \nrownames(X) &lt;- NULL \ncolnames(X) &lt;- c(\"date\", \"time\") \n       hour &lt;- as.numeric(sub(\":\",\".\",X[,2])) \n      date &lt;- as.Date(X[,1],format=format) \n      data.frame(date, hour) \n  } \n\n\nP &lt;- time_dt2dhour(dt, \"%d/%m/%Y\") \nP\n\n        date hour\n1 2011-01-01    0\n2 2011-01-01    1\n3 2011-01-01    2\n4 2011-01-01    3\n5 2011-01-01    4\n6 2011-01-01    5\n\n\n\n\ntime_2num()\nThe second example contains time given in its common format, c(\"12:35\", \"10:50\") and we would like t change it to numeric which we can use in the model.\n\nt &lt;- c(\"12:35\", \"10:50\")\n\nThe function to change this to numeric;\n\ntime_2num &lt;- function(time, pattern=\":\") \n  { \n  t &lt;- gsub(pattern, \".\", time) \n   as.numeric(t) \n  } \ntime_2num(t) \n\n[1] 12.35 10.50\n\n\nFor user who deal with a lot of time series try the function POSXct() and also the function select() from the dplur package."
  },
  {
    "objectID": "gamlss_prepdata.html#sec-Partition",
    "href": "gamlss_prepdata.html#sec-Partition",
    "title": "The Functions in the package gamlss.prepdata",
    "section": "Data Partition",
    "text": "Data Partition\n\nIntrodunction to data partition\nPartitioning the data is essential for comparing different models and checking for overfitting. Figure Figure 1 illustrates the different ways the data can be split. A dataset can be split multiple times or just once, depending on the size of the dataset. For large datasets, it is common to split the data into a training set and a test set, and, if necessary, a validation set. Observations in the training set are referred to as in-bag, while observations in the test or validation set are termedout-of-bag`.\nThe training set is used for fitting the model, the test set is used for prediction and comparison between models, and the validation set is used for tuning the model’s hyperparameters. In additive smoothing regression models, for instance, the hyperparameters refer to the smoothing parameters in the model. Data partitioning allows for performance evaluation, model assumption checks, and detection of overfitting.\nIf the fitted model performs reasonably well on both the training and test sets, one can be confident that overfitting has been avoided. Overfitting occurs when a model fails to generalize well to new data, typically because it is too closely fitted to the sample data. In contrast, underfitting refers to a poor model that fails to capture the essential patterns in the data, resulting in a model that is distant from both the sample and the population it is intended to represent.\n\nVarious goodness-of-fit measures can be used to check and compare different distributional regression models. One of the most popular methods is the Generalized Akaike Information Criterion (GAIC). To calculate the GAIC, only the in-bag data are required, meaning there is no need for data partitioning. However, calculating the GAIC requires an accurate measure of the degrees of freedom used to fit the model. In mathematical (stochastic) models, the degrees of freedom are straightforward to define as they correspond to the number of estimated parameters. In contrast, for algorithmic models, particularly over-parameterized ones like neural networks, estimating degrees of freedom is more challenging. In mathematical models, the degrees of freedom serve as a measure of the model’s complexity. More complex models are typically closer to the training data, which may reduce their ability to generalize well to new data.\nIn addition to GAIC, other commonly used goodness-of-fit measures for regression models include the deviance, \\(R^2\\), and mean squared error (MSE), among others. However, these measures are prone to overfitting when evaluated on the in-bag dataset rather than on the out-of-bag data. Furthermore, \\(R^2\\) may not generalize well to non-normal error distributions, and MSE (and its variations) is often inappropriate for distributional regression models. This is because MSE focuses primarily on the location parameters of the response distribution, while other aspects of the response distribution are often of greater interest to practitioners.\n\n\n\n\n\n\n\nflowchart TB\n  A[Data] --&gt; B{Multi-split} \n  A --&gt; C{Split once}\n  C --&gt; D[Training]\n  D --&gt; E[Validate]\n  E --&gt; O[Test]\n  B --&gt; F(Rearranged)\n  B --&gt; G(Reweighed)\n  G--&gt; H(Bayesian \\n Bootstrap)\n  G--&gt; J(Boosting)\n  F --&gt; K(Non-param. \\n Bootstrap)\n  F --&gt; L(Cross \\n Validation)\n  L --&gt; M(k-Fold) \n  L --&gt; N(LOO) \n\n\n\n\nFigure 1: Different ways of splitting data to get in order to get more information”\n\n\n\n\n\nPartitioning the data into training, test, and validation sets should be done randomly. A potential risk when splitting the data only once arises when the investigation focuses on the tails of the response distribution. In such cases, the few extreme observations may end up in only one of the partitions, leading to potential issues when evaluating the model. Repeatedly reusing parts of the data, such as through bootstrapping or cross-validation, can help mitigate this risk. Multi-splitting the data (shown on the left side of Figure Figure 1) enables the evaluation of appropriate goodness-of-fit measures using out-of-bag data. There are two conceptual ways to multi-split a dataset: the first involves rearranging the data, and the second involves reweighting the observations. [As we will see later, the distinction between these two approaches is not always as clear-cut.] Rearranging the data is typically accomplished by creating appropriate indices that select different parts of the data. This leads to methods such as bootstrapping or cross-validation. On the other hand, reweighting the observations during the fitting process is a technique employed in the Bayesian Bootstrap and boosting.\n\nIn classical bootstrapping, the data are re-sampled randomly with replacement \\(B\\) times, and the model is refitted the same number of times. While computationally expensive, this process provides additional insights into the model, particularly regarding the variability of its parameters. For distributional regression models, both classical and Bayesian bootstrapping offer extra information about all the parameters in the model—namely, the distributional parameters, fitted coefficients, hyperparameters, and random effects. The Bayesian bootstrap re-weights the data as samples from a multinomial distribution and fits the re-weighted models \\(B\\) times. Like the classical bootstrap, it provides valuable information about the variability of the model’s parameters. In boosting, each weak learner (i.e., each simple fitted model) is reweighted using the residuals from the previous fits. The results of all these learners are then aggregated into the final model. Boosting can be seen as a method of iteratively improving model accuracy by focusing on the previously mispredicted observations.\n\n\nIn \\(K\\)-fold cross-validation, the data are split into \\(K\\) sub-samples. The model is then fitted \\(k\\) times, each time using \\(k-1\\) sub-samples for training and the remaining one sub-sample as the test set. By the end of the process, all \\(n\\) observations will have been used as test data exactly once. The main advantage of \\(K\\)-fold cross-validation is that it provides reliable test data for model evaluation with the additional cost of fitting \\(K\\) models. Leave-One-Out Cross-Validation (LOO) is a special case of k-fold cross-validation, where the model is trained on \\(n-1\\) observations and tested on the remaining single observation, iterating this process \\(n\\) times. Consequently, \\(n\\) different models are refitted. While computationally intensive, LOO ensures that each data point is used for testing, providing an unbiased evaluation of model performance. Both bootstrapping and \\(K\\)-fold cross-validation, when appropriate performance metrics are used, can help in several ways: i) by facilitating model comparison on the test data, ii) by avoiding overfitting, and iii) by assisting in the selection of hyperparameters.\n\nNotice that in distributional regression models, there is no inherent need to physically partition the original data into subgroups when performing a single or multiple partitions. The same results can be achieved by indexing the original data.frame or by using prior weights during the model fitting process. For example, in \\(K\\)-fold cross-validation, we need \\(K\\) dummy vectors, \\(i_k\\), where $k = 1, , K. These dummy vectors take the value of 1 for the \\(k\\)-th set of observations and \\(0\\) for the rest. The dummy vectors \\(i_k\\) can be used in two ways:\n\nTo select the observations from the data.frame for fitting.\nAs prior weights when fitting the model.\n\nLet da represent the data.frame used to fit the model, and \\(i\\) the \\(i_k\\) dummy vector. For each of the \\(K\\) fits, the model fitting process would use the command \\(data=da[,i]\\) to select the appropriate data, and newdata=da[,i==0] for evaluating the out-of-bag measure. Alternatively, one can use weights=i during the model fitting and newdata=da[,i==0] for evaluating any out-of-bag measures. Note that the appropriate measures for comparing models include predictive deviance (PD) and continuous rank probability scores (CRPS). The function data_Kfold() in the package generates the correct dummy vectors as a matrix of dimensions n B, which can be used for the cross-validation process.\n\nFor the classical bootstrap, we need \\(B\\) vectors. Unlike cross-validation (CV), where the vectors for indexing and prior weights are identical, bootstrapping requires different vectors. The function data_boot_index() creates the appropriate vectors for indexing, while data_boot_weights() generates the vectors for prior weighting. These vectors should differ whether used as indices to select the relevant columns from the data matrix da or as prior weights, which indicate how many times each observation is selected. Typically, a vector like \\((1, 0, 2, \\ldots, 3, 2)\\) means that observation \\(1\\) is picked once, observation \\(2\\) is excluded from the fit, observation \\(3\\) is selected twice, and so on. For Bayesian bootstrap, the \\(B\\) vectors of length \\(n\\) represent weights that sum to \\(n\\). An out-of-bag observation, in this case, is one with zero weight. This distinction makes the split between rearranged and reweighed in Figure 1 somewhat artificial, as both methods can be interpreted as refitting approaches using prior weights.\n\n\n\n\n\n\nTable 4: The data partition functions\n\n\n\n\n\n\n\n\n\nFunctions\nUsage\n\n\n\n\ndata_part()\nCreates a single or multiple (CV) partitions by introducing a factor with different levels\n\n\ndata_part_list()\nCreates a single or multiple (CV) partitions with output a list of data.frames\n\n\ndata_boot_index()\nCreates two lists. The in-bag,IB, indices for fitting and the out-of-bag, OOB, indices for prediction\n\n\ndata_boot_weights()\nCreate a \\(n \\times B\\) matrix with columns weights for bootstrap fits\n\n\ndata_Kfold_index()\nCreates a \\(n \\times K\\) matrix of variables to be used for cross validation data indexing\n\n\ndata_Kfold_weights()\nCreates a \\(n \\times K\\) matrix of dummy variables to be used for cross validation weighted fits\n\n\ndata_cut()\nThis is not a partition funtion but randomly select a specified porpotion of the data\n\n\n\n\n\n\nHere are the data partition functions.\n\ndata_part()\nThe function data_part() it does not partitioning the data as such but adds a factor called partition to the data indicating the partition. By default the data are partitioned into two sets the training ,train, and the test data, test.\n\ndap &lt;- gamlss.prepdata:::data_part(da)\n\ndata partition into two sets \n\nhead(dap)\n\n      rent area yearc location bath kitchen cheating partition\n1 109.9487   26  1918        2    0       0        0     train\n2 243.2820   28  1918        2    0       0        1      test\n3 261.6410   30  1918        1    0       0        1     train\n4 106.4103   30  1918        2    0       0        0      test\n5 133.3846   30  1918        2    0       0        1      test\n6 339.0256   30  1918        2    0       0        1     train\n\n\nIf the user would like to split the data in two separate data.frame’s she/he can use;\n\ndaTrain &lt;- subset(dap, partition==\"train\")\ndaTest &lt;- subset(dap, partition==\"test\")\ndim(daTrain)\n\n[1] 1846    8\n\ndim(daTest)\n\n[1] 1236    8\n\n\nNote, that use of the option partition has the following behaviour;\n\npartition=2L (the default) the factor has two levels train, and test.\npartition=3L the factor has three levels train, val (for validation) and test.\npartition &gt; 4L say K then the levels are 1, 2…K. The factor then can be used to identify K-fold cross validation, see also the function data_Kfols_CV().\n\nback to table\n\n\ndata_part_list()\nThe function data_part_list() creates a list of dataframes which can be used either for single partition or for cross validation. The argument partition allows the splitting of the data to up to 20 subsets. The default is a list of 2 with elements training and test (single partition).\nHere is a single partition in train and test data sets.\n\nallda &lt;-  data_part_list(rent) \nlength(allda)\n\n[1] 2\n\ndim(allda[[\"train\"]]) # training data\n\n[1] 1200    9\n\ndim(allda[[\"test\"]]) # test data\n\n[1] 769   9\n\n\nHere is a multiple partition for cross validation.\n\nallda &lt;-  data_part_list(rent, partition=10) \n\n10-fold data partition \n\nlength(allda)\n\n[1] 10\n\nnames(allda)\n\n [1] \"1\"  \"2\"  \"3\"  \"4\"  \"5\"  \"6\"  \"7\"  \"8\"  \"9\"  \"10\"\n\n\nback to table\n\n\ndata_boot_index()\nThe function data_boot_index() create two lists. The first called IB (for in-bag) and the second OOB, (out-of-bag). Each ellement of the list IB can be use to select a bootstrap sample from the original data.frame while each element of OOB can be use to select the out of sample data points. Note that each bootstap sample approximately contains 2/3 of the original data so we expact on avarage to 1/3 to be in OOB sample.\n\nDD &lt;- data_boot_index(rent, B=10)\n\nThe class, length and names of the created lists;\n\nclass(DD)\n\n[1] \"list\"\n\nlength(DD)\n\n[1] 2\n\nnames(DD)\n\n[1] \"IB\"  \"OOB\"\n\n\nthe first obsevations of the\n\nhead(DD$IB[[1]]) # in bag\n\n[1] 1 2 4 6 8 8\n\nhead(DD$OOB[[1]]) # out-of-bag\n\n[1]  3  5  7 15 21 22\n\n\nback to table\n\n\ndata_boot_weights()\nThe function data_boot_weights() create a \\(n \\times B\\). matrix with columns possible weights for bootstap fits. Note that each bootstap sample approximately contains .666 of the original obsbation so we in general we expect 0.333 of the data to have zero weights.\n\nMM &lt;- data_boot_weights(rent, B=10)\n\nThe M is a matrix which columns can be used as weights in bootstrap fits; Here is a plot of the first column\n\nplot(MM[,1])\n\n\n\n\n\n\n\n\nand here is the first 6 rows os the matrix;\n\nhead(MM)\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n[1,]    1    2    2    1    0    1    3    1    3     1\n[2,]    1    1    0    2    2    1    2    1    1     2\n[3,]    0    0    2    1    2    1    2    1    1     0\n[4,]    1    0    1    0    3    0    1    1    0     2\n[5,]    0    2    3    1    1    1    2    1    2     1\n[6,]    1    0    0    1    0    1    4    0    1     0\n\n\nback to table\n\n\ndata_Kfold_index()\nThe function data_Kfold() creates a matrix which columns can be use for cross validation either as indices to select data for fitting or as prior weights.\n\nPP &lt;- data_Kfold_index(rent, K=10)\nhead(PP)\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n[1,]    1    1    1    1    0    1    1    1    1     1\n[2,]    2    2    0    2    2    2    2    2    2     2\n[3,]    3    3    3    3    3    3    3    3    0     3\n[4,]    4    4    4    4    4    0    4    4    4     4\n[5,]    5    5    5    5    0    5    5    5    5     5\n[6,]    6    6    6    6    6    6    6    0    6     6\n\n\nback to table\n\n\ndata_Kfold_weights()\nThe function data_Kfold() creates a matrix which columns can be use for cross validation either as indices to select data for fitting or as prior weights.\n\nPP &lt;- data_Kfold_weights(rent, K=10)\nhead(PP)\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n[1,]    1    1    1    1    0    1    1    1    1     1\n[2,]    1    1    0    1    1    1    1    1    1     1\n[3,]    1    1    1    1    1    1    1    1    0     1\n[4,]    1    1    1    1    1    0    1    1    1     1\n[5,]    1    1    1    1    0    1    1    1    1     1\n[6,]    1    1    1    1    1    1    1    0    1     1\n\n\nback to table\n\n\ndata_cut()\nThe function data_cut() is not included in the partition Section for its data partitioning properties. It is designed to select a random subset of the data specifically for plotting purposes. This is especially useful when working with large datasets, where plotting routines—such as those from the ggplot2 package—can become very slow.\nThe data_cut() function can either: • Automatically reduce the data size based on the total number of observations, or • Subset the data according to a user-specified percentage.\nHere is an example of the function assuming that that the user requires \\(50%\\) of the data;\n\nda1 &lt;- data_cut(rent99, percentage=.5)\n\n 50 % of data are saved, \nthat is, 1541 observations. \n\ndim(rent99)\n\n[1] 3082    9\n\ndim(da1)\n\n[1] 1541    9\n\n\nThe function data_cut() is used extensively in many plotting routines within the gamlss.ggplots package. When the percentage option is not specified, a default rule is applied to determine the proportion of the data used for plotting. This approach balances performance and visualization quality when working with large datasets.\nLet \\(n\\) denote the total number of observations. Then:\n\nif \\(n \\le 50,000\\): All data are used for plotting.\nif \\(50,000 &lt; n \\le 100,000\\): 50% of the data are used.\nIf \\(100,000 &lt; n \\le 1,000,000\\): 20% of the data are used.\nIf \\(n&gt;1,000,000\\) : 10% of the data are used.\n\nThis default behaviour ensures that plotting remains efficient even with very large datasets.\nback to table"
  },
  {
    "objectID": "gamlss_prepdata.html#sec-Family",
    "href": "gamlss_prepdata.html#sec-Family",
    "title": "The Functions in the package gamlss.prepdata",
    "section": "Plotting Distribution Families",
    "text": "Plotting Distribution Families\n\n\n\n\n\n\nNote\n\n\n\nAll functions listed below belong to the gamlss.ggplots package, not the gamlss.prepdata package. However, since they can be useful during the pre-modeling stage, they are presented here. The later version of the gamlss.ggplots can be found in https://github.com/gamlss-dev/gamlss.ggplots\n\n\nIn the following function, no fitted model is required—only values for the parameters need to be specified.\n\nfamily_pdf()\nThe function family_pdf() plots individual probability density functions (PDFs) from distributions in the gamlss.family package. Although it typically requires the family argument, it defaults to the Normal distribution (NO) if none is provided.\n\nfamily_pdf(from=-5,to=5, mu=0, sigma=c(.5,1,2))\n\n\n\n\n\n\n\nFigure 2: Continuous response example: plotting the pdf of a normal random variable.\n\n\n\n\n\nThe following demonstrates how discrete distributions are displayed. We begin with a distribution that can take infinitely many count values—the Negative Binomial distribution;\n\nfamily_pdf(NBI, to=15, mu=1, sigma=c(.5,1,2), alpha=.9, size.seqment = 3)\n\n\n\n\n\n\n\nFigure 3: Count response example: plotting the pdf of a beta binomial.\n\n\n\n\n\nHere we use a discrete distribution with a finite range of possible values: the Beta-Binomial distribution.\n\nfamily_pdf(BB, to=15, mu=.5, sigma=c(.5,1,2),  alpha=.9, , size.seqment = 3)\n\n\n\n\n\n\n\nFigure 4: Beta binomial response example: plotting the pdf of a beta binomial.\n\n\n\n\n\n\n\nfamily_cdf\nThe function family_cdf() plots cumulative distribution functions (CDFs) from the gamlss.family distributions. The primary argument required is the family specifying the distribution to be used.\n\nfamily_cdf(NBI, to=15, mu=1, sigma=c(.5,1,2), alpha=.9, size.seqment = 3)\n\n\n\n\n\n\n\nFigure 5: Count response example: plotting the cdf of a negative binomial.\n\n\n\n\n\nThe CDf of the negative binomial;\n\nfamily_cdf(BB, to=15, mu=.5, sigma=c(.5,1,2),  alpha=.9, , size.seqment = 3)\n\n\n\n\n\n\n\nFigure 6: Count response example: plotting the cdf of a beta binomial.\n\n\n\n\n\n\n\nfamily_cor()\nThe function family_cor() offers a basic method for examining the inter-correlation among the parameters of any distribution from the gamlss.family. It performs the following steps:\n\nGenerates 10,000 random values from the specified distribution.\nFits the same distribution to the generated data.\nExtracts and plots the correlation coefficients of the distributional parameters.\n\nThese correlation coefficients are derived from the variance-covariance matrix of the fitted model.\n\n\n\n\n\n\nWarning\n\n\n\nThis method provides only a rough indication of how the parameters are correlated at specific values of the distribution’s parameters. The correlation structure may vary significantly at different points in the parameter space, as the distribution can behave quite differently depending on those values.\n\n\n\n#source(\"~/Dropbox/github/gamlss-ggplots/R/family_cor.R\")\ngamlss.ggplots:::family_cor(\"BCTo\", mu=1, sigma=0.11, nu=1, tau=5, no.sim=10000)\n\n\n\n\n\n\n\nFigure 7: Family correlation of a BCTo distribution at specified values of the parameters."
  }
]